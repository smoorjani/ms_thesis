\section{Empirical Observations}
\label{section:discussion}

We analyze how the value of $\beta$ affects generations, finding that generations from $\beta=0.1$ suffer the same degeneration as fine-tuning while higher values avoid these issues. Because $\alpha_{S}$ is not always 1, the constant in front of the discriminator loss is less than $\beta$. Consequently, the discriminator is not given enough weight, and the generator cannot learn as effectively from the discriminator. It is difficult to distinguish differences between $\beta=0.5$ and $\beta=1.0$, but aside from $\beta=0.5$, $\beta=1.0$ outperforms every other value of $\beta$.  

% Beta Weights Generations
\begin{table}[t]
  \small
  \centering
  \caption{Generations of GPT2 trained with the sample-dependent discriminator loss objective with different values of $\beta$. The generations for SD-0.5 and SD-1.0 tend to be much better than for SD-0.1}
  \label{tab:example_beta}
  \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X@{}}
   \toprule[1.5pt]
  \textsc{Model} \& \textsc{Generated Text}\\
  \midrule[0.75pt]
  \textbf{SD-0.1:} IE sucks and makes development on your computer much more difficult than it should be. I believe that Internet Explorer (IE) is far inferior to Internet Explorer (IE) and Internet Explorer (IE) is far inferior to Internet Explorer (IE) \\
\textbf{SD-0.5:} IE sucks and makes development more difficult \\
\textbf{SD-1.0:} IE sucks and makes development on your computer much more difficult than it should be.  \\

  \bottomrule[1.5pt]\\
  \end{tabularx}
  \vspace{-20pt}
  \end{table}

We also experiment with hard-coding the coefficients for the discriminator and reconstruction loss in~\Cref{tab:example_hardcoded_weights}. Putting too much weight on the discriminator loss, $L_D$, (\ie 0.9) leads to poor quality arguments having some of the strongest linguistic feature changes (\eg shorter length). Conversely, limiting $L_D$ to 0.1 leads to much stronger generations. We introduced the $\beta$ parameter to cap $L_D$ at $\beta$. Because of the $\beta$ parameter, the previous experiments show similar but less obvious trends. 

% Hard-coded Weights Generations

\begin{table}[t]
  \small
  \centering
  \caption{Generations of our model trained with a mixed reconstruction and discriminator loss objective with hard-coded weights (as opposed to sample-dependent). }
  \label{tab:example_hardcoded_weights}
  \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X@{}}
   \toprule[1.5pt]
  \textsc{Model} \& \textsc{Generated Text}\\
  \midrule[0.75pt]
  \textsc{Example 1:}\\
  
  \textbf{0.9 Supervised + 0.1 MLE:} Schools should teach
physical education because it's a good thing. \\
\textbf{0.1 Supervised + 0.9 MLE:} Schools should teach 
physical education because PE helps children develop 
good habits later on in life. Plus, there's the benefit of 
working together as a team that doesn't always happen in 
other classes. \\

  \addlinespace[0.5em]
  \textsc{Example 2:}\\

\textbf{0.9 Supervised + 0.1 MLE:} Plastic water bottles 
are bad because they are not recyclable. \\
\textbf{0.1 Supervised + 0.9 MLE:} Plastic water bottles 
are bad because they are bad for the environment and they 
are bad for the economy.Some people think that bottled 
water is bad for consumers and should only be used in 
situations such as disasters when no other clean water 
is available.\\ 

  \addlinespace[0.5em]
  \textsc{Example 3:}\\

\textbf{0.9 Supervised + 0.1 MLE:} Gaming is good for
child development because you can play with other kids. \\
\textbf{0.1 Supervised + 0.9 MLE:} Gaming is good for child
development because it teaches them how to think and 
solve problems. It also teaches them how to communicate 
with each other. \\

  \bottomrule[1.5pt]\\
  \end{tabularx}
  \vspace{-20pt}
  \end{table}

