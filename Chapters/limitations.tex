\section{Limitations}
\label{sec:limitations}

As with other unconstrained natural language generation applications, our systems are prone to issues like degeneration from beam search and neural hallucinations \citep{ji2022survey}. We post-process generations to combat the former problem, but future work will hopefully provide better methods to prevent this issue. The latter primarily applies to~\Cref{chp:style_infusion}, in which we increase our dataset with samples from the CNN/DailyMail dataset \citep{see-etal-2017-get}, partially mitigating the problem, but out-of-domain topics still suffer. Expanding the dataset size will only work for so long due to diminishing marginal returns.

We also note that our frameworks are limited by the computational resources available to us. Thus, we could not effectively support long text generation while preserving the quality of the generated text. During training, we decreased the batch size and utilized the DeepSpeed framework \citep{rasley-2020-deepspeed}, but it is still insufficient to handle long text. Furthermore, traditional left-to-right generation struggles with long text as the topics tend to diverge \citep{Tan2020}. Because many styles, like persuasiveness, depend on paragraph-level features in addition to sentence-level features, it is beneficial for our application to support longer texts.

Due to the limited data available for our style infusion approach, we considered iteratively training the discriminator with the augmented data while we trained the generator. Ultimately, we felt that the weak labels would dilute the learned trends in the discriminator, but it may be interesting to see how it affects the framework's performance. Currently, collecting pairwise datasets to use with this framework can be considered a limitation. With increasing interest in the computational synthesis of persuasive text and imagery, we expect to see more relevant curated datasets in the near future. Generating pairwise data through human subject experiments is expensive, which is why the data augmentation methods introduced in this paper are crucial for future work. We also note that while we claim that humans are better at pairwise evaluations, recent work has shown that humans are not always the gold standard for labeling, especially in natural language generation tasks \citep{clark2021all}.

One of the most significant limitations of the style infusion approach is in showing the effectiveness of our chosen architecture. Because most baselines are in style transfer and fundamentally differ from our task, we find it difficult to make a fair comparison with prior work. Regardless, style infusion is a critical step for unconstrained NLG systems such as dialogue systems and chatbots, especially in the context of human-centric stylistic objectives, which are already difficult enough to define. We encounter similar challenges in our work on stylized controllable text generation because many existing architectures do not generalize well and produce poor results. 

As mentioned in~\Cref{subsec:ne_results}, the modified prototype-then-edit model for controllable text generation is limited by its formulation. While it produces good results for speed, it will likely struggle with other constraints due to how the edit vector is constructed and requires expensive retraining for different values of $\Delta$. Conversely, the adversarial architecture in~\Cref{sec:adversarial_control} is likely able to adapt well to other constraints and can exhibit control on any arbitrary value of $\Delta$. However, neither of our current formulations would allow for the control of multiple constraints simultaneously, eliminating them as candidates for controlling styles that depend on the composition of features (\eg persuasiveness). 




