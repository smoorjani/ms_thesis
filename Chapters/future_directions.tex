We plan to continue improving our work in many directions to achieve persuasive text-generation systems that make scientific literature more approachable and appealing to the general public. Primarily, we aim to run multiple human subject evaluation studies to ensure that our proposed approaches lead to fluent, relevant, and stylized or controlled generations. We also look to the following areas as promising avenues for future work:

\begin{itemize}
    \item \textbf{Abstractive Summarization}: Studies have shown that the dissemination of scientific literature is strongly correlated with sharing work through social media (\eg Twitter) \citep{eysenbach2011can, tahamtan2016factors}. \citet{eysenbach2011can} find that research tweeted about frequently in the first few days following publications was 11 times more likely to be highly cited than less tweeted articles. By utilizing abstractive summarization techniques for scientific literature \citep{cohan2018discourse, altmami2020automatic}, we hope to help scientists share their findings and encourage the direct dissemination of scientific information.

    \item \textbf{Discourse Structure}: Despite scientific news articles conveying similar information to scientific literature, the former tends to be more appealing to a layperson audience \citep{plaven2017research}. Studies have found that news articles often follow an inverted pyramid structure while scientific literature follows its own structure \citep{rafiee2018culture, cohan2018discourse}. We hypothesize that the discourse structure plays a significant role in ordering how ideas are conveyed and capturing the attention of target audiences. We hope to generate more appealing texts by altering the discourse structure while presenting the same content.
    
    \item \textbf{Long-text Processing \& Generation}: Scientific texts tend to be long \citep{tretyak2020combination} and infeasible to process with vanilla transformers \citep{vaswani2017attention} due to memory and computational requirements growing quadratically with sequence length. Furthermore, vanilla transformers struggle to remember long-term dependencies when generating text, leading to incoherent and contradictory texts. Potential solutions include incorporating long-document attention mechanisms \citep{beltagy2020longformer, zaheer2020big} to capture document-level style features and using progressive generation, instead of planning-then-generation, to improve long-document generation quality \citep{Tan2020}.
    
    \item \textbf{Knowledge-Driven Natural Language Generation}: Existing large language models are prone to neural hallucinations \citep{ji2022survey}, which can be extremely dangerous for persuasive systems for scientific literature. Existing methods have explored ways to incorporate external knowledge structures into text generation \citep{yu2022survey, wang2019paperrobot}, and we hope to utilize these works to ensure the factuality of our generations.
\end{itemize}