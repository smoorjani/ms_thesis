@STRING{IEEE_J_EC         = "{IEEE} Trans. En. Conv."}
@STRING{IEEE_J_PWRE       = "{IEEE} Trans. Power Electron."}
@STRING{IEEE_J_PWRS       = "{IEEE} Trans. Power Syst."}
@STRING{IEEE_J_MAG        = "{IEEE} Trans. Magn."}
@STRING{J_NUM_ENG         = "Int. J. Numer. Meth. Engng."}
@STRING{ENG_ANAL          = "Eng. Anal."}
@STRING{IEEE_J_EDU        = "{IEEE} Trans. Educ."}
@STRING{IEEE_J_MTT        = "{IEEE} Trans. Microwave Theory Tech."}
@STRING{IEEE_J_IA         = "{IEEE} Trans. Ind. Applicat."}
@STRING{IEEE_J_AntProp    = "{IEEE} Trans. Ant. and Prop."}

@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
CTLmax_names_forced_etal = "4",
CTLnames_show_etal = "1"
};

%% IEEEexample.bib 
%% V1.10 
%% 2002/09/27
%% Copyright (c) 2002 by Michael Shell
%% mshell(at)ece.gatech.edu
%% See support website below for current contact information.
%% 
%% NOTE: This text file uses MS Windows line feed conventions. When (human)
%% reading this file on other platforms, you may have to use a text
%% editor that can handle lines terminated by the MS Windows line feed
%% characters (0x0D 0x0A).
%% 
%% This is an example BibTeX database for the official IEEEtran.bst
%% BibTeX style file.
%% 
%% Some entries call strings that are defined in the IEEEabrv.bib file.
%% Therefore, IEEEabrv.bib should be loaded prior to this file. 
%% Usage: 
%% 
%% \bibliographystyle{./IEEEtran} % use IEEEtran.bst style
%% \bibliography{./IEEEabrv,./IEEEexample}
%% 
%% 
%% Support sites:
%% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/ 
%% and/or
%% http://www.ieee.org
%%**********************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% This code is distributed under the Perl Artistic License 
%% ( http://language.perl.com/misc/Artistic.html ) 
%% and may be freely used, distributed and modified - subject to the
%% constraints therein.
%% Retain all contribution notices, credits and disclaimers.
%% 
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%**********************************************************************


% Note that, because the example references were taken from actual IEEE
% publications, these examples do not always contain the full amount
% of information that may be desirable (for use with other BibTeX styles).
% In particular, full names (not abbreviated with initials) should be
% entered whenever possible as some (non-IEEE) bibliography styles use
% full names. IEEEtran.bst will automatically abbreviate when it
% encounters full names.
 
 
 %% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Hari Sundaram at 2022-06-23 13:04:23 -0500 


%% Saved with string encoding Unicode (UTF-8) 

@article{clark2021all,
  title={All that's' human'is not gold: Evaluating human evaluation of generated text},
  author={Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2107.00061},
  year={2021}
}

@article{goode2007implicit,
  title={The implicit and explicit role of ad memory in ad persuasion: rethinking the hidden persuaders.},
  author={Goode, Alastair},
  journal={International Journal of Market Research},
  volume={49},
  number={1},
  year={2007}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yu2022survey,
  title={A survey of knowledge-enhanced text generation},
  author={Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  journal={ACM Computing Surveys},
  volume={54},
  number={11s},
  pages={1--38},
  year={2022},
  publisher={ACM New York, NY}
}

@article{wang2019paperrobot,
  title={Paperrobot: Incremental draft generation of scientific ideas},
  author={Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  journal={arXiv preprint arXiv:1905.07870},
  year={2019}
}

@article{ji2022survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  year={2022},
  publisher={ACM New York, NY}
}

@article{tahamtan2016factors,
  title={Factors affecting number of citations: a comprehensive review of the literature},
  author={Tahamtan, Iman and Safipour Afshar, Askar and Ahamdzadeh, Khadijeh},
  journal={Scientometrics},
  volume={107},
  pages={1195--1225},
  year={2016},
  publisher={Springer}
}

@article{eysenbach2011can,
  title={Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact},
  author={Eysenbach, Gunther and others},
  journal={Journal of medical Internet research},
  volume={13},
  number={4},
  pages={e2012},
  year={2011},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{altmami2020automatic,
  title={Automatic summarization of scientific articles: A survey},
  author={Altmami, Nouf Ibrahim and Menai, Mohamed El Bachir},
  journal={Journal of King Saud University-Computer and Information Sciences},
  year={2020},
  publisher={Elsevier}
}

@article{plaven2017research,
  title={Research: The readability of scientific texts is decreasing over time. eLife},
  author={Plaven-Sigray, P and Matheson, GJ and Schiffler, BC and Thompson, WH},
  journal={DOI: https://doi. org/10.7554/eLife},
  volume={27725},
  year={2017}
}

@article{cohan2018discourse,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}

@article{rafiee2018culture,
  title={Culture and discourse structure: A comparative study of Dutch and Iranian news texts},
  author={Rafiee, Afrooz and Spooren, Wilbert and Sanders, Jos{\'e}},
  journal={Discourse \& Communication},
  volume={12},
  number={1},
  pages={58--79},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{tretyak2020combination,
  title={Combination of abstractive and extractive approaches for summarization of long scientific texts},
  author={Tretyak, Vladislav and Stepanov, Denis},
  journal={arXiv preprint arXiv:2006.05354},
  year={2020}
}

@article{bond2021deep,
  title={Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models},
  author={Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2021},
  publisher={IEEE}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@inproceedings{peng2018towards,
  title={Towards controllable story generation},
  author={Peng, Nanyun and Ghazvininejad, Marjan and May, Jonathan and Knight, Kevin},
  booktitle={Proceedings of the First Workshop on Storytelling},
  pages={43--49},
  year={2018}
}

@article{lu2022quark,
  title={Quark: Controllable text generation with reinforced unlearning},
  author={Lu, Ximing and Welleck, Sean and Jiang, Liwei and Hessel, Jack and Qin, Lianhui and West, Peter and Ammanabrolu, Prithviraj and Choi, Yejin},
  journal={arXiv preprint arXiv:2205.13636},
  year={2022}
}

@inproceedings{gamon2004linguistic,
  title={Linguistic correlates of style: authorship classification with deep linguistic analysis features},
  author={Gamon, Michael},
  booktitle={Coling 2004: Proceedings of the 20th international conference on computational linguistics},
  pages={611--617},
  year={2004}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{Zhang2022ASO,
  title={A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models},
  author={Hanqing Zhang and Haolin Song and Shaoyu Li and Ming Zhou and Dawei Song},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.05337}
}

@article{cai2020utilizing,
  title={Utilizing amari-alpha divergence to stabilize the training of generative adversarial networks},
  author={Cai, Likun and Chen, Yanjie and Cai, Ning and Cheng, Wei and Wang, Hao},
  journal={Entropy},
  volume={22},
  number={4},
  pages={410},
  year={2020},
  publisher={MDPI}
}

@article{arjovsky2017towards,
  title={Towards principled methods for training generative adversarial networks},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.04862},
  year={2017}
}

@inproceedings{thanh2020catastrophic,
  title={Catastrophic forgetting and mode collapse in gans},
  author={Thanh-Tung, Hoang and Tran, Truyen},
  booktitle={2020 international joint conference on neural networks (ijcnn)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{deng2020residual,
  title={Residual energy-based models for text generation},
  author={Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:2004.11714},
  year={2020}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{he2021parallel,
  title={Parallel refinements for lexically constrained text generation with bart},
  author={He, Xingwei},
  journal={arXiv preprint arXiv:2109.12487},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?��},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@inproceedings{liu-etal-2021-dexperts,
    title = "{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    author = "Liu, Alisa  and
      Sap, Maarten  and
      Lu, Ximing  and
      Swayamdipta, Swabha  and
      Bhagavatula, Chandra  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.522",
    doi = "10.18653/v1/2021.acl-long.522",
    pages = "6691--6706",
    abstract = "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with {``}expert{''} LMs and/or {``}anti-expert{''} LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
}

@article{wang2019topic,
  title={Topic-guided variational autoencoders for text generation},
  author={Wang, Wenlin and Gan, Zhe and Xu, Hongteng and Zhang, Ruiyi and Wang, Guoyin and Shen, Dinghan and Chen, Changyou and Carin, Lawrence},
  journal={arXiv preprint arXiv:1903.07137},
  year={2019}
}

@inproceedings{xu2020variational,
  title={On variational learning of controllable representations for text without supervision},
  author={Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
  booktitle={International Conference on Machine Learning},
  pages={10534--10543},
  year={2020},
  organization={PMLR}
}

@inproceedings{scialom2020discriminative,
  title={Discriminative adversarial search for abstractive summarization},
  author={Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  booktitle={International Conference on Machine Learning},
  pages={8555--8564},
  year={2020},
  organization={PMLR}
}

@inproceedings{Dathathri2020Plug,
    title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
    author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=H1edEyBKDS}
}

@inproceedings{Papineni2002bleu,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@inproceedings{chen2022generating,
  title={Generating Persuasive Responses to Customer Reviews with Multi-Source Prior Knowledge in E-commerce},
  author={Chen, Bo and Liu, Jiayi and Maimaiti, Mieradilijiang and Gao, Xing and Zhang, Ji},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2994--3002},
  year={2022}
}

@article{liu2021ai,
  title={AI-Empowered Persuasive Video Generation: A Survey},
  author={Liu, Chang and Yu, Han},
  journal={arXiv preprint arXiv:2112.09401},
  year={2021}
}

@article{kyle2020generating,
  title={Generating memorable images based on human visual memory schemas},
  author={Kyle-Davidson, Cameron and Bors, Adrian G and Evans, Karla K},
  journal={arXiv preprint arXiv:2005.02969},
  year={2020}
}

@inproceedings{guerini2012linguistic,
  title={Do linguistic style and readability of scientific abstracts affect their virality?},
  author={Guerini, Marco and Pepe, Alberto and Lepri, Bruno},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={6},
  number={1},
  pages={475--478},
  year={2012}
}

@article{burgoon1993interpersonal,
  title={Interpersonal expectations, expectancy violations, and emotional communication},
  author={Burgoon, Judee K},
  journal={Journal of language and social psychology},
  volume={12},
  number={1-2},
  pages={30--48},
  year={1993},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{rubin1977very,
  title={Very long-term memory for prose and verse},
  author={Rubin, David C},
  journal={Journal of Verbal Learning and Verbal Behavior},
  volume={16},
  number={5},
  pages={611--621},
  year={1977},
  publisher={Elsevier}
}

@article{boers2005finding,
  title={Finding ways to make phrase-learning feasible: The mnemonic effect of alliteration},
  author={Boers, Frank and Lindstromberg, Seth},
  journal={System},
  volume={33},
  number={2},
  pages={225--238},
  year={2005},
  publisher={Elsevier}
}

@article{hyman1990memorabeatlia,
  title={Memorabeatlia: A naturalistic study of long-term memory},
  author={Hyman, Ira E and Rubin, David C},
  journal={Memory \& Cognition},
  volume={18},
  number={2},
  pages={205--214},
  year={1990},
  publisher={Springer}
}

 @book{
    thorn_page_2009,
    title={Interactions between short-term and long-term memory in the verbal domain},
    publisher={Psychology Press},
    author={Thorn, Annabel and Page, Mike},
    year={2009}
} 

@article{wang2019persuasion,
  title={Persuasion for good: Towards a personalized persuasive dialogue system for social good},
  author={Wang, Xuewei and Shi, Weiyan and Kim, Richard and Oh, Yoojung and Yang, Sijia and Zhang, Jingwen and Yu, Zhou},
  journal={arXiv preprint arXiv:1906.06725},
  year={2019}
}

@article{iyer2019unsupervised,
  title={An unsupervised domain-independent framework for automated detection of persuasion tactics in text},
  author={Iyer, Rahul Radhakrishnan and Sycara, Katia},
  journal={arXiv preprint arXiv:1912.06745},
  year={2019}
}

 @misc{demers_2016,
 title={6 ways to persuade anyone of anything},
 url={https://www.businessinsider.com/6-ways-to-persuade-anyone-of-anything-2016-7#-1},
 journal={Business Insider},
 publisher={Business Insider},
 author={DeMers, Jayson},
 year={2016},
 month={Jul}
} 

@inproceedings{cano-basave-he-2016-study,
    title = "A Study of the Impact of Persuasive Argumentation in Political Debates",
    author = "Cano-Basave, Amparo Elizabeth  and
      He, Yulan",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1166",
    doi = "10.18653/v1/N16-1166",
    pages = "1405--1413",
}

@inproceedings{hidey-etal-2017-analyzing,
    title = "Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum",
    author = "Hidey, Christopher  and
      Musi, Elena  and
      Hwang, Alyssa  and
      Muresan, Smaranda  and
      McKeown, Kathy",
    booktitle = "Proceedings of the 4th Workshop on Argument Mining",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5102",
    doi = "10.18653/v1/W17-5102",
    pages = "11--21",
    abstract = "Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of understanding what makes a message persuasive. Premises are annotated with the three types of persuasive modes: ethos, logos, pathos, while claims are labeled as interpretation, evaluation, agreement, or disagreement, the latter two designed to account for the dialogical nature of our corpus. We aim to answer three questions: 1) can humans reliably annotate the semantic types of argument components? 2) are types of premises/claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in persuasive messages than in non-persuasive messages?",
}

@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2205.14217},
  year={2022}
}

@article{krause2020gedi,
  title={Gedi: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv preprint arXiv:2009.06367},
  year={2020}
}

@article{yang2021fudge,
  title={FUDGE: Controlled text generation with future discriminators},
  author={Yang, Kevin and Klein, Dan},
  journal={arXiv preprint arXiv:2104.05218},
  year={2021}
}

@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@article{kumar2021controlled,
  title={Controlled text generation as continuous optimization with multiple constraints},
  author={Kumar, Sachin and Malmi, Eric and Severyn, Aliaksei and Tsvetkov, Yulia},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14542--14554},
  year={2021}
}

@inproceedings{zhou2018emotional,
  title={Emotional chatting machine: Emotional conversation generation with internal and external memory},
  author={Zhou, Hao and Huang, Minlie and Zhang, Tianyang and Zhu, Xiaoyan and Liu, Bing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{ghosh-etal-2017-affect,
    title = "Affect-{LM}: A Neural Language Model for Customizable Affective Text Generation",
    author = "Ghosh, Sayan  and
      Chollet, Mathieu  and
      Laksana, Eugene  and
      Morency, Louis-Philippe  and
      Scherer, Stefan",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1059",
    doi = "10.18653/v1/P17-1059",
    pages = "634--642"
}

@inproceedings{orbach-goldberg-2020-facts2story,
    title = "{F}acts2{S}tory: Controlling Text Generation by Key Facts",
    author = "Orbach, Eyal  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.211",
    doi = "10.18653/v1/2020.coling-main.211",
    pages = "2329--2345"
}

@inproceedings{Tambwekar_2019,
	doi = {10.24963/ijcai.2019/829},
	url = {https://doi.org/10.24963%2Fijcai.2019%2F829},
	year = 2019,
	month = {aug},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Pradyumna Tambwekar and Murtaza Dhuliawala and Lara J. Martin and Animesh Mehta and Brent Harrison and Mark O. Riedl},
	title = {Controllable Neural Story Plot Generation via Reward Shaping},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence}
}

@article{gauchat2012politicization,
author = {Gordon Gauchat},
title ={Politicization of Science in the Public Sphere: A Study of Public Trust in the United States, 1974 to 2010},
journal = {American Sociological Review},
volume = {77},
number = {2},
pages = {167-187},
year = {2012},
doi = {10.1177/0003122412438225},
URL = { https://doi.org/10.1177/0003122412438225},
eprint = { https://doi.org/10.1177/0003122412438225}
}

@article{
vosoughi2018spread,
author = {Soroush Vosoughi  and Deb Roy  and Sinan Aral },
title = {The spread of true and false news online},
journal = {Science},
volume = {359},
number = {6380},
pages = {1146-1151},
year = {2018},
doi = {10.1126/science.aap9559},
URL = {https://www.science.org/doi/abs/10.1126/science.aap9559},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aap9559}}


@inproceedings{toubia-2021,
	author = {Olivier Toubia and Jonah Berger and Jehoshua Eliashberg},
	title = {How quantifying the shape of stories predicts their success},
	year = {2021}}

@article{pennebaker1999linguistic,
	author = {Pennebaker, James W and King, Laura A},
	journal = {Journal of personality and social psychology},
	number = {6},
	pages = {1296},
	publisher = {American Psychological Association},
	title = {Linguistic styles: language use as an individual difference.},
	volume = {77},
	year = {1999}}

@inproceedings{hu2017toward,
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
	booktitle = {International conference on machine learning},
	organization = {PMLR},
	pages = {1587--1596},
	title = {Toward controlled generation of text},
	year = {2017}}

@inproceedings{jain2019unsupervised,
	author = {Jain, Parag and Mishra, Abhijit and Azad, Amar Prakash and Sankaranarayanan, Karthik},
	booktitle = {Proceedings of the AAAI conference on artificial intelligence},
	number = {01},
	pages = {6554--6561},
	title = {Unsupervised controllable text formalization},
	volume = {33},
	year = {2019}}

@article{mir2019evaluating,
	author = {Mir, Remi and Felbo, Bjarke and Obradovich, Nick and Rahwan, Iyad},
	journal = {arXiv preprint arXiv:1904.02295},
	title = {Evaluating style transfer for text},
	year = {2019}}

@inproceedings{kim2019comparing,
	author = {Kim, Soomin and Lee, Joonhwan and Gweon, Gahgene},
	booktitle = {Proceedings of the 2019 CHI conference on human factors in computing systems},
	pages = {1--12},
	title = {Comparing data from chatbot and web surveys: Effects of platform and conversational style on survey response quality},
	year = {2019}}

@article{jin2022deep,
	author = {Jin, Di and Jin, Zhijing and Hu, Zhiting and Vechtomova, Olga and Mihalcea, Rada},
	journal = {Computational Linguistics},
	number = {1},
	pages = {155--205},
	publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~{\ldots}},
	title = {Deep learning for text style transfer: A survey},
	volume = {48},
	year = {2022}}

@inproceedings{habernal-gurevych-2016-argument,
	address = {Berlin, Germany},
	author = {Habernal, Ivan and Gurevych, Iryna},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/P16-1150},
	month = aug,
	pages = {1589--1599},
	publisher = {Association for Computational Linguistics},
	title = {Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional {LSTM}},
	url = {https://aclanthology.org/P16-1150},
	year = {2016},
	bdsk-url-1 = {https://aclanthology.org/P16-1150},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P16-1150}}

@article{tan2016,
	author = {Chenhao Tan and Vlad Niculae and Cristian Danescu{-}Niculescu{-}Mizil and Lillian Lee},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/TanNDL16.bib},
	eprint = {1602.01103},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:53 +0200},
	title = {Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions},
	url = {http://arxiv.org/abs/1602.01103},
	volume = {abs/1602.01103},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1602.01103}}

@inproceedings{duerr-2021,
	author = {Sebastian D{\"{u}}rr and Peter A. Gloor},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2101-05786.bib},
	eprint = {2101.05786},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 22 Jan 2021 15:16:00 +0100},
	title = {Persuasive Natural Language Generation - {A} Literature Review},
	url = {https://arxiv.org/abs/2101.05786},
	volume = {abs/2101.05786},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2101.05786}}

@inproceedings{xu2019clickbait,
	author = {Xu, Peng and Wu, Chien-Sheng and Madotto, Andrea and Fung, Pascale},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	pages = {3056--3066},
	title = {Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning},
	year = {2019}}

@article{see-2017,
	author = {Abigail See and Peter J. Liu and Christopher D. Manning},
	journal = {CoRR},
	title = {Get To The Point: Summarization with Pointer-Generator Networks},
	year = {2017}}

@article{prabhumoye2018style,
	author = {Prabhumoye, Shrimai and Tsvetkov, Yulia and Salakhutdinov, Ruslan and Black, Alan W},
	journal = {arXiv preprint arXiv:1804.09000},
	title = {Style transfer through back-translation},
	year = {2018}}

@article{williams1992simple,
	author = {Williams, Ronald J},
	journal = {Machine learning},
	number = {3},
	pages = {229--256},
	publisher = {Springer},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	year = {1992}}

@article{ranzato2015sequence,
	author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	journal = {arXiv preprint arXiv:1511.06732},
	title = {Sequence level training with recurrent neural networks},
	year = {2015}}

@article{li2020,
	author = {Jialu Li and Esin Durmus and Claire Cardie},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2010-03538.bib},
	eprint = {2010.03538},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 13 Oct 2020 15:25:23 +0200},
	title = {Exploring the Role of Argument Structure in Online Debate Persuasion},
	url = {https://arxiv.org/abs/2010.03538},
	volume = {abs/2010.03538},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.03538}}

@article{radford2019language,
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	journal = {OpenAI blog},
	number = {8},
	pages = {9},
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	year = {2019}}

@article{wolf2019,
	author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'{e}}mi Louf and Morgan Funtowicz and Jamie Brew},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
	eprint = {1910.03771},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 02 Jun 2020 12:49:01 +0200},
	title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	volume = {abs/1910.03771},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1910.03771}}

@inproceedings{devlin-etal-2019-bert,
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	address = {Minneapolis, Minnesota},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	doi = {10.18653/v1/N19-1423},
	month = jun,
	pages = {4171--4186},
	publisher = {Association for Computational Linguistics},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://aclanthology.org/N19-1423},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1423},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423}}

@article{reimers2019,
	author = {Nils Reimers and Iryna Gurevych},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1908-10084.bib},
	eprint = {1908.10084},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 26 Nov 2020 12:13:54 +0100},
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	url = {http://arxiv.org/abs/1908.10084},
	volume = {abs/1908.10084},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1908.10084}}

@inproceedings{see-etal-2017-get,
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	address = {Vancouver, Canada},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/P17-1099},
	month = jul,
	pages = {1073--1083},
	publisher = {Association for Computational Linguistics},
	title = {Get To The Point: Summarization with Pointer-Generator Networks},
	url = {https://www.aclweb.org/anthology/P17-1099},
	year = {2017},
	bdsk-url-1 = {https://www.aclweb.org/anthology/P17-1099},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1099}}

@article{cer-2018,
	author = {Daniel Cer and Yinfei Yang and Sheng{-}yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St. John and Noah Constant and Mario Guajardo{-}Cespedes and Steve Yuan and Chris Tar and Yun{-}Hsuan Sung and Brian Strope and Ray Kurzweil},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1803-11175.bib},
	eprint = {1803.11175},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
	title = {Universal Sentence Encoder},
	url = {http://arxiv.org/abs/1803.11175},
	volume = {abs/1803.11175},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1803.11175}}

@article{johnson2019billion,
	author = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
	journal = {IEEE Transactions on Big Data},
	number = {3},
	pages = {535--547},
	publisher = {IEEE},
	title = {Billion-scale similarity search with {GPUs}},
	volume = {7},
	year = {2019}}

@inproceedings{lin-2004-rouge,
	address = {Barcelona, Spain},
	author = {Lin, Chin-Yew},
	booktitle = {Text Summarization Branches Out},
	month = jul,
	pages = {74--81},
	publisher = {Association for Computational Linguistics},
	title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
	url = {https://aclanthology.org/W04-1013},
	year = {2004},
	bdsk-url-1 = {https://aclanthology.org/W04-1013}}

@inproceedings{lavie-agarwal-2007-meteor,
	address = {Prague, Czech Republic},
	author = {Lavie, Alon and Agarwal, Abhaya},
	booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
	month = jun,
	pages = {228--231},
	publisher = {Association for Computational Linguistics},
	title = {{METEOR}: An Automatic Metric for {MT} Evaluation with High Levels of Correlation with Human Judgments},
	url = {https://aclanthology.org/W07-0734},
	year = {2007},
	bdsk-url-1 = {https://aclanthology.org/W07-0734}}

@inproceedings{bird-loper-2004-nltk,
	address = {Barcelona, Spain},
	author = {Bird, Steven and Loper, Edward},
	booktitle = {Proceedings of the {ACL} Interactive Poster and Demonstration Sessions},
	month = jul,
	pages = {214--217},
	publisher = {Association for Computational Linguistics},
	title = {{NLTK}: The Natural Language Toolkit},
	url = {https://aclanthology.org/P04-3031},
	year = {2004},
	bdsk-url-1 = {https://aclanthology.org/P04-3031}}

@article{fu-2017-style,
	author = {Zhenxin Fu and Xiaoye Tan and Nanyun Peng and Dongyan Zhao and Rui Yan},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1711-06861.bib},
	eprint = {1711.06861},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Sat, 31 Aug 2019 16:23:04 +0200},
	title = {Style Transfer in Text: Exploration and Evaluation},
	url = {http://arxiv.org/abs/1711.06861},
	volume = {abs/1711.06861},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1711.06861}}

@inproceedings{lipa-urbina-2021,
	abstract = {In the last decades, the Natural Language Generation (NLG) methods have been improved to generate text automatically. However, based on the literature review, there are not works on generating text for persuading people. In this paper, we propose to use the SentiGAN framework to generate messages that are classified into levels of persuasiveness. And, we run an experiment using the Microtext dataset for the training phase. Our preliminary results show 0.78 of novelty on average, and 0.57 of diversity in the generated messages.},
	author = {E. Lipa-Urbina and N. Condori-Fernandez and F. Suni-Lopez},
	booktitle = {Persuasive Technology},
	doi = {10.1007/978-3-030-79460-6_5},
	editor = {Raian Ali and Birgit Lugrin and Fred Charles},
	isbn = {9783030794590},
	language = {English},
	note = {16th International Conference on Persuasive Technology, PERSUASIVE 2021 ; Conference date: 12-04-2021 Through 14-04-2021},
	pages = {55--62},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	title = {Towards an Automatic Generation of Persuasive Messages},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-79460-6_5}}

@inproceedings{sentigan-wang,
	author = {Ke Wang and Xiaojun Wan 0001},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden},
	citedby = {0},
	cites = {0},
	doi = {10.24963/ijcai.2018/618},
	editor = {J{\'e}r{\^o}me Lang},
	isbn = {978-0-9992411-2-7},
	pages = {4446-4452},
	publisher = {ijcai.org},
	researchr = {https://researchr.org/publication/Wang018-4},
	title = {SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks},
	url = {https://doi.org/10.24963/ijcai.2018/618},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2018/618}}

@book{cialdini2009influence,
	author = {Cialdini, R.B.},
	isbn = {9780061899874},
	publisher = {HarperCollins e-books},
	series = {Collins Business Essentials},
	title = {Influence: The Psychology of Persuasion},
	url = {https://books.google.com/books?id=5dfv0HJ1TEoC},
	year = {2009},
	bdsk-url-1 = {https://books.google.com/books?id=5dfv0HJ1TEoC}}

@article{murphy-2001,
	abstract = {Students' conceptions of persuasion have rarely been considered in the existing persuasion literature. In addition, most studies of persuasion have employed contrived reading materials, rather than naturally occurring texts (\eg unmodified newspapers or magazine articles). As such, the purpose of this study was to provide competent, undergraduate readers an opportunity to externalize their perceptions of persuasiveness, and to compare these student-determined conceptions of persuasion to those espoused by text-based persuasion and conceptual change experts. In addition, the conceptions of students and experts were compared with those advocated in the literature. The findings suggest substantive overlap between students' and experts' conceptions of persuasion. Specifically, participants' responses revealed that the type and form of supporting evidence as well as the affective nature of the text play important roles in persuasion. These findings are somewhat different than those found in the existing literature. Implications for research and practice are offered.},
	author = {P.Karen Murphy},
	doi = {https://doi.org/10.1016/S0883-0355(02)00009-5},
	issn = {0883-0355},
	journal = {International Journal of Educational Research},
	number = {7},
	pages = {675-698},
	title = {What makes a text persuasive? Comparing students' and experts' conceptions of persuasiveness},
	url = {https://www.sciencedirect.com/science/article/pii/S0883035502000095},
	volume = {35},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0883035502000095},
	bdsk-url-2 = {https://doi.org/10.1016/S0883-0355(02)00009-5}}

@inproceedings{al-khatib-etal-2020-exploiting,
	abstract = {Predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising. While clearly relevant to the task, the personal characteristics of an argument{'}s source and audience have not yet been fully exploited toward automated persuasiveness prediction. In this paper, we model debaters{'} prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater{'}s characteristics enhances the prediction of argument persuasiveness as well as of debaters{'} resistance to persuasion.},
	address = {Online},
	author = {Al Khatib, Khalid and V{\"o}lske, Michael and Syed, Shahbaz and Kolyada, Nikolay and Stein, Benno},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	doi = {10.18653/v1/2020.acl-main.632},
	month = jul,
	pages = {7067--7072},
	publisher = {Association for Computational Linguistics},
	title = {Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness},
	url = {https://aclanthology.org/2020.acl-main.632},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.632},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.632}}

@article{berger-2012,
	abstract = { Why are certain pieces of online content (\eg advertisements, videos, news articles) more viral than others? This article takes a psychological approach to understanding diffusion. Using a unique data set of all the New York Times articles published over a three-month period, the authors examine how emotion shapes virality. The results indicate that positive content is more viral than negative content, but the relationship between emotion and social transmission is more complex than valence alone. Virality is partially driven by physiological arousal. Content that evokes high-arousal positive (awe) or negative (anger or anxiety) emotions is more viral. Content that evokes low-arousal, or deactivating, emotions (\eg sadness) is less viral. These results hold even when the authors control for how surprising, interesting, or practically useful content is (all of which are positively linked to virality), as well as external drivers of attention (\eg how prominently content was featured). Experimental results further demonstrate the causal impact of specific emotion on transmission and illustrate that it is driven by the level of activation induced. Taken together, these findings shed light on why people share content and how to design more effective viral marketing campaigns. },
	author = {Jonah Berger and Katherine L. Milkman},
	doi = {10.1509/jmr.10.0353},
	eprint = {https://doi.org/10.1509/jmr.10.0353},
	journal = {Journal of Marketing Research},
	number = {2},
	pages = {192-205},
	title = {What Makes Online Content Viral?},
	url = {https://doi.org/10.1509/jmr.10.0353},
	volume = {49},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1509/jmr.10.0353}}

@article{lowrey-1998,
	author = {Lowrey, Tina},
	doi = {10.1207/s15327663jcp0702_04},
	journal = {Journal of Consumer Psychology},
	month = {12},
	pages = {187-206},
	title = {The Effects of Syntactic Complexity on Advertising Persuasiveness},
	volume = {7},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1207/s15327663jcp0702_04}}

@article{moran-2016,
	author = {Meghan Bridgid Moran and Melissa Lucas and Kristen Everhart and Ashley Morgan and Erin Prickett},
	doi = {10.1080/17538068.2016.1235531},
	eprint = {https://doi.org/10.1080/17538068.2016.1235531},
	journal = {Journal of Communication in Healthcare},
	number = {3},
	pages = {151-163},
	publisher = {Taylor & Francis},
	title = {What makes anti-vaccine websites persuasive? A content analysis of techniques used by anti-vaccine websites to engender anti-vaccine sentiment},
	url = {https://doi.org/10.1080/17538068.2016.1235531},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/17538068.2016.1235531}}

@article{jin-style-2020,
	author = {Di Jin and Zhijing Jin and Zhiting Hu and Olga Vechtomova and Rada Mihalcea},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2011-00416.bib},
	eprint = {2011.00416},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 27 Apr 2021 16:14:25 +0200},
	title = {Deep Learning for Text Style Transfer: {A} Survey},
	url = {https://arxiv.org/abs/2011.00416},
	volume = {abs/2011.00416},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2011.00416}}

@article{shivkumar2020blabla,
	author = {Shivkumar, Abhishek and Weston, Jack and Lenain, Raphael and Fristed, Emil},
	journal = {arXiv preprint arXiv:2005.10219},
	title = {BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple Languages},
	year = {2020}}

@article{doi:10.1073/pnas.2011695118,
	author = {Olivier Toubia and Jonah Berger and Jehoshua Eliashberg},
	doi = {10.1073/pnas.2011695118},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2011695118},
	journal = {Proceedings of the National Academy of Sciences},
	number = {26},
	pages = {e2011695118},
	title = {How quantifying the shape of stories predicts their success},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2011695118},
	volume = {118},
	year = {2021},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.2011695118},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2011695118}}

@unpublished{spacy2,
	author = {Honnibal, Matthew and Montani, Ines},
	note = {To appear},
	title = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
	year = {2017}}

@inproceedings{danescu-niculescu-mizil-etal-2012-hello,
	address = {Jeju Island, Korea},
	author = {Danescu-Niculescu-Mizil, Cristian and Cheng, Justin and Kleinberg, Jon and Lee, Lillian},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	month = jul,
	pages = {892--901},
	publisher = {Association for Computational Linguistics},
	title = {You Had Me at Hello: How Phrasing Affects Memorability},
	url = {https://aclanthology.org/P12-1094},
	year = {2012},
	bdsk-url-1 = {https://aclanthology.org/P12-1094}}

@article{brown2020language,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal = {Advances in neural information processing systems},
	pages = {1877--1901},
	title = {Language models are few-shot learners},
	volume = {33},
	year = {2020}}

@article{xsede,
	address = {Los Alamitos, CA, USA},
	author = {J. Towns and T. Cockerill and M. Dahan and I. Foster and K. Gaither and A. Grimshaw and V. Hazlewood and S. Lathrop and D. Lifka and G. D. Peterson and R. Roskies and J. Scott and N. Wilkins-Diehr},
	doi = {10.1109/MCSE.2014.80},
	issn = {1558-366X},
	journal = {Computing in Science \& Engineering},
	keywords = {knowledge discovery;scientific computing;digital systems;materials engineering;supercomputers},
	month = {sep},
	number = {05},
	pages = {62-74},
	publisher = {IEEE Computer Society},
	title = {XSEDE: Accelerating Scientific Discovery},
	volume = {16},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/MCSE.2014.80}}

@misc{shah-cardinal-ordinal,
	author = {Shah, Nihar B. and Balakrishnan, Sivaraman and Bradley, Joseph and Parekh, Abhay and Ramchandran, Kannan and Wainwright, Martin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1406.6618},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {When is it Better to Compare than to Score?},
	url = {https://arxiv.org/abs/1406.6618},
	year = {2014},
	bdsk-url-1 = {https://arxiv.org/abs/1406.6618},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1406.6618}}

@article{hovy-1987,
	abstract = {Though much work in natural language generation remains to be done with regard to syntax, the main stumbling block that prevents existing generators from easily producing coherent paragraphs is our lack of understanding of text planning. To remedy this, we should view generations preeminently as a planning task; that is, we should study the goals that underlie text production, the plans that help achieve these goals, and the ways the plans can interact with grammar. A clue to the nature of these goals is the fact that people say the same thing in various ways. They can vary the content and form of their text when they want to convey more information than is contained in the literal meanings of their words. This information expresses the speaker's interpersonal goals toward the hearer and, in general, his perception of the pragmatic aspects of the conversation. This paper identifies goals that arise from pragmatic aspects of the conversation, plans and strategies to achieve them, and how they constrain the decisions a generator has to make during the realization process. To illustrate some of these ideas, a computer program is described which produces stylistically appropriate texts from a single representation under various settings that model pragmatic circumstances.},
	author = {Eduard Hovy},
	doi = {https://doi.org/10.1016/0378-2166(87)90109-3},
	issn = {0378-2166},
	journal = {Journal of Pragmatics},
	number = {6},
	pages = {689-719},
	title = {Generating natural language under pragmatic constraints},
	url = {https://www.sciencedirect.com/science/article/pii/0378216687901093},
	volume = {11},
	year = {1987},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0378216687901093},
	bdsk-url-2 = {https://doi.org/10.1016/0378-2166(87)90109-3}}

@article{yang2018unsupervised,
	author = {Yang, Zichao and Hu, Zhiting and Dyer, Chris and Xing, Eric P and Berg-Kirkpatrick, Taylor},
	journal = {Advances in Neural Information Processing Systems},
	title = {Unsupervised text style transfer using language models as discriminators},
	volume = {31},
	year = {2018}}

@article{shen2017style,
	author = {Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	journal = {Advances in neural information processing systems},
	title = {Style transfer from non-parallel text by cross-alignment},
	volume = {30},
	year = {2017}}

@article{jin2019imat,
	author = {Jin, Zhijing and Jin, Di and Mueller, Jonas and Matthews, Nicholas and Santus, Enrico},
	journal = {arXiv preprint arXiv:1901.11333},
	title = {IMaT: Unsupervised text attribute transfer via iterative matching and translation},
	year = {2019}}

@inproceedings{jhamtani-etal-2017-shakespearizing,
	abstract = {Variations in writing styles are commonly used to adapt the content to a specific context, audience, or purpose. However, applying stylistic variations is still by and large a manual process, and there have been little efforts towards automating it. In this paper we explore automated methods to transform text from modern English to Shakespearean English using an end to end trainable neural model with pointers to enable copy action. To tackle limited amount of parallel data, we pre-train embeddings of words by leveraging external dictionaries mapping Shakespearean words to modern English words as well as additional text. Our methods are able to get a BLEU score of 31+, an improvement of {\mbox{$\approx$}} 6 points above the strongest baseline. We publicly release our code to foster further research in this area.},
	address = {Copenhagen, Denmark},
	author = {Jhamtani, Harsh and Gangal, Varun and Hovy, Eduard and Nyberg, Eric},
	booktitle = {Proceedings of the Workshop on Stylistic Variation},
	doi = {10.18653/v1/W17-4902},
	month = sep,
	pages = {10--19},
	publisher = {Association for Computational Linguistics},
	title = {Shakespearizing Modern Language Using Copy-Enriched Sequence to Sequence Models},
	url = {https://aclanthology.org/W17-4902},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/W17-4902},
	bdsk-url-2 = {https://doi.org/10.18653/v1/W17-4902}}

@inproceedings{li-etal-2018-delete,
	abstract = {We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (\eg sentiment) while preserving its attribute-independent content (\eg {``}screen is just the right size{''} to {``}screen is too small{''}). Our training data includes only sentences labeled with their attribute (\eg positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (\eg {``}too small{''}). Our strongest method extracts content words by deleting phrases associated with the sentence{'}s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22{\%} more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.},
	address = {New Orleans, Louisiana},
	author = {Li, Juncen and Jia, Robin and He, He and Liang, Percy},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	doi = {10.18653/v1/N18-1169},
	month = jun,
	pages = {1865--1874},
	publisher = {Association for Computational Linguistics},
	title = {Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer},
	url = {https://aclanthology.org/N18-1169},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1169},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1169}}

@article{hu-et-al-controllable,
	author = {Zhiting Hu and Zichao Yang and Xiaodan Liang and Ruslan Salakhutdinov and Eric P. Xing},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/HuYLSX17.bib},
	eprint = {1703.00955},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
	title = {Controllable Text Generation},
	url = {http://arxiv.org/abs/1703.00955},
	volume = {abs/1703.00955},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1703.00955}}

@article{guu2018generating,
	author = {Guu, Kelvin and Hashimoto, Tatsunori B and Oren, Yonatan and Liang, Percy},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {437--450},
	publisher = {MIT Press},
	title = {Generating sentences by editing prototypes},
	volume = {6},
	year = {2018}}

@inproceedings{cameron2018assessing,
  title={Assessing the usability of a chatbot for mental health care},
  author={Cameron, Gillian and Cameron, David and Megaw, Gavin and Bond, Raymond and Mulvenna, Maurice and O’Neill, Siobhan and Armour, Cherie and McTear, Michael},
  booktitle={International Conference on Internet Science},
  pages={121--132},
  year={2018},
  organization={Springer}
}
@article{woodward2008more,
  title={More than just jargon--the nature and role of specialist language in learning disciplinary knowledge},
  author={Woodward-Kron, Robyn},
  journal={Journal of English for Academic Purposes},
  volume={7},
  number={4},
  pages={234--249},
  year={2008},
  publisher={Elsevier}
}

@article{okuda2018ai,
  title={AI-based chatbot service for financial industry},
  author={Okuda, Takuma and Shoda, Sanae},
  journal={Fujitsu Scientific and Technical Journal},
  volume={54},
  number={2},
  pages={4--8},
  year={2018}
}

@inproceedings{jin-etal-2019-imat,
	abstract = {Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.},
	address = {Hong Kong, China},
	author = {Jin, Zhijing and Jin, Di and Mueller, Jonas and Matthews, Nicholas and Santus, Enrico},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	doi = {10.18653/v1/D19-1306},
	month = nov,
	pages = {3097--3109},
	publisher = {Association for Computational Linguistics},
	title = {{IM}a{T}: Unsupervised Text Attribute Transfer via Iterative Matching and Translation},
	url = {https://aclanthology.org/D19-1306},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1306},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1306}}

@article{zhang2018style,
	author = {Zhang, Zhirui and Ren, Shuo and Liu, Shujie and Wang, Jianyong and Chen, Peng and Li, Mu and Zhou, Ming and Chen, Enhong},
	journal = {arXiv preprint arXiv:1808.07894},
	title = {Style transfer as unsupervised machine translation},
	year = {2018}}

@article{yang-2022-adversarial,
	abstract = {This article considers the task of text style transfer: transforming a specific style of sentence into another while preserving its style-independent content. A dominate approach to text style transfer is to learn a good content factor of text, define a fixed vector for every style and recombine them to generate text in the required style. In fact, there are a large number of different words to convey the same style from different aspects. Thus, using a fixed vector to represent one style is very inefficient, which causes the weak representation power of the style vector and limits text diversity of the same style. To address this problem, we propose a novel neural generative model called Adversarial Separation Network (ASN), which can learn the content and style vector jointly and the learnt vectors have strong representation power and good interpretabilities. In our method, adversarial learning is implemented to enhance our model's capability of disentangling the two factors. To evaluate our method, we conduct experiments on two benchmark datasets. Experimental results show our method can perform style transfer better than strong comparison systems. We also demonstrate the strong interpretability of the learnt latent vectors.},
	address = {New York, NY, USA},
	articleno = {24},
	author = {Yang, Haitong and Zhou, Guangyou and He, Tingting},
	doi = {10.1145/3472621},
	issn = {2375-4699},
	issue_date = {March 2022},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	keywords = {latent factor, variational autoencoder, adversarial separation network, neural generative model, text style transfer, Adversarial learning, mapping},
	month = {dec},
	number = {2},
	numpages = {14},
	publisher = {Association for Computing Machinery},
	title = {Adversarial Separation Network for Text Style Transfer},
	url = {https://doi-org.proxy2.library.illinois.edu/10.1145/3472621},
	volume = {21},
	year = {2022},
	bdsk-url-1 = {https://doi-org.proxy2.library.illinois.edu/10.1145/3472621},
	bdsk-url-2 = {https://doi.org/10.1145/3472621}}

@inproceedings{jin2020hooks,
	author = {Jin, Di and Jin, Zhijing and Zhou, Tianyi and Orii, Lisa and Szolovits, Peter},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020},
	pages = {5082--5093},
	publisher = {Association for Computational Linguistics},
	title = {Hooks in the Headline: Learning to Generate Headlines with Controlled Styles},
	url = {https://www.aclweb.org/anthology/2020.acl-main.456/},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.acl-main.456/}}

@inproceedings{chawla-yang-2020-semi,
	address = {Online},
	author = {Chawla, Kunal and Yang, Diyi},
	booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
	doi = {10.18653/v1/2020.findings-emnlp.212},
	month = nov,
	pages = {2340--2354},
	publisher = {Association for Computational Linguistics},
	title = {Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization},
	url = {https://aclanthology.org/2020.findings-emnlp.212},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.findings-emnlp.212},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.findings-emnlp.212}}

@inproceedings{rasley-2020-deepspeed,
	abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
	address = {New York, NY, USA},
	author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	date-modified = {2022-06-23 13:04:10 -0500},
	doi = {10.1145/3394486.3406703},
	isbn = {9781450379984},
	keywords = {distributed deep learning, machine learning},
	location = {Virtual Event, CA, USA},
	numpages = {2},
	pages = {3505--3506},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
	url = {https://doi.org/10.1145/3394486.3406703},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3406703}}

@article{gururangan2020pretraining,
	author = {Suchin Gururangan and Ana Marasovic and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2004-10964.bib},
	eprint = {2004.10964},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Sat, 23 Jan 2021 01:12:02 +0100},
	title = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
	url = {https://arxiv.org/abs/2004.10964},
	volume = {abs/2004.10964},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2004.10964}}

@article{bell1984style,
 ISSN = {00474045, 14698013},
 URL = {http://www.jstor.org/stable/4167516},
 abstract = {The style dimension of language variation has not been adequately explained in sociolinguistic theory. Stylistic or intraspeaker variation derives from and mirrors interspeaker variation. Style is essentially speakers' response to their audience. In audience design, speakers accommodate primarily to their addressee. Third persons -- auditors and overhearers -- affect style to a lesser but regular degree. Audience design also accounts for bilingual or bidialectal code choices. Nonaudience factors like topic and setting derive their effect by association with addressee types. These style shifts are mainly responsive -- caused by a situational change. Speakers can also use style as initiative, to redefine the existing situation. Initiative style is primarily referee design: divergence from the addressee and towards an absent reference group. Referee design is especially prevalent in mass communication.},
 author = {Allan Bell},
 journal = {Language in Society},
 number = {2},
 pages = {145--204},
 publisher = {Cambridge University Press},
 title = {Language Style as Audience Design},
 urldate = {2022-06-25},
 volume = {13},
 year = {1984}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{van-noort-2020-automatic,
author = {Guda van Noort and Itai Himelboim and Jolie Martin and Tom Collinger},
title = {Introducing a Model of Automated Brand-Generated Content in an Era of Computational Advertising},
journal = {Journal of Advertising},
volume = {49},
number = {4},
pages = {411-427},
year  = {2020},
publisher = {Routledge},
doi = {10.1080/00913367.2020.1795954},
URL = { 
        https://doi.org/10.1080/00913367.2020.1795954
    
},
eprint = { 
        https://doi.org/10.1080/00913367.2020.1795954
}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{reimers-gurevych-2021-curse,
    title = "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.77",
    doi = "10.18653/v1/2021.acl-short.77",
    pages = "605--611",
    abstract = "Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. We show that this behavior is tightly connected to the number of dimensions of the representations: The lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents",
}

@article{brazinskas2019opinion,
   abstract = {Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the "amount of novelty" going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator ("decoder") has direct access to the text of input reviews through the pointer-generator mechanism. Experiments on Amazon and Yelp datasets, show that setting at test time the review's latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.},
   author = {Arthur Bražinskas and Mirella Lapata and Ivan Titov},
   month = {11},
   title = {Unsupervised Opinion Summarization as Copycat-Review Generation},
   year = {2019},
}
@article{Maulana2021,
   abstract = {Ensemble and auxiliary tasks are both well known to improve the performance of machine learning models when data is limited. However, the interaction between these two methods is not well studied, particularly in the context of deep reinforcement learning. In this paper, we study the effects of ensemble and auxiliary tasks when combined with the deep Q-learning algorithm. We perform a case study on ATARI games under limited data constraint. Moreover, we derive a refined bias-variance-covariance decomposition to analyze the different ways of learning ensembles and using auxiliary tasks, and use the analysis to help provide some understanding of the case study. Our code is open source and available at https://github.com/NUS-LID/RENAULT.},
   author = {Muhammad Rizki Maulana and Wee Sun Lee},
   month = {7},
   title = {Ensemble and Auxiliary Tasks for Data-Efficient Deep Reinforcement Learning},
   year = {2021},
}
@report{Ammanabrolu2021,
   abstract = {Automated story plot generation is the task of generating a coherent sequence of plot events. Causal relations between plot events are believed to increase the perception of story and plot coherence. In this work, we introduce the concept of soft causal relations as causal relations inferred from commonsense reasoning. We demonstrate C2PO, an approach to narrative generation that operationalizes this concept through Causal, Commonsense Plot Ordering. Using human-participant protocols, we evaluate our system against baseline systems with different commonsense reasoning approaches and inductive biases to determine the role of soft causal relations in perceived story quality. Through these studies we also probe the interplay of how changes in commonsense norms across storytelling genres affect perceptions of story quality. 1},
   author = {Prithviraj Ammanabrolu and Wesley Cheung and William Broniec and Mark O Riedl},
   keywords = {Humans and AI: Game Design -- Procedural Content Generation & Storytelling,Speech & Natural Language Processing: Generation},
   title = {Automated Storytelling via Causal, Commonsense Plot Ordering},
   url = {www.aaai.org},
   year = {2021},
}
@article{Asghar2017,
   abstract = {Existing neural conversational models process natural language primarily on a lexico-syntactic level, thereby ignoring one of the most crucial components of human-to-human dialogue: its affective content. We take a step in this direction by proposing three novel ways to incorporate affective/emotional aspects into long short term memory (LSTM) encoder-decoder neural conversation models: (1) affective word embeddings, which are cognitively engineered, (2) affect-based objective functions that augment the standard cross-entropy loss, and (3) affectively diverse beam search for decoding. Experiments show that these techniques improve the open-domain conversational prowess of encoder-decoder networks by enabling them to produce emotionally rich responses that are more interesting and natural.},
   author = {Nabiha Asghar and Pascal Poupart and Jesse Hoey and Xin Jiang and Lili Mou},
   month = {9},
   title = {Affective Neural Response Generation},
   url = {http://arxiv.org/abs/1709.03968},
   year = {2017},
}

@report{Chaganty2016,
   abstract = {How much is 131 million US dollars? To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives, e.g. "$131 million is about the cost to employ everyone in Texas over a lunch period". First, we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.},
   author = {Arun Tejasvi Chaganty and Percy Liang},
   title = {How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions},
   year = {2016},
}
@report{Demiraj2014,
   abstract = {We present a hierarchical convolutional document model with an architecture designed to support introspection of the document structure. Using this model, we show how to use visualisation techniques from the computer vision literature to identify and extract topic-relevant sentences. We also introduce a new scalable evaluation technique for automatic sentence extraction systems that avoids the need for time consuming human annotation of validation data.},
   author = {Alban Demiraj and Misha Denil and Nando De Freitas},
   title = {Extraction of Salient Sentences from Labelled Documents},
   url = {https://www.researchgate.net/publication/269935539},
   year = {2014},
}
@article{Guerini2012,
   abstract = {Reactions to textual content posted in an online social network show different dynamics depending on the linguistic style and readability of the submitted content. Do similar dynamics exist for responses to scientific articles? Our intuition, supported by previous research, suggests that the success of a scientific article depends on its content, rather than on its linguistic style. In this article, we examine a corpus of scientific abstracts and three forms of associated reactions: article downloads, citations, and bookmarks. Through a class-based psycholinguistic analysis and readability indices tests, we show that certain stylistic and readability features of abstracts clearly concur in determining the success and viral capability of a scientific article.},
   author = {Marco Guerini and Alberto Pepe and Bruno Lepri},
   month = {3},
   title = {Do Linguistic Style and Readability of Scientific Abstracts affect their Virality?},
   url = {http://arxiv.org/abs/1203.4238},
   year = {2012},
}
@article{Guerini2015,
   abstract = {While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and non-persuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion.},
   author = {Marco Guerini and Gözde Özbal and Carlo Strapparava},
   month = {8},
   title = {Echoes of Persuasion: The Effect of Euphony in Persuasive Communication},
   url = {http://arxiv.org/abs/1508.05817},
   year = {2015},
}
@report{Huang2018,
   abstract = {Despite myriad efforts in the literature designing neural dialogue generation systems in recent years, very few consider putting restrictions on the response itself. They learn from collections of past responses and generate one based on a given utterance without considering , speech act, desired style or emotion to be expressed. In this research, we address the problem of forcing the dialogue generation to express emotion. We present three models that either concatenate the desired emotion with the source input during the learning, or push the emotion in the decoder. The results, evaluated with an emotion tagger, are encouraging with all three models, but present better outcome and promise with our model that adds the emotion vector in the decoder.},
   author = {Chenyang Huang and Osmar R Za¨ıaneza¨ıane and Amine Trabelsi and Nouha Dziri},
   pages = {49-54},
   title = {Automatic Dialogue Generation with Expressed Emotions},
   url = {http://www.cs.ualberta.ca/},
   year = {2018},
}
@report{Lauscher2018,
   abstract = {Argumentation is an essential feature of scientific language. We present an annotation study resulting in a corpus of scientific publications annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus , already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between argumentation and other rhetorical aspects of scientific writing, such as discourse roles and citation contexts.},
   author = {Anne Lauscher and Goran Glavaš and Simone Paolo Ponzetto},
   pages = {40-46},
   title = {An Argument-Annotated Corpus of Scientific Publications},
   url = {http://data.dws.informatik.},
   year = {2018},
}
@article{Jaradeh2021,
   abstract = {In the last decade, a large number of Knowledge Graph (KG) information extraction approaches were proposed. Albeit effective, these efforts are disjoint, and their collective strengths and weaknesses in effective KG information extraction (IE) have not been studied in the literature. We propose Plumber, the first framework that brings together the research community's disjoint IE efforts. The Plumber architecture comprises 33 reusable components for various KG information extraction subtasks, such as coreference resolution, entity linking, and relation extraction. Using these components,Plumber dynamically generates suitable information extraction pipelines and offers overall 264 distinct pipelines.We study the optimization problem of choosing suitable pipelines based on input sentences. To do so, we train a transformer-based classification model that extracts contextual embeddings from the input and finds an appropriate pipeline. We study the efficacy of Plumber for extracting the KG triples using standard datasets over two KGs: DBpedia, and Open Research Knowledge Graph (ORKG). Our results demonstrate the effectiveness of Plumber in dynamically generating KG information extraction pipelines,outperforming all baselines agnostics of the underlying KG. Furthermore,we provide an analysis of collective failure cases, study the similarities and synergies among integrated components, and discuss their limitations.},
   author = {Mohamad Yaser Jaradeh and Kuldeep Singh and Markus Stocker and Andreas Both and Sören Auer},
   month = {2},
   title = {Better Call the Plumber: Orchestrating Dynamic Information Extraction Pipelines},
   url = {http://arxiv.org/abs/2102.10966},
   year = {2021},
}
@article{Niculae2017,
   abstract = {We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.},
   author = {Vlad Niculae and Joonsuk Park and Claire Cardie},
   month = {4},
   title = {Argument Mining with Structured SVMs and RNNs},
   url = {http://arxiv.org/abs/1704.06869},
   year = {2017},
}
@article{Niklaus2019,
   abstract = {We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.},
   author = {Christina Niklaus and Matthias Cetto and Andre Freitas and Siegfried Handschuh},
   month = {9},
   title = {DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German},
   url = {http://arxiv.org/abs/1909.12140},
   year = {2019},
}
@report{Orbach2020,
   abstract = {Recent advancements in self-attention neural network architectures have raised the bar for open-ended text generation. Yet, while current methods are capable of producing a coherent text which is several hundred words long, attaining control over the content that is being generated-as well as evaluating it-are still open questions. We propose a controlled generation task which is based on expanding a sequence of facts, expressed in natural language, into a longer narrative. We introduce human-based evaluation metrics for this task, as well as a method for deriving a large training dataset. We evaluate three methods on this task, based on fine-tuning pre-trained models. We show that while auto-regressive, unidirectional Language Models such as GPT2 produce better fluency, they struggle to adhere to the requested facts. We propose a plan-and-cloze model (using fine-tuned XLNet) which produces competitive fluency while adhering to the requested content.},
   author = {Eyal Orbach and Yoav Goldberg},
   pages = {2329-2345},
   publisher = {Online},
   title = {Facts2Story: Controlling Text Generation by Key Facts},
   url = {https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze},
   year = {2020},
}
@report{Pichotta2016,
   abstract = {There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on struc-tured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.},
   author = {Karl Pichotta and Raymond J Mooney},
   pages = {279-289},
   title = {Using Sentence-Level LSTM Language Models for Script Inference},
   year = {2016},
}
@article{Niklaus2021,
   abstract = {We present a context-preserving text simplification (TS) approach that recursively splits and rephrases complex English sentences into a semantic hierarchy of simplified sentences. Using a set of linguistically principled transformation patterns, input sentences are converted into a hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. Hence, as opposed to previously proposed sentence splitting approaches, which commonly do not take into account discourse-level aspects, our TS approach preserves the semantic relationship of the decomposed constituents in the output. A comparative analysis with the annotations contained in the RST-DT shows that we are able to capture the contextual hierarchy between the split sentences with a precision of 89% and reach an average precision of 69% for the classification of the rhetorical relations that hold between them.},
   author = {Christina Niklaus and Matthias Cetto and André Freitas and Siegfried Handschuh},
   month = {5},
   title = {Context-Preserving Text Simplification},
   url = {http://arxiv.org/abs/2105.11178},
   year = {2021},
}
@report{Santhanam2019,
   abstract = {Emotional language generation is one of the keys to human-like artificial intelligence. Humans use different type of emotions depending on the situation of the conversation. Emotions also play an important role in mediating the engagement level with conversational partners. However, current conversational agents do not effectively account for emotional content in the language generation process. To address this problem, we develop a language modeling approach that generates af-fective content when the dialogue is situated in a given context. We use the recently released Empathetic-Dialogues corpus to build our models. Through detailed experiments, we find that our approach outperforms the state-of-the-art method on the perplexity metric by about 5 points and achieves a higher BLEU metric score.},
   author = {Sashank Santhanam and Samira Shaikh},
   title = {Emotional Neural Language Generation Grounded in Situational Contexts},
   url = {https://github.com/huggingface/},
   year = {2019},
}
@generic{Salazar2021,
   author = {James W. Salazar and Jennifer D. Claytor and Anand R. Habib and Vinay Guduguntla and Rita F. Redberg},
   doi = {10.1001/jamainternmed.2021.2363},
   issn = {21686114},
   issue = {9},
   journal = {JAMA Internal Medicine},
   month = {9},
   pages = {1248-1251},
   pmid = {34115835},
   publisher = {American Medical Association},
   title = {Gender, Race, Ethnicity, and Sexual Orientation of Editors at Leading Medical and Scientific Journals: A Cross-sectional Survey},
   volume = {181},
   year = {2021},
}
@article{See2019,
   abstract = {Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.},
   author = {Abigail See and Aneesh Pappu and Rohun Saxena and Akhila Yerukola and Christopher D. Manning},
   month = {9},
   title = {Do Massively Pretrained Language Models Make Better Storytellers?},
   url = {http://arxiv.org/abs/1909.10705},
   year = {2019},
}
@report{Ippolito2019,
   abstract = {Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.},
   author = {Daphne Ippolito and David Grangier and Chris Callison-Burch and Douglas Eck},
   pages = {37-43},
   title = {Unsupervised Hierarchical Story Infilling},
   year = {2019},
}
@report{Guan2021,
   abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in mod-eling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.},
   author = {Jian Guan and Xiaoxi Mao and Changjie Fan and Zitao Liu and Wenbiao Ding and Minlie Huang},
   pages = {6379-6393},
   title = {Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence},
   url = {https://github.com/},
   year = {2021},
}
@report{Sugiyama2021,
   abstract = {Although many end-to-end context-aware neu-ral machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sen-tential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contex-tual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.},
   author = {Amane Sugiyama and Naoki Yoshinaga},
   pages = {5781-5791},
   title = {Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model},
   year = {2021},
}
@inproceedings{Sutoyo2013,
   abstract = {The increasing role of Wikipedia as a source of human-readable knowledge is evident as it contains an enormous amount of high quality information written in natural language by human authors. However, querying this information using traditional keyword based approaches requires often a time-consuming, iterative process to explore the document collection to find the information of interest. Therefore, a structured representation of information and queries would be helpful to be able to directly query for the relevant information. An important challenge in this context is the extraction of structured information from unstructured knowledge bases which is addressed by Information Extraction (IE) systems. However, these systems struggle with the complexity of natural language and produce frequently unsatisfying results. In addition to the plain natural language text,Wikipedia contains links between documents which directly link a term of one document to another document. In our approach for fact extraction fromWikipedia, we consider these links as an important indicator for the relevance of the linked information. Thus, our proposed system FactRunner focusses on extracting structured information from sentences containing such links. We show that a natural language parser combined with Wikipedia markup can be exploited for extracting facts in form of triple statements with a high accuracy. Copyright © 2013 SCITEPRESS.},
   author = {Rhio Sutoyo and Christoph Quix and Fisnik Kastrati},
   doi = {10.5220/0004375604230432},
   isbn = {9789898565549},
   journal = {WEBIST 2013 - Proceedings of the 9th International Conference on Web Information Systems and Technologies},
   keywords = {Information extraction,Semantic search},
   pages = {423-432},
   title = {FactRunner: Fact extraction over wikipedia},
   year = {2013},
}
@inproceedings{Sheng2019,
   abstract = {In today's interconnected world, there is an endless 24/7 stream of new articles appearing online. Faced with these overwhelming amounts of data, it is often helpful to consider only the key entities and concepts and their relationships. This is challenging, as relevant connections may be spread across a number of disparate articles and sources. In this paper, we propose a unified framework to aid users in quickly discerning salient connections and facts from a set of related documents, and presents the resulting information in a graph-based visualization. Specifically, given a set of relevant documents as input, we firstly extract candidate facts from above sources by exploiting Open Information Extraction (Open IE) approaches. Then, we design a Two-Stage Candidate Triple Filtering (TCTF) approach based on a self-training framework to maintain only coherent facts associated with the specified document topic from the candidates and connect them in the form of an initial graph. We further construct this graph by a heuristic to ensure the final conceptual graph only consist of facts likely to represent meaningful and salient relationships, which users may explore graphically. The experiments on two real-world datasets illustrate that our extraction approach achieves 2.4% higher on the average of F-score over several OpenIE baselines. We also further present an empirical evaluation of the quality of the final generated conceptual graph towards different topics on its coverage rate of topic entities and concepts, confidence score, and the compatibility of involved facts. Experimental results show the effectiveness of our proposed approach.},
   author = {Yongpan Sheng and Zenglin Xu},
   doi = {10.1007/978-3-030-26072-9_30},
   isbn = {9783030260712},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Graph-based visualization,Multi-Document relationship mining},
   pages = {414-430},
   publisher = {Springer Verlag},
   title = {Coherence and salience-based multi-document relationship mining},
   volume = {11641 LNCS},
   year = {2019},
}
@article{Wang2020,
   abstract = {We propose a method for controlled narrative/story generation where we are able to guide the model to produce coherent narratives with user-specified target endings by interpolation: for example, we are told that Jim went hiking and at the end Jim needed to be rescued, and we want the model to incrementally generate steps along the way. The core of our method is an interpolation model based on GPT-2 which conditions on a previous sentence and a next sentence in a narrative and fills in the gap. Additionally, a reranker helps control for coherence of the generated text. With human evaluation, we show that ending-guided generation results in narratives which are coherent, faithful to the given ending guide, and require less manual effort on the part of the human guide writer than past approaches.},
   author = {Su Wang and Greg Durrett and Katrin Erk},
   month = {8},
   title = {Narrative Interpolation for Generating and Understanding Stories},
   url = {http://arxiv.org/abs/2008.07466},
   year = {2020},
}
@article{Xu2020,
   abstract = {The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.},
   author = {Yumo Xu and Mirella Lapata},
   month = {12},
   title = {Generating Query Focused Summaries from Query-Free Resources},
   url = {http://arxiv.org/abs/2012.14774},
   year = {2020},
}
@report{Inkpen2009,
   abstract = {This paper explores the task of automatic emotion analysis and generation in texts. We present preliminary results for the task of classifying texts by classes of emotions. Then, we present detailed experiments in classifying texts by classes of mood. We propose a novel approach that uses the hierarchy of possible moods in order to achieve better results than a standard flat classification. We also show that using sentiment orientation features improves the performance of classification. At the end, the possibility of generating texts that express specific emotions is discussed.},
   author = {Diana Inkpen and Fazel Keshtkar and Diman Ghazi},
   title = {Analysis and generation of emotion in texts},
   url = {https://www.researchgate.net/publication/228675951},
   year = {2009},
}
@report{Yang2019,
   abstract = {Modeling what makes a request persuasive-eliciting the desired response from a reader-is critical to the study of propaganda, behav-ioral economics, and advertising. Yet current models can't quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persua-siveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies-offering increased interpretability of persuasive speech-and has applications for other situations with document-level supervision but only partial sentence supervision.},
   author = {Diyi Yang and Jiaao Chen and Zichao Yang and Dan Jurafsky and Eduard Hovy},
   title = {Let's Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms},
   url = {https://www.kiva.org/},
   year = {2019},
}
@report{Xu2019,
   abstract = {In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification , fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation.},
   author = {Qiongkai Xu and Chenchen Xu and Lizhen Qu},
   pages = {13-18},
   title = {ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation},
   url = {https://github.com/google-research/},
   year = {2019},
}
@report{Accuosto2021,
   abstract = {In this work we propose to tackle the limitations posed by the lack of annotated data for argument mining in scientific texts by annotating argumentative units and relations in research abstracts in two scientific domains. We evaluate our annotations by computing inter-annotator agreements, which range from moderate to substantial according to the difficulty level of the tasks and domains. We use our newly annotated corpus to fine-tune BERT-based models for argument mining in single and multi-task settings, finally exploring the adaptation of models trained in one scientific discipline (computational linguistics) to predict the argumentative structure of abstracts in a different one (biomedicine).},
   author = {Pablo Accuosto and Mariana Neves and Horacio Saggion},
   keywords = {argument mining,domain adaptation,scientific corpora,transformer models},
   title = {Argumentation mining in scientific literature: From computational linguistics to biomedicine},
   url = {http://ceur-ws.org},
   year = {2021},
}
@inproceedings{Meng2017,
   abstract = {Speaker change detection (SCD) is an important task in dialog modeling. Our paper addresses the problem of text-based SCD, which differs from existing audio-based studies and is useful in various scenarios, for example, processing dialog transcripts where speaker identities are missing (e.g., OpenSubtitle), and enhancing audio SCD with textual information. We formulate text-based SCD as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms non-attention neural networks.},
   author = {Zhao Meng and Lili Mou and Zhi Jin},
   doi = {10.1145/3132847.3133110},
   isbn = {9781450349185},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   month = {11},
   pages = {2203-2206},
   publisher = {Association for Computing Machinery},
   title = {Hierarchical RNN with static sentence-level attention for text-based speaker change detection},
   volume = {Part F131841},
   year = {2017},
}
@report{Song2019,
   abstract = {It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.},
   author = {Zhenqiao Song and Xiaoqing Zheng and Lu Liu and Mu Xu and Xuanjing Huang},
   pages = {3685-3695},
   title = {Generating Responses with a Specific Emotion in Dialog},
   year = {2019},
}
@report{Yu2019,
   abstract = {Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazon-selected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction .},
   author = {Dian Yu and Michelle Cohn and Yi Mang Yang and Chun-Yen Chen and Weiming Wen and Jiaping Zhang and Mingyang Zhou and Kevin Jesse and Austin Chau and Antara Bhowmick and Shreenath Iyer and Giritheja Sreenivasulu and Sam Davidson and Ashwin Bhandare and Zhou Yu},
   pages = {79-84},
   title = {Gunrock: A Social Bot for Complex and Engaging Long Conversations},
   url = {https://www.evi.com/},
   year = {2019},
}
@report{Atalay2019,
   abstract = {Acknowledgments:},
   author = {A Selin Atalay and Siham El Kihal and Florian Ellsaesser},
   title = {The Role of Syntax in Persuasive Marketing Communication: A Natural Language Processing Approach},
   url = {https://ssrn.com/abstract=3410351},
   year = {2019},
}
@article{Zhang2021,
   abstract = {Automatic summarization systems provide an effective solution to today's unprecedented growth of textual data. For real-world tasks, such as data mining and information retrieval, the factual correctness of generated summary is critical. However, existing models usually focus on improving the informativeness rather than optimizing factual correctness. In this work, we present a Fact-Aware Reinforced Abstractive Sentence Summarization framework to improve the factual correctness of neural abstractive summarization models, denoted as FAR-ASS. Specifically, we develop an automatic fact extraction scheme leveraging OpenIE (Open Information Extraction) and dependency parser tools to extract structured fact tuples. Then, to quantitatively evaluate the factual correctness, we define a factual correctness score function that considers the factual accuracy and factual redundancy. We further propose to adopt reinforcement learning to improve readability and factual correctness by jointly optimizing a mixed-objective learning function. We use the English Gigaword and DUC 2004 datasets to evaluate our model. Experimental results show that compared with competitive models, our model significantly improves the factual correctness and readability of generated summaries, and also reduces duplicates while improving the informativeness.},
   author = {Mengli Zhang and Gang Zhou and Wanting Yu and Wenfen Liu},
   doi = {10.1016/j.ipm.2020.102478},
   issn = {03064573},
   issue = {3},
   journal = {Information Processing and Management},
   keywords = {Abstractive summarization,Factual correctness,NLP,Reinforcement learning,Sequence-to-sequence},
   month = {5},
   publisher = {Elsevier Ltd},
   title = {FAR-ASS: Fact-aware reinforced abstractive sentence summarization},
   volume = {58},
   year = {2021},
}
@inproceedings{Xin2021,
   abstract = {Knowledge extraction has become a hot topic recently with the increasing number of applications needed for large-scale knowledge bases (KBs), such as semantic search and QA systems. The goal of knowledge extraction is to extract relations and their arguments from natural language text. Recent research proposes two kinds of solutions. The first one, called Closed IE, tries to construct KB through predefined features or rules with respect to a specific domain. It requires specifying the interested predicates in advance, which restricts its application to the domains where prior knowledge about the interested predicates must be given. The second one, called Open IE, tries to extract facts by using the parsing structure from the unstructured text. However, they cannot avoid extracting redundant facts. Such extractions can hardly be directly used to populate the existing KB. Moreover, many correct extractions are not relevant to the document, which limits the applications to understand the essential information that the document conveys. In this paper, we propose an end-to-end system which takes a target incomplete KB and documents as input. It first performs joint entity and relation linking to the existing KB based on both contexts of document and background KB information. Then it summarizes the extracted facts by considering the relevance to the document and the diversity between them. Extensive experiments over real datasets demonstrate the effectiveness and efficiency of the proposed methods.},
   author = {Hao Xin and Xueling Lin and Lei Chen},
   doi = {10.1109/ICDE51399.2021.00192},
   isbn = {9781728191843},
   issn = {10844627},
   journal = {Proceedings - International Conference on Data Engineering},
   keywords = {Entity Linking,Graph Based Ranking,Knowledge Base,Relation Linking,Semantic graph},
   month = {4},
   pages = {2009-2014},
   publisher = {IEEE Computer Society},
   title = {CaSIE: Canonicalize and informative selection of the OpenIE system},
   volume = {2021-April},
   year = {2021},
}
@article{Fan2019,
   abstract = {Writers generally rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.},
   author = {Angela Fan and Mike Lewis and Yann Dauphin},
   month = {2},
   title = {Strategies for Structuring Story Generation},
   url = {http://arxiv.org/abs/1902.01109},
   year = {2019},
}
@article{Ponza2019,
   abstract = {We study the problem of entity salience by proposing the design and implementation of Swat, a system that identifies the salient Wikipedia entities occurring in an input document. Swat consists of several modules that are able to detect and classify on-the-fly Wikipedia entities as salient or not, based on a large number of syntactic, semantic, and latent features properly extracted via a supervised process, which has been trained over millions of examples drawn from the New York Times corpus. The validation process is performed through a large experimental assessment, eventually showing that Swat improves known solutions over all publicly available datasets. We release Swat via an API that we describe and comment in the paper to ease its use in other software.},
   author = {Marco Ponza and Paolo Ferragina and Francesco Piccinno},
   doi = {10.1111/coin.12216},
   issn = {14678640},
   issue = {4},
   journal = {Computational Intelligence},
   keywords = {Wikipedia,entity linking,entity salience,information retrieval,machine learning,natural language processing},
   month = {11},
   pages = {858-890},
   publisher = {Blackwell Publishing Inc.},
   title = {Swat: A system for detecting salient Wikipedia entities in texts},
   volume = {35},
   year = {2019},
}
@article{Tan2020,
   abstract = {Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.},
   author = {Bowen Tan and Zichao Yang and Maruan AI-Shedivat and Eric P. Xing and Zhiting Hu},
   month = {6},
   title = {Progressive Generation of Long Text with Pretrained Language Models},
   url = {http://arxiv.org/abs/2006.15720},
   year = {2020},
}
@article{Zhou2017,
   abstract = {Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.},
   author = {Hao Zhou and Minlie Huang and Tianyang Zhang and Xiaoyan Zhu and Bing Liu},
   month = {4},
   title = {Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory},
   url = {http://arxiv.org/abs/1704.01074},
   year = {2017},
}
@report{Heilman2011,
   abstract = {Texts with potential educational value are becoming available through the Internet (e.g., Wikipedia, news services). However, using these new texts in classrooms introduces many challenges, one of which is that they usually lack practice exercises and assessments. Here, we address part of this challenge by automating the creation of a specific type of assessment item. Specifically, we focus on automatically generating factual WH questions. Our goal is to create an automated system that can take as input a text and produce as output questions for assessing a reader's knowledge of the information in the text. The questions could then be presented to a teacher, who could select and revise the ones that he or she judges to be useful. After introducing the problem, we describe some of the computational and linguistic challenges presented by factual question generation. We then present an implemented system that leverages existing natural language processing techniques to address some of these challenges. The system uses a combination of manually encoded transformation rules and a statistical question ranker trained on a tailored dataset of labeled system output. We present experiments that evaluate individual components of the system as well as the system as a whole. We found, among other things, that the question ranker roughly doubled the acceptability rate of top-ranked questions. In a user study, we tested whether K-12 teachers could efficiently create factual questions by selecting and revising suggestions from the system. Offering automatic suggestions reduced the time and effort spent by participants, though it also affected the types of questions that were created. This research supports the idea that natural language processing can help teachers efficiently create instructional content. It provides solutions to some of the major challenges in question generation and an analysis and better understanding of those that remain.},
   author = {Michael Heilman and Vincent Aleven and William W Cohen and Diane J Litman and Noah A Smith},
   title = {Automatic Factual Question Generation from Text Thesis Committee},
   url = {www.lti.cs.cmu.edu},
   year = {2011},
}
@article{Rashkin2020,
   abstract = {We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.},
   author = {Hannah Rashkin and Asli Celikyilmaz and Yejin Choi and Jianfeng Gao},
   month = {4},
   title = {PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking},
   url = {http://arxiv.org/abs/2004.14967},
   year = {2020},
}
@report{Lamm2018,
   abstract = {To understand a sentence like "whereas only 10% of White Americans live at or below the poverty line, 28% of African Americans do" it is important not only to identify individual facts, e.g., poverty rates of distinct demographic groups, but also the higher-order relations between them, e.g., the disparity between them. In this paper, we propose the task of Textual Analogy Parsing (TAP) to model this higher-order meaning. The output of TAP is a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation can enable new applications that rely on discourse understanding such as automated chart generation from quantitative text. We present a new dataset for TAP, baselines, and a model that successfully uses an ILP to enforce the structural constraints of the problem.},
   author = {Matthew Lamm and Arun Tejasvi Chaganty and Christopher D Manning and Dan Jurafsky and Percy Liang},
   title = {Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts},
   year = {2018},
}
@report{sari2019discourse,
   abstract = {this study analyzed the discourse analysis on online media "detik.com". It uses Fairlough theory in the analyses which try to see text in three dimension : text, discourse and social practice dimension. The method used to analyze the data is pragmatic identity method by Sudaryanto. The result shows that the journalist use some figurative language to attract's reader attention. In arranging the headline, the news writer some used tendentious and suggestive vocabulary.},
   author = {Desi Ratna Sari},
   keywords = {discourse,headline,online media},
   title = {Discourse Analysis on Headline News},
   year = {2019},
}
@report{John2019,
   abstract = {This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives , for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches. 1},
   author = {Vineet John and Lili Mou and Hareesh Bahuleyan and Olga Vechtomova},
   pages = {424-434},
   title = {Disentangled Representation Learning for Non-Parallel Text Style Transfer},
   url = {https://sites.google.com/view/},
   year = {2019},
}
@report{Mai2018,
   abstract = {The past decades have witnessed a rapid increase in the global scientific output as measured by publish papers. Exploring a scientific field and searching for relevant papers and authors seems like a needle-in-a-haystack problem. Although many academic search engines have been developed to accelerate this retrieval process, most of them rely on content-based methods and feature engineering. In this work, we present an entity retrieval prototype system on top of IOS Press LD Connect which utilizes both textual and structure information. Paragraph vector and knowledge graph embedding are used to embed papers and entities into low dimensional hidden space. Next, the semantic similarity between papers and entities can be measured based on the learned embedding models. Two benchmark datasets have been collected from Semantic Scholar and DBLP to evaluate the performance of our entity retrieval models. Results show that paragraph vectors are effective at capturing the similarity and relatedness among papers and knowledge graph embedding models can preserve the inherent structure of the original knowledge graph and hence assist in link prediction tasks such as co-author inference.},
   author = {Gengchen Mai and Krzysztof Janowicz and Bo Yan},
   keywords = {Embedding,Entity,Graph,Knowledge,Paper,Paragraph,Recommender,Retrieval ·,System ·,Vector ·},
   title = {Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines},
   url = {http://stko.geog.ucsb.edu/},
   year = {2018},
}
@article{koncel2019generation,
   abstract = {Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.},
   author = {Rik Koncel-Kedziorski and Dhanush Bekal and Yi Luan and Mirella Lapata and Hannaneh Hajishirzi},
   month = {4},
   title = {Text Generation from Knowledge Graphs with Graph Transformers},
   url = {http://arxiv.org/abs/1904.02342},
   year = {2019},
}
@article{Moryossef2019,
   abstract = {Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system's reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.},
   author = {Amit Moryossef and Yoav Goldberg and Ido Dagan},
   month = {4},
   title = {Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation},
   url = {http://arxiv.org/abs/1904.03396},
   year = {2019},
}
@report{Yang2018,
   abstract = {Binary classifiers are often employed as discriminators in GAN-based unsupervised style transfer systems to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with this approach is that the error signal provided by the discriminator can be unstable and is sometimes insufficient to train the generator to produce fluent language. In this paper, we propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process. We train the generator to minimize the negative log likelihood (NLL) of generated sentences, evaluated by the language model. By using a continuous approximation of discrete sampling under the generator, our model can be trained using back-propagation in an end-to-end fashion. Moreover, our empirical results show that when using a language model as a structured discriminator, it is possible to forgo adversarial steps during training, making the process more stable. We compare our model with previous work that uses convolutional networks (CNNs) as discriminators, as well as a broad set of other approaches. Results show that the proposed method achieves improved performance on three tasks: word substitution decipherment, sentiment modification, and related language translation.},
   author = {Zichao Yang and Zhiting Hu and Chris Dyer and Eric P Xing and Taylor Berg-Kirkpatrick},
   title = {Unsupervised Text Style Transfer using Language Models as Discriminators},
   year = {2018},
}
@article{Yang2022,
   abstract = {This article considers the task of text style transfer: transforming a specific style of sentence into another while preserving its style-independent content. A dominate approach to text style transfer is to learn a good content factor of text, define a fixed vector for every style and recombine them to generate text in the required style. In fact, there are a large number of different words to convey the same style from different aspects. Thus, using a fixed vector to represent one style is very inefficient, which causes the weak representation power of the style vector and limits text diversity of the same style. To address this problem, we propose a novel neural generative model called Adversarial Separation Network (ASN), which can learn the content and style vector jointly and the learnt vectors have strong representation power and good interpretabilities. In our method, adversarial learning is implemented to enhance our model's capability of disentangling the two factors. To evaluate our method, we conduct experiments on two benchmark datasets. Experimental results show our method can perform style transfer better than strong comparison systems. We also demonstrate the strong interpretability of the learnt latent vectors.},
   author = {Haitong Yang and Guangyou Zhou and Tingting He},
   doi = {10.1145/3472621},
   issn = {23754702},
   issue = {2},
   journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
   keywords = {Adversarial learning,adversarial separation network,latent factor,mapping,neural generative model,text style transfer,variational autoencoder},
   month = {3},
   publisher = {Association for Computing Machinery},
   title = {Adversarial Separation Network for Text Style Transfer},
   volume = {21},
   year = {2022},
}
@article{Li2019,
   abstract = {Text style transfer without parallel data has achieved some practical success. However, in the scenario where less data is available, these methods may yield poor performance. In this paper, we examine domain adaptation for text style transfer to leverage massively available data from other domains. These data may demonstrate domain shift, which impedes the benefits of utilizing such data for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to: (i) distinguish stylized information and generic content information; (ii) maximally preserve content information; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines.},
   author = {Dianqi Li and Yizhe Zhang and Zhe Gan and Yu Cheng and Chris Brockett and Ming-Ting Sun and Bill Dolan},
   month = {8},
   title = {Domain Adaptive Text Style Transfer},
   url = {http://arxiv.org/abs/1908.09395},
   year = {2019},
}
@article{Jin2020,
   abstract = {Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this paper, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task. Our curated paper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey},
   author = {Di Jin and Zhijing Jin and Zhiting Hu and Olga Vechtomova and Rada Mihalcea},
   month = {11},
   title = {Deep Learning for Text Style Transfer: A Survey},
   url = {http://arxiv.org/abs/2011.00416},
   year = {2020},
}
@article{Liu2022,
   abstract = {Text style transfer is an important task in controllable language generation. Supervised approaches have pushed performance improvement on style-oriented rewriting such as formality conversion. However, challenges remain due to the scarcity of large-scale parallel data in many domains. While unsupervised approaches do not rely on annotated sentence pairs for each style, they are often plagued with instability issues such as mode collapse or quality degradation. To take advantage of both supervised and unsupervised paradigms and tackle the challenges, in this work, we propose a semi-supervised framework for text style transfer. First, the learning process is bootstrapped with supervision guided by automatically constructed pseudo-parallel pairs using lexical and semantic-based methods. Then the model learns from unlabeled data via reinforcement rewards. Specifically, we propose to improve the sequence-to-sequence policy gradient via stepwise reward optimization, providing fine-grained learning signals and stabilizing the reinforced learning process. Experimental results show that the proposed approach achieves state-of-the-art performance on multiple datasets, and produces effective generation with as minimal as 10\% of training data.},
   author = {Zhengyuan Liu and Nancy F. Chen},
   month = {5},
   title = {Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer},
   url = {http://arxiv.org/abs/2205.09324},
   year = {2022},
}
@report{Kiesler2011,
   author = {Sara Kiesler and Robert Kraut and Paul Resnick and Aniket Kittur},
   title = {Regulating Behavior in Online Communities},
   year = {2011},
}
@report{Grimmelmann2015,
   abstract = {TL;DR-On a Friday in 2005, the Los Angeles Times launched an experiment: a "wikitorial" on the Iraq War that any of the paper's readers could edit. By Sunday, the experiment had ended in abject failure: vandals overran it with crude pro-fanity and graphic pornography. The wikitorial took its inspiration and its technology from Wikipedia, but missed something essential about how the "the free encyclopedia that anyone can edit" staves off abuse while maintaining its core commitment to open participation. The difference is moderation: the governance mechanisms that structure participation in a community to facilitate cooperation and prevent abuse. Town meetings have moderators, and so do online communities. A community's moderators can promote posts or hide them, honor posters or shame them, recruit users or ban them. Their decisions influence what is seen, what is valued, what is said. They create the conditions under which cooperation is possible. This Article provides a novel taxonomy of moderation in online communities. It breaks down the basic verbs of moderation exclusion , pricing, organizing, and norm-setting-and shows how they help communities walk the tightrope between the chaos of too much freedom and the sterility of too much control. Scholars studying the commons can learn from moderation , and so can policy-makers debating the regulation of online communities.},
   author = {James Grimmelmann and Aislinn Black and B J Ard and Jack Bal-Kin and Shyam Balganesh and Nicholas Bramble and Danielle Citron and Anne Huang and Matt Haughey and Sarah Jeong and Amy Kapczynski and David Krinsky and Chris Ri-Ley and Henry Smith and Jessamyn West and Steven Wu},
   title = {THE VIRTUES OF MODERATION},
   url = {http://www.aaronsw.com/weblog/morewikipedias},
   year = {2015},
}
@article{Sun2019,
   abstract = {We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL is the first provably efficient algorithm in ILFO setting, which learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results, which typically only consider tabular reinforcement learning settings or settings that require access to a near-optimal reset distribution. We also investigate the extension of FAIL in a model-based setting. Finally we demonstrate the efficacy of FAIL on multiple OpenAI Gym control tasks.},
   author = {Wen Sun and Anirudh Vemula and Byron Boots and J. Andrew Bagnell},
   month = {5},
   title = {Provably Efficient Imitation Learning from Observation Alone},
   url = {http://arxiv.org/abs/1905.10948},
   year = {2019},
}
@report{Chang2015,
   abstract = {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently , LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.},
   author = {Kai-Wei Chang and Akshay Krishnamurthy and Alekh Agarwal and John Langford},
   title = {Learning to Search Better than Your Teacher},
   year = {2015},
}
@report{Whitfield2020,
   abstract = {Classification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training.},
   author = {Dewayne Whitfield},
   title = {Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models},
   year = {2020},
}
@article{Cheng2018,
   abstract = {Sample efficiency is critical in solving real-world reinforcement learning problems, where agent-environment interactions can be costly. Imitation learning from expert advice has proved to be an effective strategy for reducing the number of interactions required to train a policy. Online imitation learning, which interleaves policy evaluation and policy optimization, is a particularly effective technique with provable performance guarantees. In this work, we seek to further accelerate the convergence rate of online imitation learning, thereby making it more sample efficient. We propose two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based on solving variational inequalities and MoBIL-Prox based on stochastic first-order updates. These two methods leverage a model to predict future gradients to speed up policy learning. When the model oracle is learned online, these algorithms can provably accelerate the best known convergence rate up to an order. Our algorithms can be viewed as a generalization of stochastic Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style analysis of performance.},
   author = {Ching-An Cheng and Xinyan Yan and Evangelos A. Theodorou and Byron Boots},
   month = {6},
   title = {Accelerating Imitation Learning with Predictive Models},
   url = {http://arxiv.org/abs/1806.04642},
   year = {2018},
}
@article{Oraby2019,
   abstract = {Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics.},
   author = {Shereen Oraby and Vrindavan Harrison and Abteen Ebrahimi and Marilyn Walker},
   month = {6},
   title = {Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG},
   url = {http://arxiv.org/abs/1906.01334},
   year = {2019},
}
@article{Ye2021,
   abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
   author = {Weirui Ye and Shaohuai Liu and Thanard Kurutach and Pieter Abbeel and Yang Gao},
   month = {10},
   title = {Mastering Atari Games with Limited Data},
   url = {http://arxiv.org/abs/2111.00210},
   year = {2021},
}
@inproceedings{Mahar2018,
   abstract = {Communication platforms have struggled to provide effective tools for people facing harassment online. We conducted interviews with 18 recipients of online harassment to understand their strategies for coping, finding that they often resorted to asking friends for help. Inspired by these findings, we explore the feasibility of friendsourced moderation as a technique for combating online harassment. We present Squadbox, a tool to help recipients of email harassment coordinate a "squad" of friend moderators to shield and support them during attacks. Friend moderators intercept email from strangers and can reject, organize, and redirect emails, as well as collaborate on filters. Squadbox is designed to let its users implement highly customized workflows, as we found in interviews that harassment and preferences for mitigating it vary widely. We evaluated Squadbox on five pairs of friends in a field study, finding that participants could comfortably navigate around privacy and personalization concerns.},
   author = {Kaitlin Mahar and Amy X. Zhang and David Karger},
   doi = {10.1145/3173574.3174160},
   isbn = {9781450356206},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Crowdsourcing,Email,Friendsourcing,Moderation,Online harassment,Private messages,Social media},
   month = {4},
   note = {<b>What other platforms might this work on?</b><br/>Twitter might be a good one to target<br/><br/><b>What are the pros and cons of using friends as moderators?</b><br/>What if a friend sees something you don't want them to?<br/>Friends know you the best<br/><br/>Many moderators said after the field study that they were not able to get emails to the owner in a timely manner. Many also felt they were not able to interpret the owner's needs.<br/><br/>There is a lot of trust involved in having an external individual go through your emails and also trusting them to know your preferences.<br/><br/>Moderators grew less confident in their abilities.},
   publisher = {Association for Computing Machinery},
   title = {Squadbox: A tool to combat email harassment using friendsourced moderation},
   volume = {2018-April},
   year = {2018},
}
@article{Jones2022,
   abstract = {In recent years there has been substantial growth in the capabilities of systems designed to generate text that mimics the fluency and coherence of human language. From this, there has been considerable research aimed at examining the potential uses of these natural language generators (NLG) towards a wide number of tasks. The increasing capabilities of powerful text generators to mimic human writing convincingly raises the potential for deception and other forms of dangerous misuse. As these systems improve, and it becomes ever harder to distinguish between human-written and machine-generated text, malicious actors could leverage these powerful NLG systems to a wide variety of ends, including the creation of fake news and misinformation, the generation of fake online product reviews, or via chatbots as means of convincing users to divulge private information. In this paper, we provide an overview of the NLG field via the identification and examination of 119 survey-like papers focused on NLG research. From these identified papers, we outline a proposed high-level taxonomy of the central concepts that constitute NLG, including the methods used to develop generalised NLG systems, the means by which these systems are evaluated, and the popular NLG tasks and subtasks that exist. In turn, we provide an overview and discussion of each of these items with respect to current research and offer an examination of the potential roles of NLG in deception and detection systems to counteract these threats. Moreover, we discuss the broader challenges of NLG, including the risks of bias that are often exhibited by existing text generation systems. This work offers a broad overview of the field of NLG with respect to its potential for misuse, aiming to provide a high-level understanding of this rapidly developing area of research.},
   author = {Keenan Jones and Enes Altuncu and Virginia N. L. Franqueira and Yichao Wang and Shujun Li},
   month = {8},
   title = {A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception},
   url = {http://arxiv.org/abs/2208.05757},
   year = {2022},
}
@article{Tambwekar2018,
   abstract = {Language-modeling--based approaches to story plot generation attempt to construct a plot by sampling from a language model (LM) to predict the next character, word, or sentence to add to the story. LM techniques lack the ability to receive guidance from the user to achieve a specific goal, resulting in stories that don't have a clear sense of progression and lack coherence. We present a reward-shaping technique that analyzes a story corpus and produces intermediate rewards that are backpropagated into a pre-trained LM in order to guide the model towards a given goal. Automated evaluations show our technique can create a model that generates story plots which consistently achieve a specified goal. Human-subject studies show that the generated stories have more plausible event ordering than baseline plot generation techniques.},
   author = {Pradyumna Tambwekar and Murtaza Dhuliawala and Lara J. Martin and Animesh Mehta and Brent Harrison and Mark O. Riedl},
   doi = {10.24963/ijcai.2019/829},
   month = {9},
   title = {Controllable Neural Story Plot Generation via Reinforcement Learning},
   url = {http://arxiv.org/abs/1809.10736 http://dx.doi.org/10.24963/ijcai.2019/829},
   year = {2018},
}
@article{Wu2019,
   abstract = {Many tasks in natural language processing can be viewed as multi-label classification problems. However, most of the existing models are trained with the standard cross-entropy loss function and use a fixed prediction policy (e.g., a threshold of 0.5) for all the labels, which completely ignores the complexity and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a meta-learner to jointly learn the training policies and prediction policies for different labels. The training policies are then used to train the classifier with the cross-entropy loss function, and the prediction policies are further implemented for prediction. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.},
   author = {Jiawei Wu and Wenhan Xiong and William Yang Wang},
   month = {9},
   title = {Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification},
   url = {http://arxiv.org/abs/1909.04176},
   year = {2019},
}
@article{Mao2019,
   abstract = {While existing hierarchical text classification (HTC) methods attempt to capture label hierarchies for model training, they either make local decisions regarding each label or completely ignore the hierarchy information during inference. To solve the mismatch between training and inference as well as modeling label dependencies in a more principled way, we formulate HTC as a Markov decision process and propose to learn a Label Assignment Policy via deep reinforcement learning to determine where to place an object and when to stop the assignment process. The proposed method, HiLAP, explores the hierarchy during both training and inference time in a consistent manner and makes inter-dependent decisions. As a general framework, HiLAP can incorporate different neural encoders as base models for end-to-end training. Experiments on five public datasets and four base models show that HiLAP yields an average improvement of 33.4% in Macro-F1 over flat classifiers and outperforms state-of-the-art HTC methods by a large margin. Data and code can be found at https://github.com/morningmoni/HiLAP.},
   author = {Yuning Mao and Jingjing Tian and Jiawei Han and Xiang Ren},
   doi = {10.18653/v1/D19-1042},
   month = {8},
   title = {Hierarchical Text Classification with Reinforced Label Assignment},
   url = {http://arxiv.org/abs/1908.10419 http://dx.doi.org/10.18653/v1/D19-1042},
   year = {2019},
}
@article{Madaan2020,
   abstract = {This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.},
   author = {Aman Madaan and Amrith Setlur and Tanmay Parekh and Barnabas Poczos and Graham Neubig and Yiming Yang and Ruslan Salakhutdinov and Alan W Black and Shrimai Prabhumoye},
   month = {4},
   title = {Politeness Transfer: A Tag and Generate Approach},
   url = {http://arxiv.org/abs/2004.14257},
   year = {2020},
}
@article{Lai2021,
   abstract = {Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content -- the two core aspects of the task -- we achieve a new state-of-the-art.},
   author = {Huiyuan Lai and Antonio Toral and Malvina Nissim},
   month = {5},
   title = {Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer},
   url = {http://arxiv.org/abs/2105.06947},
   year = {2021},
}
@article{Jin2020,
   abstract = {Current summarization systems only produce plain, factual headlines, but do not meet the practical needs of creating memorable titles to increase exposure. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), in order to attract more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates style-specific headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from the text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines surpasses that of the state-of-the-art summarization model by 9.68%, and even outperforms human-written references.},
   author = {Di Jin and Zhijing Jin and Joey Tianyi Zhou and Lisa Orii and Peter Szolovits},
   month = {4},
   title = {Hooks in the Headline: Learning to Generate Headlines with Controlled Styles},
   url = {http://arxiv.org/abs/2004.01980},
   year = {2020},
}
@article{Ribeiro2020,
   abstract = {Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recently proposed pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. In particular, we report new state-of-the-art BLEU scores of 49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-to-text tasks. We find evidence that their knowledge about true facts helps them perform well even when the input graph representation is reduced to a simple bag of node and edge labels.},
   author = {Leonardo F. R. Ribeiro and Martin Schmitt and Hinrich Schütze and Iryna Gurevych},
   month = {7},
   title = {Investigating Pretrained Language Models for Graph-to-Text Generation},
   url = {http://arxiv.org/abs/2007.08426},
   year = {2020},
}
@article{Li2021,
   abstract = {This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at https://github.com/RUCAIBox/Few-Shot-KG2Text.},
   author = {Junyi Li and Tianyi Tang and Wayne Xin Zhao and Zhicheng Wei and Nicholas Jing Yuan and Ji-Rong Wen},
   month = {6},
   title = {Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models},
   url = {http://arxiv.org/abs/2106.01623},
   year = {2021},
}
@article{Feng2022,
   abstract = {Growing evidence shows that long noncoding RNAs (lncRNAs) play an important role in cellular biological processes at multiple levels, such as gene imprinting, immune response, and genetic regulation, and are closely related to diseases because of their complex and precise control. However, most functions of lncRNAs remain undiscovered. Current computational methods for exploring lncRNA functions can avoid high-throughput experiments, but they usually focus on the construction of similarity networks and ignore the certain directed acyclic graph (DAG) formed by gene ontology annotations. In this paper, we view the function annotation work as a hierarchical multilabel classification problem and design a method HLSTMBD for classification with DAG-structured labels. With the help of a mathematical model based on Bayesian decision theory, the HLSTMBD algorithm is implemented with the long-short term memory network and a hierarchical constraint method DAGLabel. Compared with other state-of-the-art algorithms, the results on GOA-lncRNA datasets show that the proposed method can efficiently and accurately complete the label prediction work.},
   author = {Shou Feng and Huiying Li and Jiaqing Qiao},
   doi = {10.1038/s41598-022-09672-1},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   pmid = {35388048},
   publisher = {Nature Research},
   title = {Hierarchical multi-label classification based on LSTM network and Bayesian decision theory for LncRNA function prediction},
   volume = {12},
   year = {2022},
}
@inproceedings{Peng2018,
   abstract = {Text classification to a hierarchical taxonomy of topics is a common and practical problem. Traditional approaches simply use bag-of-words and have achieved good results. However, when there are a lot of labels with different topical granularities, bag-of-words representation may not be enough. Deep learning models have been proven to be effective to automatically learn different levels of representations for image data. It is interesting to study what is the best way to represent texts. In this paper, we propose a graph-CNN based deep learning model to first convert texts to graph-of-words, and then use graph convolution operations to convolve the word graph. Graph-of-words representation of texts has the advantage of capturing non-consecutive and long-distance semantics. CNN models have the advantage of learning different level of semantics. To further leverage the hierarchy of labels, we regularize the deep architecture with the dependency among labels. Our results on both RCV1 and NYTimes datasets show that we can significantly improve large-scale hierarchical text classification over traditional hierarchical text classification and existing deep models.},
   author = {Hao Peng and Jianxin Li and Yu He and Yaopeng Liu and Mengjiao Bao and Lihong Wang and Yangqiu Song and Qiang Yang},
   doi = {10.1145/3178876.3186005},
   isbn = {9781450356398},
   journal = {The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018},
   keywords = {Convolutional neural networks,Deep learning,Graph-of-words,Hierarchical text classification,Recursive regularization},
   month = {4},
   pages = {1063-1072},
   publisher = {Association for Computing Machinery, Inc},
   title = {Large-scale hierarchical text classification with recursively regularized deep graph-CNN},
   year = {2018},
}
@article{Romero2022,
   abstract = {Node classification is the task of inferring or predicting missing node attributes from information available for other nodes in a network. This paper presents a general prediction model to hierarchical multi-label classification, where the attributes to be inferred can be specified as a strict poset. It is based on a top-down classification approach that addresses hierarchical multi-label classification with supervised learning by building a local classifier per class. The proposed model is showcased with a case study on the prediction of gene functions for Oryza sativa Japonica, a variety of rice. It is compared to the Hierarchical Binomial-Neighborhood, a probabilistic model, by evaluating both approaches in terms of prediction performance and computational cost. The results in this work support the working hypothesis that the proposed model can achieve good levels of prediction efficiency, while scaling up in relation to the state of the art.},
   author = {Miguel Romero and Jorge Finke and Camilo Rocha},
   doi = {10.1007/s41109-022-00445-3},
   issn = {23648228},
   issue = {1},
   journal = {Applied Network Science},
   keywords = {Gene function prediction,Hierarchical classification,Oryza sativa,Supervised learning,Top-down approach,XGBoost},
   month = {12},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A top-down supervised learning approach to hierarchical multi-label classification in networks},
   volume = {7},
   year = {2022},
}
@article{Giunchiglia2020,
   abstract = {Hierarchical multi-label classification (HMC) is a challenging classification task extending standard multi-label classification problems by imposing a hierarchy constraint on the classes. In this paper, we propose C-HMCNN(h), a novel approach for HMC problems, which, given a network h for the underlying multi-label classification problem, exploits the hierarchy information in order to produce predictions coherent with the constraint and improve performance. We conduct an extensive experimental analysis showing the superior performance of C-HMCNN(h) when compared to state-of-the-art models.},
   author = {Eleonora Giunchiglia and Thomas Lukasiewicz},
   month = {10},
   title = {Coherent Hierarchical Multi-Label Classification Networks},
   url = {http://arxiv.org/abs/2010.10151},
   year = {2020},
}
@report{Wachsmuth2018,
   abstract = {Persuasion is rarely achieved through a loose set of arguments alone. Rather, an effective delivery of arguments follows a rhetorical strategy, combining logical reasoning with appeals to ethics and emotion. We argue that such a strategy means to select, arrange, and phrase a set of argumentative discourse units. In this paper, we model rhetorical strategies for the computational synthesis of effective argumentation. In a study, we let 26 experts synthesize argumentative texts with different strategies for 10 topics. We find that the experts agree in the selection significantly more when following the same strategy. While the texts notably vary for different strategies, especially their arrangement remains stable. The results suggest that our model enables a strategical synthesis.},
   author = {Henning Wachsmuth and Manfred Stede and Roxanne El Baff and Khalid Al-Khatib and Maria Skeppstedt and Benno Stein},
   pages = {3753-3765},
   title = {Argumentation Synthesis following Rhetorical Strategies},
   year = {2018},
}
@report{Baff2019,
   abstract = {Synthesis approaches in computational argu-mentation so far are restricted to generating claim-like argument units or short summaries of debates. Ultimately, however, we expect computers to generate whole new arguments for a given stance towards some topic, backing up claims following argumentative and rhetorical considerations. In this paper, we approach such an argumentation synthesis as a language modeling task. In our language model, argumentative discourse units are the "words", and arguments represent the "sentences". Given a pool of units for any unseen topic-stance pair, the model selects a set of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units' argumentative roles, and finally "phrases" an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments.},
   author = {Roxanne El Baff and Henning Wachsmuth and Khalid Al-Khatib and Manfred Stede and Benno Stein},
   pages = {54-64},
   title = {Computational Argumentation Synthesis as a Language Modeling Task},
   url = {https://github.com/webis-de/},
   year = {2019},
}
@article{Chen2021,
   abstract = {Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called \textit\{context\}. We push forward the scientific text generation by proposing a new task, namely \textbf\{context-aware text generation\} in the scientific domain, aiming at exploiting the contributions of context in generated texts. To this end, we present a novel challenging large-scale \textbf\{Sci\}entific Paper Dataset for Conte\textbf\{X\}t-Aware Text \textbf\{Gen\}eration (SciXGen), consisting of well-annotated 205,304 papers with full references to widely-used objects (e.g., tables, figures, algorithms) in a paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of our newly constructed SciXGen dataset in generating description and paragraph. Our dataset and benchmarks will be made publicly available to hopefully facilitate the scientific text generation research.},
   author = {Hong Chen and Hiroya Takamura and Hideki Nakayama},
   month = {10},
   title = {SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation},
   url = {http://arxiv.org/abs/2110.10774},
   year = {2021},
}
@report{Balakrishnan2019,
   abstract = {Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.},
   author = {Anusha Balakrishnan and Jinfeng Rao and Kartikeya Upasani and Michael White and Rajen Subba},
   pages = {831-844},
   title = {Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue},
   url = {https://github.com/facebookresearch/},
   year = {2019},
}
@article{Gehrmann2022,
   abstract = {Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural NLG models have improved to the point where they can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for NLG evaluation and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 NLG papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
   author = {Sebastian Gehrmann and Elizabeth Clark and Thibault Sellam},
   month = {2},
   title = {Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text},
   url = {http://arxiv.org/abs/2202.06935},
   year = {2022},
}
@article{Krening2017,
   abstract = {In order for robots to learn from people with no machine learning expertise, robots should learn from natural human instruction. Most machine learning techniques that incorporate explanations require people to use a limited vocabulary and provide state information, even if it is not intuitive. This paper discusses a software agent that learned to play the Mario Bros. game using explanations. Our goals to improve learning from explanations were twofold: 1) to filter explanations into advice and warnings and 2) to learn policies from sentences without state information. We used sentiment analysis to filter explanations into advice of what to do and warnings of what to avoid. We developed object-focused advice to represent what actions the agent should take when dealing with objects. A reinforcement learning agent used object-focused advice to learn policies that maximized its reward. After mitigating false negatives, using sentiment as a filter was approximately 85% accurate. object-focused advice performed better than when no advice was given, the agent learned where to apply the advice, and the agent could recover from adversarial advice. We also found the method of interaction should be designed to ease the cognitive load of the human teacher or the advice may be of poor quality.},
   author = {Samantha Krening and Brent Harrison and Karen M. Feigh and Charles Lee Isbell and Mark Riedl and Andrea Thomaz},
   doi = {10.1109/TCDS.2016.2628365},
   issn = {23798939},
   issue = {1},
   journal = {IEEE Transactions on Cognitive and Developmental Systems},
   keywords = {Advice,reinforcement learning (RL),sentiment},
   month = {3},
   pages = {44-55},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Learning From Explanations Using Sentiment and Advice in RL},
   volume = {9},
   year = {2017},
}
@article{Fu2017,
   abstract = {Style transfer is an important problem in natural language processing (NLP). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and principle evaluation metrics. In this paper, we propose to learn style transfer with non-parallel data. We explore two models to achieve this goal, and the key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. We also propose novel evaluation metrics which measure two aspects of style transfer: transfer strength and content preservation. We access our models and the evaluation metrics on two tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with higher style transfer strength and similar content preservation score comparing to auto-encoder.},
   author = {Zhenxin Fu and Xiaoye Tan and Nanyun Peng and Dongyan Zhao and Rui Yan},
   month = {11},
   title = {Style Transfer in Text: Exploration and Evaluation},
   url = {http://arxiv.org/abs/1711.06861},
   year = {2017},
}
@article{Alabdulkarim2021,
   abstract = {The advent of large pre-trained generative language models has provided a common framework for AI story generation via sampling the model to create sequences that continue the story. However, sampling alone is insufficient for story generation. In particular, it is hard to direct a language model to create stories to reach a specific goal event. We present two automated techniques grounded in deep reinforcement learning and reward shaping to control the plot of computer-generated stories. The first utilizes proximal policy optimization to fine-tune an existing transformer-based language model to generate text continuations but also be goal-seeking. The second extracts a knowledge graph from the unfolding story, which is used by a policy network with graph attention to select a candidate continuation generated by a language model. We report on automated metrics pertaining to how often stories achieve a given goal event as well as human participant rankings of coherence and overall story quality compared to baselines and ablations.},
   author = {Amal Alabdulkarim and Winston Li and Lara J. Martin and Mark O. Riedl},
   month = {12},
   title = {Goal-Directed Story Generation: Augmenting Generative Language Models with Reinforcement Learning},
   url = {http://arxiv.org/abs/2112.08593},
   year = {2021},
}
@article{Mir2019,
   abstract = {Research in the area of style transfer for text is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover's Distance, Word Mover's Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined models exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.},
   author = {Remi Mir and Bjarke Felbo and Nick Obradovich and Iyad Rahwan},
   month = {4},
   title = {Evaluating Style Transfer for Text},
   url = {http://arxiv.org/abs/1904.02295},
   year = {2019},
}
@article{Pathak2016,
   abstract = {We propose a new approach for extracting argument structure from natural language texts that contain an underlying argument. Our approach comprises of two phases: Score Assignment and Structure Prediction. The Score Assignment phase trains models to classify relations between argument units (Support, Attack or Neutral). To that end, different training strategies have been explored. We identify different linguistic and lexical features for training the classifiers. Through ablation study, we observe that our novel use of word-embedding features is most effective for this task. The Structure Prediction phase makes use of the scores from the Score Assignment phase to arrive at the optimal structure. We perform experiments on three argumentation datasets, namely, AraucariaDB, Debatepedia and Wikipedia. We also propose two baselines and observe that the proposed approach outperforms baseline systems for the final task of Structure Prediction.},
   author = {Arkanath Pathak and Pawan Goyal and Plaban Bhowmick},
   month = {12},
   title = {A Two-Phase Approach Towards Identifying Argument Structure in Natural Language},
   url = {http://arxiv.org/abs/1612.05420},
   year = {2016},
}
@article{Stab2016,
   abstract = {In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using integer linear programming. We show that our model considerably improves the performance of base classifiers and significantly outperforms challenging heuristic baselines. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement. This corpus and the annotation guidelines are freely available for ensuring reproducibility and to encourage future research in computational argumentation.},
   author = {Christian Stab and Iryna Gurevych},
   month = {4},
   title = {Parsing Argumentation Structures in Persuasive Essays},
   url = {http://arxiv.org/abs/1604.07370},
   year = {2016},
}
@article{Li2017,
   abstract = {Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.},
   author = {Mengxue Li and Shiqiang Geng and Yang Gao and Haijing Liu and Hao Wang},
   month = {5},
   title = {Crowdsourcing Argumentation Structures in Chinese Hotel Reviews},
   url = {http://arxiv.org/abs/1705.02077},
   year = {2017},
}
@article{Reisert2017,
   abstract = {In this paper, we compose a new task for deep argumentative structure analysis that goes beyond shallow discourse structure analysis. The idea is that argumentative relations can reasonably be represented with a small set of predefined patterns. For example, using value judgment and bipolar causality, we can explain a support relation between two argumentative segments as follows: Segment 1 states that something is good, and Segment 2 states that it is good because it promotes something good when it happens. We are motivated by the following questions: (i) how do we formulate the task?, (ii) can a reasonable pattern set be created?, and (iii) do the patterns work? To examine the task feasibility, we conduct a three-stage, detailed annotation study using 357 argumentative relations from the argumentative microtext corpus, a small, but highly reliable corpus. We report the coverage of explanations captured by our patterns on a test set composed of 270 relations. Our coverage result of 74.6% indicates that argumentative relations can reasonably be explained by our small pattern set. Our agreement result of 85.9% shows that a reasonable inter-annotator agreement can be achieved. To assist with future work in computational argumentation, the annotated corpus is made publicly available.},
   author = {Paul Reisert and Naoya Inoue and Naoaki Okazaki and Kentaro Inui},
   month = {12},
   title = {A Corpus of Deep Argumentative Structures as an Explanation to Argumentative Relations},
   url = {http://arxiv.org/abs/1712.02480},
   year = {2017},
}
@report{Wachsmuth2018,
   abstract = {Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterar-gument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissim-ilarity of pairs of arguments, based on the words and embeddings of the arguments' premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.},
   author = {Henning Wachsmuth and Shahbaz Syed and Benno Stein},
   pages = {241-251},
   title = {Retrieval of the Best Counterargument without Prior Topic Knowledge},
   url = {http://args.me},
   year = {2018},
}
@article{Guu2017,
   abstract = {We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.},
   author = {Kelvin Guu and Tatsunori B. Hashimoto and Yonatan Oren and Percy Liang},
   month = {9},
   title = {Generating Sentences by Editing Prototypes},
   url = {http://arxiv.org/abs/1709.08878},
   year = {2017},
}
@article{Peng2021,
   abstract = {We focus on the task of creating a reinforcement learning agent that is inherently explainable -- with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce causal explanations. This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, text-based game environments in which an agent perceives and acts upon the world using textual natural language. These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed -- providing ideal environments in which to test an agent's ability to explain its actions. Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions. Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.},
   author = {Xiangyu Peng and Mark O. Riedl and Prithviraj Ammanabrolu},
   month = {12},
   title = {Inherently Explainable Reinforcement Learning in Natural Language},
   url = {http://arxiv.org/abs/2112.08907},
   year = {2021},
}
@report{Duerr2021,
   abstract = {The ability to persuade others is critical to professional and personal success. However, crafting persuasive messages is demanding and poses various challenges. We conducted nine exploratory case studies to identify adaptations that professional and non-professional writers make in written scenarios to increase their subjective persuasiveness. Furthermore, we identified challenges that those writers faced and identified strategies to resolve them with persuasive natural language generation, i.e., artificial intelligence. Our findings show that humans can achieve high degrees of persuasiveness (more so for professional-level writers), and artificial intelligence can complement them to achieve increased celerity and alignment in the process.},
   author = {Sebastian Duerr and Krystian Teodor Lange and Peter A Gloor},
   title = {What Makes a Message Persuasive? Identifying Adaptations Towards Persuasiveness in Nine Exploratory Case Studies},
   year = {2021},
}
@report{Duerr2021,
   abstract = {This literature review focuses on the use of Natural Language Generation (NLG) to automatically detect and generate persuasive texts. Extending previous research on automatic identification of persuasion in text, we concentrate on generative aspects through conceptualizing determinants of persuasion in five business-focused categories: benevolence, linguistic appropriacy, logical argumentation, trustworthiness, tools & datasets. These allow NLG to increase an existing message's persuasiveness. Previous research illustrates key aspects in each of the above mentioned five categories. A research agenda to further study persuasive NLG is developed. The review includes analysis of seventy-seven articles, outlining the existing body of knowledge and showing the steady progress in this research field.},
   author = {Sebastian Duerr and Peter A Gloor},
   title = {Persuasive Natural Language Generation-A Literature Review},
   year = {2021},
}
@report{alkhatib2020predicting,
   abstract = {Predicting the persuasiveness of arguments has applications as diverse as writing assistance , essay scoring, and advertising. While clearly relevant to the task, the personal characteristics of an argument's source and audience have not yet been fully exploited toward automated persuasiveness prediction. In this paper, we model debaters' prior beliefs, interests, and personality traits based on their previous activity , without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater's characteristics enhances the prediction of argument persuasiveness as well as of debaters' resistance to persuasion.},
   author = {Khalid Al-Khatib and Michael Völske and Shahbaz Syed and Nikolay Kolyada and Benno Stein},
   pages = {7067-7072},
   title = {Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness},
   url = {https://github.com/webis-de/ACL-20},
   year = {2020},
}
@inproceedings{Ghosh2017,
   abstract = {Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.},
   author = {Sayan Ghosh and Mathieu Chollet and Eugene Laksana and Louis Philippe Morency and Stefan Scherer},
   doi = {10.18653/v1/P17-1059},
   isbn = {9781945626753},
   journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {634-642},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Affect-LM: A neural language model for customizable affective text generation},
   volume = {1},
   year = {2017},
}
@report{Goswamy2020,
   abstract = {Human use language not just to convey information but also to express their inner feelings and mental states. In this work, we adapt the state-of-the-art language generation models to generate affective (emotional) text. We posit a model capable of generating affect-driven and topic focused sentences without losing grammatical correctness as the affect intensity increases. We propose to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions fall out on grammatical correctness at extreme intensities, but our model is resilient to this and delivers robust results at all intensities. We conduct automated evaluations and human studies to test the performance of our model, and provide a detailed comparison of the results with other models. In all evaluations, our model outperforms existing affective text generation models.},
   author = {Tushar Goswamy and Ishika Singh and Ahsan Barkati and Ashutosh Modi},
   pages = {2787-2801},
   publisher = {Online},
   title = {Adapting a Language Model for Controlled Affective Text Generation},
   url = {https://github.com/ishikasingh/Affective-text-gen},
   year = {2020},
}
@report{Xu2019,
   abstract = {Sensational headlines are headlines that capture people's attention and generate reader interest. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a model that generates sensational headlines without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments ("clickbait") against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the reward for a reinforcement learner. However , maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel loss function, Auto-tuned Reinforcement Learning (ARL), to dynamically balance reinforcement learning (RL) with maximum likelihood estimation (MLE). Human evaluation shows that 60.8% of samples generated by our model are sensational, which is significantly better than the Pointer-Gen baseline (See et al., 2017) and other RL models.},
   author = {Peng Xu and Chien-Sheng Wu and Andrea Madotto and Pascale Fung},
   pages = {3065-3075},
   title = {Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning},
   url = {https://www.nytimes.com/2016/06/13/insider/which-},
   year = {2019},
}
@report{Vadapalli2018,
   abstract = {We present an online interactive tool 1 that generates titles of blog titles and thus take the first step toward automating science journalism. Science journalism aims to transform jargon-laden scientific articles into a form that the common reader can comprehend while ensuring that the underlying meaning of the article is retained. In this work, we present a tool, which, given the title and abstract of a research paper will generate a blog title by mimicking a human science journalist. The tool makes use of a model trained on a corpus of 87, 328 pairs of research papers and their corresponding blogs, built from two science news aggre-gators. The architecture of the model is a two-stage mechanism which generates blog titles. Evaluation using standard metrics indicate the viability of the proposed system.},
   author = {Raghuram Vadapalli and Bakhtiyar Syed and Nishant Prabhu and Balaji Vasan Srinivasan and Vasudeva Varma and Iiit Hyderabad and Adobe Research},
   pages = {163-168},
   title = {When science journalism meets artificial intelligence : An interactive demonstration},
   url = {https://irel.iiit.ac.in/science-ai/},
   year = {2018},
}
@report{Golchha2019,
   abstract = {In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses. The interaction between a customer and the customer care representative contributes substantially to the overall customer experience. Thus, it is imperative for customer care agents and chat-bots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention. Our system aims at automatically transforming neutral customer care responses into courteous replies. Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer. Our technique is based on a reinforced pointer-generator model for the sequence to sequence task. The model is also conditioned on a hierarchically encoded and emotionally aware conversational context. We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: generic and courteous. We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based. Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content. Experimental results also prove that our proposed approach performs better than the base-line models.},
   author = {Hitesh Golchha and Mauajama Firdaus and Asif Ekbal and Pushpak Bhattacharyya},
   pages = {851-860},
   title = {Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network},
   year = {2019},
}
@report{Dong2021,
   abstract = {We propose ParaSCI, the first large-scale paraphrase dataset in the scientific field, including 33,981 paraphrase pairs from ACL (ParaSCI-ACL) and 316,063 pairs from arXiv (ParaSCI-arXiv). Digging into characteristics and common patterns of scientific papers, we construct this dataset though intra-paper and inter-paper methods, such as collecting citations to the same paper or aggregating definitions by scientific terms. To take advantage of sentences paraphrased partially, we put up PDBERT as a general paraphrase discovering method. The major advantages of paraphrases in ParaSCI lie in the prominent length and textual diversity , which is complementary to existing paraphrase datasets. ParaSCI obtains satisfactory results on human evaluation and downstream tasks, especially long paraphrase generation.},
   author = {Qingxiu Dong and Xiaojun Wan and Yue Cao},
   pages = {424-434},
   title = {ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation},
   url = {https://github.com/dqxiu/ParaSCI},
   year = {2021},
}
@report{canobasave2016imapct,
   abstract = {Persuasive communication is the process of shaping, reinforcing and changing others' responses. In political debates, speakers express their views towards the debated topics by choosing both the content of their discourse and the argumentation process. In this work we study the use of semantic frames for modelling argumentation in speakers' discourse. We investigate the impact of a speaker's argu-mentation style and their effect in influencing an audience in supporting their candidature. We model the influence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker's influence rank, persuasive argumentation also affects such indices.},
   author = {Amparo Elizabeth Cano-Basave and Yulan He},
   pages = {1405-1413},
   title = {A Study of the Impact of Persuasive Argumentation in Political Debates},
   url = {http://www.presidency.ucsb.edu/debates.},
   year = {2016},
}
@report{Hidey2017,
   abstract = {Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of understanding what makes a message persuasive. Premises are annotated with the three types of persuasive modes: ethos, logos, pathos, while claims are labeled as interpretation, evaluation , agreement, or disagreement, the latter two designed to account for the dialog-ical nature of our corpus. We aim to answer three questions: 1) can humans reliably annotate the semantic types of argument components? 2) are types of premises/claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in persuasive messages than in non-persuasive messages?},
   author = {Christopher Hidey and Elena Musi and Alyssa Hwang and Smaranda Muresan and Kathleen Mckeown},
   pages = {11-21},
   title = {Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum},
   url = {https://github.com/chridey/change-my-view-modes},
   year = {2017},
}
@report{Becker2016,
   abstract = {Argumentative texts have been thoroughly analyzed for their argumentative structure, and recent efforts aim at their automatic classification. This work investigates linguistic properties of argumentative texts and text passages in terms of their semantic clause types. We annotate argumentative texts with Situation Entity (SE) classes, which combine notions from lexical aspect (states, events) with genericity and habit-uality of clauses. We analyse the correlation of SE classes with argumentative text genres, components of argument structures, and some functions of those components. Our analysis reveals interesting relations between the distribution of SE types and the argumentative text genre, compared to other genres like fiction or report. We also see tendencies in the correlations between argument components (such as premises and conclusions) and SE types, as well as between argumentative functions (such as support and rebuttal) and SE types. The observed tendencies can be deployed for automatic recognition and fine-grained classification of argumentative text passages.},
   author = {Maria Becker and Alexis Palmer and Anette Frank},
   pages = {21-30},
   title = {Argumentative texts and clause types},
   url = {https://github.com/peldszus/},
   year = {2016},
}
@report{Chakrabarty2021,
   abstract = {Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Ent-man, 1983). Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument re-framing that combines controllable text generation (positive connotation) with a postde-coding entailment component (same denota-tion). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustwor-thiness/reduction of fear.},
   author = {Tuhin Chakrabarty and Christopher Hidey and Smaranda Muresan},
   pages = {4958-4971},
   title = {ENTRUST: Argument Reframing with Language Models and Entailment},
   url = {https://github.com/},
   year = {2021},
}
@report{Stab2014,
   abstract = {In this paper, we present a novel approach for identifying argumentative discourse structures in persuasive essays. The structure of argumentation consists of several components (i.e. claims and premises) that are connected with argumentative relations. We consider this task in two consecutive steps. First, we identify the components of arguments using multiclass classification. Second, we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse. For both tasks, we evaluate several classifiers and propose novel feature sets including structural , lexical, syntactic and contextual features. In our experiments, we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations.},
   author = {Christian Stab and Iryna Gurevych},
   pages = {46-56},
   title = {Identifying Argumentative Discourse Structures in Persuasive Essays},
   url = {www.ukp.tu-darmstadt.de},
   year = {2014},
}
@report{Chakrabarty2019,
   abstract = {Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argu-mentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contex-tual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one's argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model.},
   author = {Tuhin Chakrabarty and Christopher Hidey and Smaranda Muresan and Kathleen Mckeown and Alyssa Hwang},
   pages = {2933-2943},
   title = {AMPERSAND: Argument Mining for PERSuAsive oNline Discussions},
   url = {https://github.com/chridey/change-my-view-modes},
   year = {2019},
}

@report{Cohan2018,
   abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly out-performs state-of-the-art models.},
   author = {Arman Cohan and Franck Dernoncourt Doo Soon Kim Trung Bui Seokhwan Kim Walter Chang Nazli Goharian},
   pages = {615-621},
   title = {A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents},
   url = {https://github.com/acohan/long-summarization},
   year = {2018},
}
@report{alvamanchego2020asset,
   abstract = {In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences , paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.},
   author = {Fernando Alva-Manchego and Louis Martin and Antoine Bordes and Carolina Scarton and Benoˆıt Benoˆıt Sagot and Lucia Specia},
   pages = {4668-4679},
   title = {ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations},
   url = {https://github.com/facebookresearch/},
   year = {2020},
}
@report{Strapparava2010,
   abstract = {In political speeches, the audience tends to react or resonate to signals of persuasive communication, including an expected theme, a name or an expression. Automatically predicting the impact of such discourses is a challenging task. In fact nowadays, with the huge amount of textual material that flows on the Web (news, discourses, blogs, etc.), it can be useful to have a measure for testing the persuasiveness of what we retrieve or possibly of what we want to publish on Web. In this paper we exploit a corpus of political discourses collected from various Web sources, tagged with audience reactions, such as applause, as indicators of persuasive expressions. In particular, we use this data set in a machine learning framework to explore the possibility of classifying the transcript of political discourses, according to their persuasive power, predicting the sentences that possibly trigger applause. We also explore differences between Democratic and Republican speeches, experiment the resulting classifiers in grading some of the discourses in the Obama-McCain presidential campaign available on the Web.},
   author = {Carlo Strapparava and Marco Guerini and Oliviero Stock},
   title = {Predicting Persuasiveness in Political Discourses},
   url = {http://www.barackobama.com/speeches/index.php,},
   year = {2010},
}
@report{Kyleelo2020,
   author = {Isabellcachola Kyleelo and Armanncohan and Daniells Weld},
   title = {TLDR: Extreme Summarization of Scientific Documents},
   url = {https://github.com/allenai/scitldr..},
   year = {2020},
}
@report{Guerini2010,
   abstract = {Evaluating systems and theories about persuasion represents a bottleneck for both theoretical and applied fields: experiments are usually expensive and time consuming. Still, measuring the persuasive impact of a message is of paramount importance. In this paper we present a new "cheap and fast" methodology for measuring the persuasiveness of communication. This methodology allows conducting experiments with thousands of subjects for a few dollars in a few hours, by tweaking and using existing commercial tools for advertising on the web, such as Google AdWords. The central idea is to use AdWords features for defining message persuasiveness metrics. Along with a description of our approach we provide some pilot experiments, conducted both with text and image based ads, that confirm the effectiveness of our ideas. We also discuss the possible application of research on persuasive systems to Google AdWords in order to add more flexibility in the wearing out of persuasive messages.},
   author = {Marco Guerini and Carlo Strapparava and Oliviero Stock},
   title = {Evaluation Metrics for Persuasive NLP with Google AdWords},
   year = {2010},
}
@report{Wang2019,
   abstract = {Developing intelligent persuasive conversational agents to change people's opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore , to develop an understanding of per-sonalized persuasion processes, we analyzed the relationships between individuals' demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals' personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system. 1},
   author = {Xuewei Wang and Weiyan Shi and Richard Kim and Yoojung Oh and Sijia Yang and Jingwen Zhang and Zhou Yu},
   pages = {5635-5649},
   title = {Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good},
   url = {https://www.savethechildren.org/.},
   year = {2019},
}
@report{Ghazarian2021,
   abstract = {Having engaging and informative conversations with users is the utmost goal for open-domain conversational systems. Recent advances in transformer-based language models and their applications to dialogue systems have succeeded in generating fluent and human-like responses. However, those systems still lack control over the generation process toward producing contentful responses and achieving engaging conversations. To address this, we present DiSCoL (Dialogue Systems through Coversational Line guided response generation). DiSCoL is an open-domain dialogue system that leverages conversational lines (briefly convlines) as controllable and informative content-planning elements to guide the generation model in producing engaging and informative responses. Two primary modules in DiSCoL's pipeline are conditional generators trained for 1) predicting relevant and informative convlines for dialogue contexts and 2) generating high-quality responses conditioned on the predicted convlines. Users can also change the returned convlines to control the direction of the conversations toward topics that are more interesting for them. Through automatic and human evaluations, we demonstrate the efficiency of the convlines in producing engaging conversations.},
   author = {Sarik Ghazarian and Zixi Liu and Tuhin Chakrabarty and Xuezhe Ma and Aram Galstyan and Nanyun Peng},
   pages = {26-34},
   title = {DiSCoL: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation},
   url = {https://github.com/huggingface/},
   year = {2021},
}
@report{Teufel2009,
   abstract = {Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scientific paper. It has been shown to be reliably used by independent human coders, and has proven useful for various information access tasks. Annotation experiments have however so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics.},
   author = {Simone Teufel and Advaith Siddharthan and Colin Batchelor},
   journal = {ACL and AFNLP},
   pages = {6-7},
   title = {Towards Discipline-Independent Argumentative Zoning: Evidence from Chemistry and Computational Linguistics},
   url = {www.cl.cam.ac.uk/research/nl/sciborg.},
   year = {2009},
}
@report{Louis2013,
   abstract = {Great writing is rare and highly admired. Readers seek out articles that are beautifully written, informative and entertaining. Yet information-access technologies lack capabilities for predicting article quality at this level. In this paper we present first experiments on article quality prediction in the science journalism domain. We introduce a corpus of great pieces of science journalism, along with typical articles from the genre. We implement features to capture aspects of great writing , including surprising, visual and emotional content, as well as general features related to discourse organization and sentence structure. We show that the distinction between great and typical articles can be detected fairly accurately , and that the entire spectrum of our features contribute to the distinction.},
   author = {Annie Louis and Ani Nenkova},
   title = {What Makes Writing Great? First Experiments on Article Quality Prediction in the Science Journalism Domain},
   url = {http://www.cis.upenn.edu/},
   year = {2013},
}
@report{Simpson2018,
   abstract = {We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.},
   author = {Edwin Simpson and Iryna Gurevych},
   title = {Finding Convincing Arguments Using Scalable Bayesian Preference Learning},
   url = {https://github.com/ukplab/},
   year = {2018},
}

@inproceedings{Wang2018,
   abstract = {Most of the previous emotion classifications are based on binary or ternary classifications, and the final emotion classification results contain only one type of emotion. There is little research on multi-emotional coexistence, which has certain limitations on the restoration of human’s true emotions. Aiming at these deficiencies, this paper proposes a Bidirectional Long-Short Term Memory Multiple Classifiers (BLSTM-MC) model to study the five classification problems in code-switching text, and obtains text contextual relations through BLSTM-MC model. It fully considers the relationship between different emotions in a single post, at the same time, the Attention mechanism is introduced to find the importance of different features and predict all emotions expressed by each post. The model achieved third place in all submissions in the conference NLP&&CC_task1 2018.},
   author = {Tingwei Wang and Xiaohua Yang and Chunping Ouyang and Aodong Guo and Yongbin Liu and Zhixing Li},
   doi = {10.1007/978-3-319-99501-4_16},
   isbn = {9783319995007},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Attention mechanism,BLSTM multiple classifiers,Code-switching texts,Multiple emotion classification},
   pages = {190-199},
   publisher = {Springer Verlag},
   title = {A Multi-emotion Classification Method Based on BLSTM-MC in Code-Switching Text},
   volume = {11109 LNAI},
   year = {2018},
}
@article{Chang2021,
   abstract = {Neural natural language generation (NLG) and understanding (NLU) models are data-hungry and require massive amounts of annotated data to be competitive. Recent frameworks address this bottleneck with generative models that synthesize weak labels at scale, where a small amount of training labels are expert-curated and the rest of the data is automatically annotated. We follow that approach, by automatically constructing a large-scale weakly-labeled data with a fine-tuned GPT-2, and employ a semi-supervised framework to jointly train the NLG and NLU models. The proposed framework adapts the parameter updates to the models according to the estimated label-quality. On both the E2E and Weather benchmarks, we show that this weakly supervised training paradigm is an effective approach under low resource scenarios and outperforming benchmark systems on both datasets when 100% of training data is used.},
   author = {Ernie Chang and Vera Demberg and Alex Marin},
   month = {2},
   title = {Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling},
   url = {http://arxiv.org/abs/2102.03551},
   year = {2021},
}
@article{Mocanu2017,
   abstract = {Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd\H\{o\}s-R\'enyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.},
   author = {Decebal Constantin Mocanu and Elena Mocanu and Peter Stone and Phuong H. Nguyen and Madeleine Gibescu and Antonio Liotta},
   doi = {10.1038/s41467-018-04316-3},
   month = {7},
   title = {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},
   url = {http://arxiv.org/abs/1707.04780 http://dx.doi.org/10.1038/s41467-018-04316-3},
   year = {2017},
}
@report{Zhou2020,
   abstract = {Automated evaluation of open domain natural language generation (NLG) models remains a challenge and widely used metrics such as BLEU and Perplexity can be misleading in some cases. In our paper, we propose to evaluate natural language generation models by learning to compare a pair of generated sentences by fine-tuning BERT, which has been shown to have good natural language understanding ability. We also propose to evaluate the model-level quality of NLG models with sample-level comparison results with skill rating system. While able to be trained in a fully self-supervised fashion, our model can be further fine-tuned with a little amount of human preference annotation to better imitate human judgment. In addition to evaluating trained models, we propose to apply our model as a performance indicator during training for better hyperparameter tuning and early-stopping. We evaluate our approach on both story generation and chitchat dialogue response generation. Experimental results show that our model correlates better with human preference compared with previous automated evaluation approaches. Training with the proposed metric yields better performance in human evaluation, which further demonstrates the effectiveness of the proposed model.},
   author = {Wangchunshu Zhou and Ke Xu},
   keywords = {Natural Language Processing},
   title = {Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models},
   url = {www.aaai.org},
   year = {2020},
}
@article{Mager2020,
   abstract = {Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.},
   author = {Manuel Mager and Ramon Fernandez Astudillo and Tahira Naseem and Md Arafat Sultan and Young-Suk Lee and Radu Florian and Salim Roukos},
   month = {5},
   title = {GPT-too: A language-model-first approach for AMR-to-text generation},
   url = {http://arxiv.org/abs/2005.09123},
   year = {2020},
}
@report{Tan2014,
   abstract = {Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work looking into predicting popularity of social-media content, the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, humans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data.},
   author = {Chenhao Tan and Lillian Lee and Bo Pang},
   title = {The effect of wording on message propagation: Topic-and author-controlled natural experiments on Twitter},
   url = {http://t.co/qy7GGuYW},
   year = {2014},
}
@article{Curci2021,
   abstract = {Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era.},
   author = {Selima Curci and Decebal Constantin Mocanu and Mykola Pechenizkiyi},
   month = {2},
   title = {Truly Sparse Neural Networks at Scale},
   url = {http://arxiv.org/abs/2102.01732},
   year = {2021},
}
@report{Fisas2015,
   abstract = {Understanding the structure of scientific discourse is of paramount importance for the development of appropriate Natural Language Processing tools able to extract and summarize information from research articles. In this paper we present an annotated corpus of scientific discourse in the domain of Computer Graphics. We describe the way we built our corpus by designing an annotation schema and relying on three annotators for manually classifying all sentences into the defined categories. Our corpus constitutes a semantically rich resource for scientific text mining. In this respect, we also present the results of our initial experiments of automatic classification of sentences into the 5 main categories in our corpus .},
   author = {Beatriz Fisas and Francesco Ronzano and Horacio Saggion},
   pages = {42-51},
   title = {On the Discoursive Structure of Computer Graphics Research Papers},
   year = {2015},
}
@report{Shao2019,
   abstract = {Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts: they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our model outperforms state-of-the-art baselines in long and diverse text generation .},
   author = {Zhihong Shao and Minlie Huang and Jiangtao Wen and Wenfei Xu and Xiaoyan Zhu},
   pages = {3257-3268},
   title = {Long and Diverse Text Generation with Planning-based Hierarchical Variational Model},
   year = {2019},
}
@article{Liu2021,
   abstract = {This paper studies constrained text generation, which is to generate sentences under certain pre-conditions. We focus on CommonGen, the task of generating text based on a set of concepts, as a representative task of constrained text generation. Traditional methods mainly rely on supervised training to maximize the likelihood of target sentences.However, global constraints such as common sense and coverage cannot be incorporated into the likelihood objective of the autoregressive decoding process. In this paper, we consider using reinforcement learning to address the limitation, measuring global constraints including fluency, common sense and concept coverage with a comprehensive score, which serves as the reward for reinforcement learning. Besides, we design a guided decoding method at the word, fragment and sentence levels. Experiments demonstrate that our method significantly increases the concept coverage and outperforms existing models in various automatic evaluations.},
   author = {Yixian Liu and Liwen Zhang and Wenjuan Han and Yue Zhang and Kewei Tu},
   month = {3},
   title = {Constrained Text Generation with Global Guidance -- Case Study on CommonGen},
   url = {http://arxiv.org/abs/2103.07170},
   year = {2021},
}
@report{Yu2022,
   abstract = {The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry. Acknowledgements: We thank all anonymous reviewers for valuable comments. We also appreciate the suggestions from readers of our pre-print version. We thank Dr. Michael Zeng (Microsoft Research) and Dr. Nazneen Rajani (Saleforce Research) for their constructive comments and suggestions.},
   author = {Wenhao Yu and Chenguang Zhu and J I Heng and Zaitang Li and Zhiting Hu and Qingyun Wang and Heng Ji and Meng Jiang},
   issue = {1},
   journal = {ACM Comput. Surv},
   keywords = {Knowledge-enhanced Methods,Natural language generation},
   title = {A Survey of Knowledge-Enhanced Text Generation; A Survey of Knowledge-Enhanced Text Generation},
   volume = {1},
   url = {https://doi.org/xxx.xxx},
   year = {2022},
}
@article{Fan2018,
   abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
   author = {Angela Fan and Mike Lewis and Yann Dauphin},
   month = {5},
   title = {Hierarchical Neural Story Generation},
   url = {http://arxiv.org/abs/1805.04833},
   year = {2018},
}
@article{Yao2018,
   abstract = {Automatic storytelling is challenging since it requires generating long, coherent natural language to describes a sensible sequence of events. Despite considerable efforts on automatic story generation in the past, prior work either is restricted in plot planning, or can only generate stories in a narrow domain. In this paper, we explore open-domain story generation that writes stories given a title (topic) as input. We propose a plan-and-write hierarchical generation framework that first plans a storyline, and then generates a story based on the storyline. We compare two planning strategies. The dynamic schema interweaves story planning and its surface realization in text, while the static schema plans out the entire storyline before generating stories. Experiments show that with explicit storyline planning, the generated stories are more diverse, coherent, and on topic than those generated without creating a full plan, according to both automatic and human evaluations.},
   author = {Lili Yao and Nanyun Peng and Ralph Weischedel and Kevin Knight and Dongyan Zhao and Rui Yan},
   month = {11},
   title = {Plan-And-Write: Towards Better Automatic Storytelling},
   url = {http://arxiv.org/abs/1811.05701},
   year = {2018},
}
@report{Hardmeier2013,
   abstract = {We describe Docent, an open-source de-coder for statistical machine translation that breaks with the usual sentence-by-sentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models.},
   author = {Christian Hardmeier and Sara Stymne and Jörg Tiedemann and Joakim Nivre},
   pages = {193-198},
   title = {Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation},
   url = {https://github.com/chardmeier/docent/wiki},
   year = {2013},
}
@report{goldfarbtarrant2020planning,
   abstract = {Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprin-cipled way. 1},
   author = {Seraphina Goldfarb-Tarrant and Tuhin Chakrabarty and Ralph Weischedel and Nanyun Peng},
   pages = {4319-4338},
   title = {Content Planning for Neural Story Generation with Aristotelian Rescoring},
   url = {https://github.com/PlusLabNLP/},
   year = {2020},
}
@article{Santhanam2020,
   abstract = {Long short-term memory(LSTM) units on sequence-based models are being used in translation, question-answering systems, classification tasks due to their capability of learning long-term dependencies. In Natural language generation, LSTM networks are providing impressive results on text generation models by learning language models with grammatically stable syntaxes. But the downside is that the network does not learn about the context. The network only learns the input-output function and generates text given a set of input words irrespective of pragmatics. As the model is trained without any such context, there is no semantic consistency among the generated sentences. The proposed model is trained to generate text for a given set of input words along with a context vector. A context vector is similar to a paragraph vector that grasps the semantic meaning(context) of the sentence. Several methods of extracting the context vectors are proposed in this work. While training a language model, in addition to the input-output sequences, context vectors are also trained along with the inputs. Due to this structure, the model learns the relation among the input words, context vector and the target word. Given a set of context terms, a well trained model will generate text around the provided context. Based on the nature of computing context vectors, the model has been tried out with two variations (word importance and word clustering). In the word clustering method, the suitable embeddings among various domains are also explored. The results are evaluated based on the semantic closeness of the generated text to the given context.},
   author = {Sivasurya Santhanam},
   month = {4},
   title = {Context based Text-generation using LSTM networks},
   url = {http://arxiv.org/abs/2005.00048},
   year = {2020},
}
@report{Guan2021,
   abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in mod-eling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.},
   author = {Jian Guan and Xiaoxi Mao and Changjie Fan and Zitao Liu and Wenbiao Ding and Minlie Huang},
   pages = {6379-6393},
   title = {Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence},
   url = {https://github.com/},
   year = {2021},
}
@report{Schmidt2017,
   abstract = {This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches .},
   author = {Anna Schmidt and Michael Wiegand},
   pages = {1-10},
   publisher = {Association for Computational Linguistics},
   title = {A Survey on Hate Speech Detection using Natural Language Processing},
   url = {https://en.wikipedia.org/wiki/List_},
   year = {2017},
}
@report{Roberts2016,
   author = {Sarah T Roberts},
   note = {Delete this citation, it is a poor and misguided report.},
   title = {Commercial Content Moderation: Digital
Laborers' Dirty Work},
   url = {https://ir.lib.uwo.ca/commpubhttps://ir.lib.uwo.ca/commpub/12},
   year = {2016},
}
@report{Oba2022,
   author = {Demi Oba and Jonah Berger},
   title = {How Hedges Impact Persuasion},
   year = {2022},
}
@report{Pan2019,
   abstract = {Automated representation learning is behind many recent success stories in machine learning. It is often used to transfer knowledge learned from a large dataset (e.g., raw text) to tasks for which only a small number of training examples are available. In this paper, we review recent advance in learning to represent social media users in low-dimensional embeddings. The technology is critical for creating high performance social media-based human traits and behavior models since the ground truth for assessing latent human traits and behavior is often expensive to acquire at a large scale. In this survey, we review typical methods for learning a unified user embeddings from heterogeneous user data (e.g., combines social media texts with images to learn a unified user representation). Finally we point out some current issues and future directions.},
   author = {Shimei Pan and Tao Ding},
   keywords = {Humans and AI: Personalization and User Modeling,Machine Learning Applications: Applications of Unsupervised Learning,Multidisciplinary Topics and Applications: Social Sciences,Natural Language Processing: Embeddings},
   note = {<i>Sidenote:</i> Yelpdata has<br/>User profile statistics<br/>Text from each user (review)<br/>Statistics about the review<br/>Network information (i.e. friends)<br/>Restaurant information},
   title = {Social Media-based User Embedding: A Literature Review},
   year = {2019},
}
@report{Wang2014,
   abstract = {We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus. We propose a novel method of jointly embedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
   author = {Zhen Wang and Jianwen Zhang and Jianlin Feng and Zheng Chen},
   title = {Knowledge Graph and Text Jointly Embedding},
   year = {2014},
}
@report{Mai2019,
   abstract = {The past decades have witnessed a rapid increase in the global scientific output as measured by published papers. Exploring a scientific field and searching for relevant papers and authors seems like a needle-in-a-haystack problem. Although many academic search engines have been developed to accelerate this retrieval process, most of them rely on content-based methods and feature engineering. In this work, we present an entity retrieval prototype system on top of IOS Press LD Connect which utilizes both textual and structure information. Paragraph vector and knowledge graph embedding are used to embed papers and entities into low dimensional hidden space. Next, the semantic similarity between papers and entities can be measured based on the learned embedding models. Two benchmark datasets have been collected from Semantic Scholar and DBLP to evaluate the performance of our entity retrieval models. Results show that paragraph vectors are effective at capturing the similarity and relatedness among papers and knowledge graph embedding models can preserve the inherent structure of the original knowledge graph and hence assist in link prediction tasks such as co-author inference.},
   author = {Gengchen Mai and Krzysztof Janowicz and Bo Yan},
   keywords = {Embedding,Entity,Graph,Knowledge,Paper,Paragraph,Recommender,Retrieval ·,System ·,Vector ·},
   title = {Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines},
   url = {http://stko.geog.ucsb.edu/},
   year = {2019},
}
@article{Li2019,
   abstract = {In this paper, we propose a novel model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. RevGAN utilizes the combination of three novel components, including self-attentive recursive autoencoders, conditional discriminators, and personalized decoders. We test its performance on the several real-world datasets, where our model significantly outperforms state-of-the-art generation models in terms of sentence quality, coherence, personalization and human evaluations. We also empirically show that the generated reviews could not be easily distinguished from the organically produced reviews and that they follow the same statistical linguistics laws.},
   author = {Pan Li and Alexander Tuzhilin},
   month = {9},
   title = {Towards Controllable and Personalized Review Generation},
   url = {http://arxiv.org/abs/1910.03506},
   year = {2019},
}
@article{Soni2021,
   abstract = {There is a growing interest in designing autonomous agents that can work alongside humans. Such agents will undoubtedly be expected to explain their behavior and decisions. While generating explanations is an actively researched topic, most works tend to focus on methods that generate explanations that are one size fits all. As in the specifics of the user-model are completely ignored. The handful of works that look at tailoring their explanation to the user's background rely on having specific models of the users (either analytic models or learned labeling models). The goal of this work is thus to propose an end-to-end adaptive explanation generation system that begins by learning the different types of users that the agent could interact with. Then during the interaction with the target user, it is tasked with identifying the type on the fly and adjust its explanations accordingly. The former is achieved by a data-driven clustering approach while for the latter, we compile our explanation generation problem into a POMDP. We demonstrate the usefulness of our system on two domains using state-of-the-art POMDP solvers. We also report the results of a user study that investigates the benefits of providing personalized explanations in a human-robot interaction setting.},
   author = {Utkarsh Soni and Sarath Sreedharan and Subbarao Kambhampati},
   month = {6},
   title = {Not all users are the same: Providing personalized explanations for sequential decision making problems},
   url = {http://arxiv.org/abs/2106.12207},
   year = {2021},
}
@article{Zeng2019,
   abstract = {Comments on social media are very diverse, in terms of content, style and vocabulary, which make generating comments much more challenging than other existing natural language generation~(NLG) tasks. Besides, since different user has different expression habits, it is necessary to take the user's profile into consideration when generating comments. In this paper, we introduce the task of automatic generation of personalized comment~(AGPC) for social media. Based on tens of thousands of users' real comments and corresponding user profiles on weibo, we propose Personalized Comment Generation Network~(PCGN) for AGPC. The model utilizes user feature embedding with a gated memory and attends to user description to model personality of users. In addition, external user representation is taken into consideration during the decoding to enhance the comments generation. Experimental results show that our model can generate natural, human-like and personalized comments.},
   author = {Wenhuan Zeng and Abulikemu Abuduweili and Lei Li and Pengcheng Yang},
   month = {7},
   note = {<b>Summary</b><br/>This paper addresses the challenge of personalized comment generation using user profile information with context. They use LSTM's to gather post context and user description context and add 2 things:<br/>1. Gated memory module based off of a user profile's numeric features<br/>2. external personality expression based off of<br/><br/><b>What can we learn?</b><br/>},
   title = {Automatic Generation of Personalized Comment Based on User Profile},
   url = {http://arxiv.org/abs/1907.10371},
   year = {2019},
}
@article{Li2021,
   abstract = {Generating personalized responses is one of the major challenges in natural human-robot interaction. Current researches in this field mainly focus on generating responses consistent with the robot's pre-assigned persona, while ignoring the user's persona. Such responses may be inappropriate or even offensive, which may lead to the bad user experience. Therefore, we propose a Bilateral Personalized Dialogue Generation (BPDG) method for dyadic conversation, which integrates user and robot personas into dialogue generation via designing a dynamic persona-aware fusion method. To bridge the gap between the learning objective function and evaluation metrics, the Conditional Mutual Information Maximum (CMIM) criterion is adopted with contrastive learning to select the proper response from the generated candidates. Moreover, a bilateral persona accuracy metric is designed to measure the degree of bilateral personalization. Experimental results demonstrate that, compared with several state-of-the-art methods, the final results of the proposed method are more personalized and consistent with bilateral personas in terms of both automatic and manual evaluations.},
   author = {Bin Li and Hanjun Deng},
   month = {6},
   title = {Bilateral Personalized Dialogue Generation with Contrastive Learning},
   url = {http://arxiv.org/abs/2106.07857},
   year = {2021},
}
@article{Cho2022,
   abstract = {Current works in the generation of personalized dialogue primarily contribute to the agent presenting a consistent personality and driving a more informative response. However, we found that the generated responses from most previous models tend to be self-centered, with little care for the user in the dialogue. Moreover, we consider that human-like conversation is essentially built based on inferring information about the persona of the other party. Motivated by this, we propose a novel personalized dialogue generator by detecting an implicit user persona. Because it is hard to collect a large number of detailed personas for each user, we attempted to model the user's potential persona and its representation from dialogue history, with no external knowledge. The perception and fader variables were conceived using conditional variational inference. The two latent variables simulate the process of people being aware of each other's persona and producing a corresponding expression in conversation. Finally, posterior-discriminated regularization was presented to enhance the training procedure. Empirical studies demonstrate that, compared to state-of-the-art methods, our approach is more concerned with the user's persona and achieves a considerable boost across the evaluations.},
   author = {Itsugun Cho and Dongyang Wang and Ryota Takahashi and Hiroaki Saito},
   month = {4},
   note = {<b>Summary</b><br/>This paper addresses the challenge of personalized dialogue generation by modeling two latent variables: Z_p for the user's persona and Z_alpha for indicating how much persona information is carried. The construction is relatively simple but they show that these latent variables can be input into GPT-2 along with context to generate personalized dialogue.<br/><br/><b>What can we learn?</b><br/>Creating a latent variable for the user embeddings and combining with text context may be a good way of generating text from user embeddings.},
   title = {A Personalized Dialogue Generator with Implicit User Persona Detection},
   url = {http://arxiv.org/abs/2204.07372},
   year = {2022},
}
@article{Majumder2019,
   abstract = {Existing approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of ingredients in specific dishes. We propose a new task of personalized recipe generation to help these users: expanding a name and incomplete ingredient details into complete natural-text instructions aligned with the user's historical preferences. We attend on technique- and recipe-level representations of a user's previously consumed recipes, fusing these 'user-aware' representations in an attention fusion layer to control recipe text generation. Experiments on a new dataset of 180K recipes and 700K interactions show our model's ability to generate plausible and personalized recipes compared to non-personalized baselines.},
   author = {Bodhisattwa Prasad Majumder and Shuyang Li and Jianmo Ni and Julian McAuley},
   month = {8},
   title = {Generating Personalized Recipes from Historical User Preferences},
   url = {http://arxiv.org/abs/1909.00105},
   year = {2019},
}
@article{Deng2022,
   abstract = {Recently, Product Question Answering (PQA) on E-Commerce platforms has attracted increasing attention as it can act as an intelligent online shopping assistant and improve the customer shopping experience. Its key function, automatic answer generation for product-related questions, has been studied by aiming to generate content-preserving while question-related answers. However, an important characteristic of PQA, i.e., personalization, is neglected by existing methods. It is insufficient to provide the same "completely summarized"answer to all customers, since many customers are more willing to see personalized answers with customized information only for themselves, by taking into consideration their own preferences toward product aspects or information needs. To tackle this challenge, we propose a novel Personalized Answer GEneration method with multi-perspective preference modeling, which explores historical user-generated contents to model user preference for generating personalized answers in PQA. Specifically, we first retrieve question-related user history as external knowledge to model knowledge-level user preference. Then, we leverage the Gaussian Softmax distribution model to capture latent aspect-level user preference. Finally, we develop a persona-aware pointer network to generate personalized answers in terms of both content and style by utilizing personal user preference and dynamic user vocabulary. Experimental results on real-world E-Commerce QA datasets demonstrate that the proposed method outperforms existing methods by generating informative and customized answers and show that answer generation in E-Commerce can benefit from personalization.},
   author = {Yang Deng and Yaliang Li and Wenxuan Zhang and Bolin Ding and Wai Lam},
   doi = {10.1145/3507782},
   issn = {15582868},
   issue = {4},
   journal = {ACM Transactions on Information Systems},
   keywords = {Answer generation,E-Commerce,personalization,product question answering},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Toward Personalized Answer Generation in E-Commerce via Multi-perspective Preference Modeling},
   volume = {40},
   year = {2022},
}
@inproceedings{Musto2019,
   abstract = {In this paper we present a methodology to justify recommendations that relies on the information extracted from users' reviews discussing the available items. The intuition behind the approach is to conceive the justifcation as a summary of the most relevant and distinguishing aspects of the item, automatically obtained by analyzing its reviews. To this end, we designed a pipeline of natural language processing techniques including aspect extraction, sentiment analysis and text summarization to gather the reviews, process the relevant excerpts, and generate a unique synthesis presenting the main characteristics of the item. Such a summary is fnally presented to the target user as a justifcation of the received recommendation. In the experimental evaluation we carried out a user study in the movie domain (N=141) and the results showed that our approach is able to make the recommendation process more transparent, engaging and trustful for the users.},
   author = {Cataldo Musto and Gaetano Rossiello and Marco De Gemmis and Pasquale Lops and Giovanni Semeraro},
   doi = {10.1145/3298689.3347024},
   isbn = {9781450362436},
   journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
   keywords = {Explanation,Recommender Systems,Sentiment Analysis,Text Summarization},
   month = {9},
   pages = {383-387},
   publisher = {Association for Computing Machinery, Inc},
   title = {Combining text summarization and aspect-based sentiment analysis of users' reviews to justify recommendations},
   year = {2019},
}
@inproceedings{Li2019,
   abstract = {Tips, as a compacted and concise form of reviews, were paid less attention by researchers. In this paper, we investigate the task of tips generation by considering the “persona” information which captures the intrinsic language style of the users or the different characteristics of the product items. In order to exploit the persona information, we propose a framework based on adversarial variational auto-encoders (aVAE) for persona modeling from the historical tips and reviews of users and items. The latent variables from aVAE are regarded as persona embeddings. Besides representing persona using the latent embeddings, we design a persona memory for storing the persona related words for users and items. Pointer Network is used to retrieve persona wordings from the memory when generating tips. Moreover, the persona embeddings are used as latent factors by a rating prediction component to predict the sentiment of a user over an item. Finally, the persona embeddings and the sentiment information are incorporated into a recurrent neural networks based tips generation component. Extensive experimental results are reported and discussed to elaborate the peculiarities of our framework.},
   author = {Piji Li and Lidong Bing and Zihao Wang and Wai Lam},
   doi = {10.1145/3308558.3313496},
   isbn = {9781450366748},
   journal = {The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019},
   keywords = {Abstractive Tips Generation,Adversarial Variational Auto-Encoders,Persona Modeling,Rating Prediction},
   month = {5},
   pages = {1006-1016},
   publisher = {Association for Computing Machinery, Inc},
   title = {Persona-aware TIPS generation},
   year = {2019},
}
@inproceedings{Li2020prg,
   abstract = {Personalized review generation (PRG) aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive user preference in multiple aspects. To address the above issues, we propose a novel knowledgeenhanced PRG model based on capsule graph neural network (CapsGNN). We first construct a heterogeneous knowledge graph (HKG) for utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence. Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task.},
   author = {Junyi Li and Siqing Li and Wayne Xin Zhao and Gaole He and Zhicheng Wei and Nicholas Jing Yuan and Ji Rong Wen},
   doi = {10.1145/3340531.3411893},
   isbn = {9781450368599},
   journal = {International Conference on Information and Knowledge Management, Proceedings},
   keywords = {capsule graph neural network,knowledge graph,review generation},
   month = {10},
   pages = {735-744},
   publisher = {Association for Computing Machinery},
   title = {Knowledge-Enhanced Personalized Review Generation with Capsule Graph Neural Network},
   year = {2020},
}
@report{Ni2018,
   abstract = {In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries , product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns 'aspect-aware' user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.},
   author = {Jianmo Ni and Julian Mcauley},
   pages = {706-711},
   title = {Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations},
   year = {2018},
}
@report{Ni2019,
   abstract = {Several recent works have considered the problem of generating reviews (or 'tips') as a form of explanation as to why a recommendation might match a user's interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users' decision-making process. We seek to introduce new datasets and methods to address this recommendation justification task. In terms of data, we first propose an 'extractive' approach to identify review segments which justify users' intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.},
   author = {Jianmo Ni and Jiacheng Li and Julian Mcauley},
   title = {Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects},
   year = {2019},
}
@article{Song2020,
   abstract = {Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.},
   author = {Haoyu Song and Yan Wang and Wei-Nan Zhang and Xiaojiang Liu and Ting Liu},
   month = {4},
   title = {Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation},
   url = {http://arxiv.org/abs/2004.07672},
   year = {2020},
}
@inproceedings{Sun2020,
   abstract = {In many recommender systems, users express item opinions through two kinds of behaviors: giving preferences and writing detailed reviews. As both kinds of behaviors reflect users' assessment of items, review enhanced recommender systems leverage these two kinds of user behaviors to boost recommendation performance. On the one hand, researchers proposed to better model the user and item embeddings with additional review information for enhancing preference prediction accuracy. On the other hand, some recent works focused on automatically generating item reviews for recommendation explanations with related user and item embeddings. We argue that, while the task of preference prediction with the accuracy goal is well recognized in the community, the task of generating reviews for explainable recommendation is also important to gain user trust and increase conversion rate. Some preliminary attempts have considered jointly modeling these two tasks, with the user and item embeddings are shared. These studies empirically showed that these two tasks are correlated, and jointly modeling them would benefit the performance of both tasks. In this paper, we make a further study of unifying these two tasks for explainable recommendation. Instead of simply correlating these two tasks with shared user and item embeddings, we argue that these two tasks are presented in dual forms. In other words, the input of the primal preference prediction task is exactly the output of the dual review generation task , with and denote the preference value space and review space. Therefore, we could explicitly model the probabilistic correlation between these two dual tasks with . We design a unified dual framework of how to inject the probabilistic duality of the two tasks in the training stage. Furthermore, as the detailed preference and review information are not available for each user-item pair in the test stage, we propose a transfer learning based model for preference prediction and review generation. Finally, extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed model for both user preference prediction and review generation.},
   author = {Peijie Sun and Le Wu and Kun Zhang and Yanjie Fu and Richang Hong and Meng Wang},
   doi = {10.1145/3366423.3380164},
   isbn = {9781450370233},
   journal = {The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020},
   keywords = {dual learning,recommender system,review generation},
   month = {4},
   pages = {837-847},
   publisher = {Association for Computing Machinery, Inc},
   title = {Dual Learning for Explainable Recommendation: Towards Unifying User Preference Prediction and Review Generation},
   year = {2020},
}
@article{Song2019,
   abstract = {In human conversations, due to their personalities in mind, people can easily carry out and maintain the conversations. Giving conversational context with persona information to a chatbot, how to exploit the information to generate diverse and sustainable conversations is still a non-trivial task. Previous work on persona-based conversational models successfully make use of predefined persona information and have shown great promise in delivering more realistic responses. And they all learn with the assumption that given a source input, there is only one target response. However, in human conversations, there are massive appropriate responses to a given input message. In this paper, we propose a memory-augmented architecture to exploit persona information from context and incorporate a conditional variational autoencoder model together to generate diverse and sustainable conversations. We evaluate the proposed model on a benchmark persona-chat dataset. Both automatic and human evaluations show that our model can deliver more diverse and more engaging persona-based responses than baseline approaches.},
   author = {Haoyu Song and Wei-Nan Zhang and Yiming Cui and Dong Wang and Ting Liu},
   month = {5},
   title = {Exploiting Persona Information for Diverse Generation of Conversational Responses},
   url = {http://arxiv.org/abs/1905.12188},
   year = {2019},
}
@report{Li2019,
   abstract = {We address personalized review summarization, which generates a condensed summary for a user's review, accounting for his preference on different aspects or his writing style. We propose a novel personalized review summarization model named User-aware Sequence Network (USN) to consider the aforementioned users' characteristics when generating summaries , which contains a user-aware encoder and a user-aware decoder. Specifically, the user-aware encoder adopts a user-based selective mechanism to select the important information of a review, and the user-aware decoder incorporates user characteristic and user-specific word-using habits into word prediction process to generate personalized summaries. To validate our model, we collected a new dataset Trip, comprising 536,255 reviews from 19,400 users. With quantitative and human evaluation, we show that USN achieves state-of-the-art performance on personalized review summarization.},
   author = {Junjie Li and Haoran Li and Chengqing Zong},
   pages = {19},
   title = {Towards Personalized Review Summarization via User-Aware Sequence Network},
   url = {www.aaai.org},
   year = {2019},
}
@article{Xu2020,
   abstract = {Unstructured Persona-oriented Dialogue Systems (UPDS) has been demonstrated effective in generating persona consistent responses by utilizing predefined natural language user persona descriptions (e.g., "I am a vegan"). However, the predefined user persona descriptions are usually short and limited to only a few descriptive words, which makes it hard to correlate them with the dialogues. As a result, existing methods either fail to use the persona description or use them improperly when generating persona consistent responses. To address this, we propose a neural topical expansion framework, namely Persona Exploration and Exploitation (PEE), which is able to extend the predefined user persona description with semantically correlated content before utilizing them to generate dialogue responses. PEE consists of two main modules: persona exploration and persona exploitation. The former learns to extend the predefined user persona description by mining and correlating with existing dialogue corpus using a variational auto-encoder (VAE) based topic model. The latter learns to generate persona consistent responses by utilizing the predefined and extended user persona description. In order to make persona exploitation learn to utilize user persona description more properly, we also introduce two persona-oriented loss functions: Persona-oriented Matching (P-Match) loss and Persona-oriented Bag-of-Words (P-BoWs) loss which respectively supervise persona selection in encoder and decoder. Experimental results show that our approach outperforms state-of-the-art baselines, in terms of both automatic and human evaluations.},
   author = {Minghong Xu and Piji Li and Haoran Yang and Pengjie Ren and Zhaochun Ren and Zhumin Chen and Jun Ma},
   month = {2},
   title = {A Neural Topical Expansion Framework for Unstructured Persona-oriented Dialogue Generation},
   url = {http://arxiv.org/abs/2002.02153},
   year = {2020},
}
@report{Zhao2019,
   author = {Guoshuai Zhao and Xing Xie},
   title = {Tetsuya Sakai Association for Computing Machinery Personalized Reason Generation for Explainable Song Recommendation},
   url = {https://www.researchgate.net/publication/333984657},
   year = {2019},
}
@article{Wu2019,
   abstract = {Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progresses achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via the End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.},
   author = {Bowen Wu and Mengyuan Li and Zongsheng Wang and Yifu Chen and Derek Wong and Qihang Feng and Junhong Huang and Baoxun Wang},
   month = {11},
   title = {Guiding Variational Response Generator to Exploit Persona},
   url = {http://arxiv.org/abs/1911.02390},
   year = {2019},
}
@inproceedings{Zhang2020,
   abstract = {The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members? opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia's Request for Adminship (RfA) process.},
   author = {Amy X. Zhang and Grant Hugh and Michael S. Bernstein},
   doi = {10.1145/3379337.3415858},
   isbn = {9781450375146},
   journal = {UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
   keywords = {Governance,Moderation,Online communities,Policy,Toolkit},
   month = {10},
   pages = {365-378},
   publisher = {Association for Computing Machinery, Inc},
   title = {PolicyKit: Building governance in online communities},
   year = {2020},
}
@report{graber,
   abstract = {Temporal graph networks are powerful tools for solving the cold-start problem in sequential recommender systems. However, graph models are still susceptible to changes in the external environment dynamics. The paper proposes two simple yet efficient graph-based exploration methods for the mitigation of the aforementioned issue. The first one adopts the counter-based state exploration from reinforcement learning to the bipartite graph domain. We suggest a method that biases model predictions toward locally unexplored items using random walks. The second method implements the idea of a small-world network model via conducting link creation outside of network communities by adding the inverse of soft-cluster similarity. Both methods show competitive quality on the popular recommender systems benchmarks. Finally, we provide an extensive qualitative analysis of experiment results and recommendations for the use of our method in production applications.},
   author = {Anonymous Author},
   keywords = {exploration,fine-tuning,gnn,graph neural networks,graphs,interactive recommender systems,intrinsic motivation,online adaptation,pretraining,recommender systems,recsys,self-supervised},
   title = {GRABER: GRAph-Based Exploration for sequential Recommender systems},
   url = {https://doi.org/XXXXXXX.XXXXXXX},
}
@article{Li2022,
   abstract = {Practical applications of event extraction systems have long been hindered by their need for heavy human annotation. In order to scale up to new domains and event types, models must learn to cope with limited supervision, as in few-shot learning settings. To this end, the major challenge is to let the model master the semantics of event types, without requiring abundant event mention annotations. In our study, we employ cloze prompts to elicit event-related knowledge from pretrained language models and further use event definitions and keywords to pinpoint the trigger word. By formulating the event detection task as an identify-then-localize procedure, we minimize the number of type-specific parameters, enabling our model to quickly adapt to event detection tasks for new types. Experiments on three event detection benchmark datasets (ACE, FewEvent, MAVEN) show that our proposed method performs favorably under fully supervised settings and surpasses existing few-shot methods by 21% F1 on the FewEvent dataset and 20% on the MAVEN dataset when only 5 examples are provided for each event type.},
   author = {Sha Li and Liyuan Liu and Yiqing Xie and Heng Ji and Jiawei Han},
   month = {2},
   title = {PILED: An Identify-and-Localize Framework for Few-Shot Event Detection},
   url = {http://arxiv.org/abs/2202.07615},
   year = {2022},
}
@inproceedings{Sharma2021,
   abstract = {Disinformation campaigns on social media, involving coordinated activities from malicious accounts towards manipulating public opinion, have become increasingly prevalent. Existing approaches to detect coordinated accounts either make very strict assumptions about coordinated behaviours, or require part of the malicious accounts in the coordinated group to be revealed in order to detect the rest. To address these drawbacks, we propose a generative model, AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group Estimation) which jointly models account activities and hidden group behaviours based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to capture inherent characteristics of coordination which is, accounts that coordinate must strongly influence each other's activities, and collectively appear anomalous from normal accounts. To address the challenges of optimizing the proposed model, we provide a bilevel optimization algorithm with theoretical guarantee on convergence. We verified the effectiveness of the proposed method and training algorithm on real-world social network data collected from Twitter related to coordinated campaigns from Russia's Internet Research Agency targeting the 2016 U.S. Presidential Elections, and to identify coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned model, we find that the average influence between coordinated account pairs is the highest. On COVID-19, we found coordinated group spreading anti-vaccination, anti-masks conspiracies that suggest the pandemic is a hoax and political scam.},
   author = {Karishma Sharma and Yizhou Zhang and Emilio Ferrara and Yan Liu},
   doi = {10.1145/3447548.3467391},
   isbn = {9781450383325},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {coordinated influence campaigns,disinformation,fake news,social media,temporal point process},
   month = {8},
   pages = {1441-1451},
   publisher = {Association for Computing Machinery},
   title = {Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours},
   year = {2021},
}
@report{Author2022,
   abstract = {Our goal is to control text generation with more fine-grained conditions at lower computational cost than current alternatives; these conditions include attributes, and denote both multiple codes and free-text. As large-scale pre-trained neural language models (NLMs) offer excellent performance in representation learning and free-form text generation, we explore efficient architectures and training schemes that can best leverage NLMs. Our framework, Friendly Conditional Text Generator (FCTG), introduces a multi-view attention (MVA) mechanism and two training tasks, Masked Attribute Modeling (MAM) and Attribute Linguistic Matching (ALM), to guide various pre-trained NLMs via modalities between the text and its attributes. The motivation of FCTG is to bridge these modality gaps and so map them into a shared space, as these embedded representations can be used to realize more accurate text generation. To avoid catastrophic forgetting, learn modality-free embedded representations , and guide text generation in this space, FCTG applies MAM to learn attribute representations, projects both these attributes and words in the same space through MVA, and optimizes their alignment in this space via ALM. Experiments on publicly available datasets show that FCTG outperforms baselines over higher level conditions at lower computation cost.},
   author = {Anonymous Author},
   keywords = {Conditional Text Generation,Neural Language Generation,Neural Language Model,Transformer},
   title = {Friendly Conditional Text Generator; Friendly Conditional Text Generator},
   url = {https://doi.org/},
   year = {2022},
}
@article{Keskar2019,
   abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
   author = {Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
   month = {9},
   title = {CTRL: A Conditional Transformer Language Model for Controllable Generation},
   url = {http://arxiv.org/abs/1909.05858},
   year = {2019},
}
@article{Darwish2019,
   abstract = {We present a highly effective unsupervised framework for detecting the stance of prolific Twitter users with respect to controversial topics. In particular, we use dimensionality reduction to project users onto a low-dimensional space, followed by clustering, which allows us to find core users that are representative of the different stances. Our framework has three major advantages over pre-existing methods, which are based on supervised or semi-supervised classification. First, we do not require any prior labeling of users: instead, we create clusters, which are much easier to label manually afterwards, e.g., in a matter of seconds or minutes instead of hours. Second, there is no need for domain- or topic-level knowledge either to specify the relevant stances (labels) or to conduct the actual labeling. Third, our framework is robust in the face of data skewness, e.g., when some users or some stances have greater representation in the data. We experiment with different combinations of user similarity features, dataset sizes, dimensionality reduction methods, and clustering algorithms to ascertain the most effective and most computationally efficient combinations across three different datasets (in English and Turkish). We further verified our results on additional tweet sets covering six different controversial topics. Our best combination in terms of effectiveness and efficiency uses retweeted accounts as features, UMAP for dimensionality reduction, and Mean Shift for clustering, and yields a small number of high-quality user clusters, typically just 2--3, with more than 98\% purity. The resulting user clusters can be used to train downstream classifiers. Moreover, our framework is robust to variations in the hyper-parameter values and also with respect to random initialization.},
   author = {Kareem Darwish and Peter Stefanov and Michaël Aupetit and Preslav Nakov},
   month = {4},
   title = {Unsupervised User Stance Detection on Twitter},
   url = {http://arxiv.org/abs/1904.02000},
   year = {2019},
}
@report{bie2020recipe,
   abstract = {Semi-structured text generation is a non-trivial problem. Although last years have brought lots of improvements in natural language generation , thanks to the development of neural models trained on large scale datasets, these approaches still struggle with producing struc-tured, context-and commonsense-aware texts. Moreover, it is not clear how to evaluate the quality of generated texts. To address these problems, we introduce RecipeNLG-a novel dataset of cooking recipes. We discuss the data collection process and the relation between the semi-structured texts and cooking recipes. We use the dataset to approach the problem of generating recipes. Finally, we make use of multiple metrics to evaluate the generated recipes.},
   author = {Michał Bié and Michał Gilski and Martyna Maciejewska and Wojciech Taisner and Dawid Wi´sniewski Wi´sniewski and Agnieszka Ławrynowicz},
   pages = {15-18},
   title = {RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation},
   year = {2020},
}
@inproceedings{Haque2022,
   abstract = {Source code summarization involves creating brief descriptions of source code in natural language. These descriptions are a key component of software documentation such as JavaDocs. Automatic code summarization is a prized target of software engineering research, due to the high value summaries have to programmers and the simultaneously high cost of writing and maintaining documentation by hand. Current work is almost all based on machine models trained via big data input. Large datasets of examples of code and summaries of that code are used to train an e.g. encoder-decoder neural model. Then the output predictions of the model are evaluated against a set of reference summaries. The input is code not seen by the model, and the prediction is compared to a reference. The means by which a prediction is compared to a reference is essentially word overlap, calculated via a metric such as BLEU or ROUGE. The problem with using word overlap is that not all words in a sentence have the same importance, and many words have synonyms. The result is that calculated similarity may not match the perceived similarity by human readers. In this paper, we conduct an experiment to measure the degree to which various word overlap metrics correlate to human-rated similarity of predicted and reference summaries. We evaluate alternatives based on current work in semantic similarity metrics and propose recommendations for evaluation of source code summarization.},
   author = {Sakib Haque and Zachary Eberhart and Aakash Bansal and Collin McMillan},
   doi = {10.1145/nnnnnnn.nnnnnnn},
   isbn = {9781450392983},
   journal = {IEEE International Conference on Program Comprehension},
   keywords = {automatic documentation generation,evaluation metrics,source code summarization},
   pages = {36-47},
   publisher = {IEEE Computer Society},
   title = {Identifying Coordinated Accounts on Social Media through Hidden Influence and Group Behaviours},
   volume = {2022-March},
   year = {2022},
}
@report{mai2020pplm,
   abstract = {Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoen-coder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.},
   author = {Florian Mai and Nikolaos Pappas and Ivan Montero and Noah A Smith and James Henderson},
   note = {<b>Critiques</b><br/>Not only do these approaches involve computational overhead and estimation errors associated with the training of language models, but they are also dependent on access to a large amount of attribute-specific language data which can be impractical in many scenarios and exacerbate privacy concerns<br/><br/>Although it has remarkable style accuracy, this method can hardly guarantee content preservation due to the concrete output sharply changes with the latent representation. We can see that all of these existing methods belong to the inductive learning approach, because they aim to learn a general style transfer rule from the training data, and then apply the rule to the test cases. Due to the lack of parallel corpus for supervision, this inductive learning approach fails to learn an accurate style representation application for various contexts, and may cause some severe inconsistency problems as illustrated before.<br/>https://arxiv.org/pdf/2109.07812.pdf<br/><br/>Previous attempts of editing text in latent space have often been limited to single attribute and small-scale models, due to the incompatibility of the latent space with the existing transformer-based pretrained LMs<br/>https://arxiv.org/pdf/2208.00638.pdf<br/><br/><br/><b>My Critiques:</b><br/><b>- </b>The content preservation task seems to restrict the latent vector to be similar to the input. In many tasks, this could be too limiting and lead to marginal changes. One idea is for unsupervised methods to instead disentangle style and content in the latent vector z (see Shen et al., 2017). To use pretrained autoencoders (AE), we would train Shen's AE on recreating z_x (sort of like a hierarchical AE). You could then directly apply phi on the style component of z while preserving the content.<br/>- },
   title = {Plug and Play Autoencoders for Conditional Text Generation},
   url = {https://github.com/},
}
@article{Li2022,
   abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
   author = {Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori B. Hashimoto},
   month = {5},
   note = {Theory video: https://www.youtube.com/watch?v=HoKDTa5jHvg&t=1338s<br/><br/>Implementation video: https://www.youtube.com/watch?v=a4Yfz2FxXiY<br/><br/>NN has shared parameters across timesteps! It cannot distinguish between timesteps.<br/><br/>Positional embeddings are added as well (probably automatic with BERT) to help w/ timestep issue},
   title = {Diffusion-LM Improves Controllable Text Generation},
   url = {http://arxiv.org/abs/2205.14217},
   year = {2022},
}
@article{Arora2022,
   abstract = {Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly "perfect prompt" for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False."). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-Neo-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting},
   author = {Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel Orr and Neel Guha and Kush Bhatia and Ines Chami and Frederic Sala and Christopher Ré},
   month = {10},
   title = {Ask Me Anything: A simple strategy for prompting language models},
   url = {http://arxiv.org/abs/2210.02441},
   year = {2022},
}
@article{Lu2022,
   abstract = {Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.},
   author = {Ximing Lu and Sean Welleck and Liwei Jiang and Jack Hessel and Lianhui Qin and Peter West and Prithviraj Ammanabrolu and Yejin Choi},
   month = {5},
   title = {Quark: Controllable Text Generation with Reinforced Unlearning},
   url = {http://arxiv.org/abs/2205.13636},
   year = {2022},
}
@article{Luo2022,
   abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
   author = {Calvin Luo},
   month = {8},
   title = {Understanding Diffusion Models: A Unified Perspective},
   url = {http://arxiv.org/abs/2208.11970},
   year = {2022},
}
@report{sap2020social,
   abstract = {Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that "we shouldn't lower our standards to hire more women," most listeners will infer the implicature intended by the speaker-that "women (candidates) are less qualified." Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce SOCIAL BIAS FRAMES, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition , we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover SOCIAL BIAS FRAMES from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F 1), they are not effective at spelling out more detailed explanations in terms of SOCIAL BIAS FRAMES. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications .},
   author = {Maarten Sap and Saadia Gabriel and Lianhui Qin and Dan Jurafsky and Noah A Smith and Yejin Choi and Paul G Allen},
   title = {SOCIAL BIAS FRAMES: Reasoning about Social and Power Implications of Language},
   url = {http://tinyurl.com/social-bias-frames.},
}
@article{Chen2021,
   abstract = {The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. To tackle this problem in NLP, we propose $\textit\{in-context tuning\}$, which recasts adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, the labeled examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label from the input sequences on a collection of tasks. We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to first-order MAML which adapts the model with gradient descent, our method better leverages the inductive bias of LMs to perform pattern matching, and outperforms MAML by an absolute $6\%$ AUC ROC score on BinaryClfs, with increasing advantage w.r.t. model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning directly learns to learn from in-context examples. On BinaryClfs, in-context tuning improves the average AUC-ROC score by an absolute $10\%$, and reduces the variance with respect to example ordering by 6x and example choices by 2x.},
   author = {Yanda Chen and Ruiqi Zhong and Sheng Zha and George Karypis and He He},
   month = {10},
   title = {Meta-learning via Language Model In-context Tuning},
   url = {http://arxiv.org/abs/2110.07814},
   year = {2021},
}
@article{Pang2020,
   abstract = {Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. We propose GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. Intuitively, GOLD upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. According to both automatic and human evaluation, models trained by GOLD outperform those trained by MLE and policy gradient on summarization, question generation, and machine translation. Further, our models are less sensitive to decoding algorithms and alleviate exposure bias.},
   author = {Richard Yuanzhe Pang and He He},
   month = {9},
   title = {Text Generation by Learning from Demonstrations},
   url = {http://arxiv.org/abs/2009.07839},
   year = {2020},
}
@article{Chang2015,
   abstract = {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.},
   author = {Kai-Wei Chang and Akshay Krishnamurthy and Alekh Agarwal and Hal Daumé and John Langford},
   month = {2},
   title = {Learning to Search Better Than Your Teacher},
   url = {http://arxiv.org/abs/1502.02206},
   year = {2015},
}
@article{Kumar2021,
   abstract = {As large-scale language model pretraining pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via fine-tuning remains the popular approach, it incurs a significant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MuCoCO -- a flexible and modular algorithm for controllable inference from pretrained models. We formulate the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe significant improvements over baselines.},
   author = {Sachin Kumar and Eric Malmi and Aliaksei Severyn and Yulia Tsvetkov},
   month = {8},
   note = {limited to n tokens in sentence. if you want more, increase n. if less, assumpe we use padding token. cant be good...<br/><b>correction: </b>length of target is not known in advance,<br/>- greedily decode from G to get length L<br/>- use approach for each length from [L-5, L+5]<br/>Kind of a hack...<br/><br/><br/>graddescent for every generation, must be slow},
   title = {Controlled Text Generation as Continuous Optimization with Multiple Constraints},
   url = {http://arxiv.org/abs/2108.01850},
   year = {2021},
}
@article{Yang2021,
   abstract = {We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in language generation, and formality change in machine translation -- and observe gains in all three tasks.},
   author = {Kevin Yang and Dan Klein},
   doi = {10.18653/v1/2021.naacl-main.276},
   month = {4},
   title = {FUDGE: Controlled Text Generation With Future Discriminators},
   url = {http://arxiv.org/abs/2104.05218 http://dx.doi.org/10.18653/v1/2021.naacl-main.276},
   year = {2021},
}
@article{Meng2022,
   abstract = {We propose a general and efficient framework to control auto-regressive generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained base language model and a sequence-level boolean oracle function, we propose to decompose the oracle function into token-level guidance to steer the base model in text generation. Specifically, the token-level guidance is approximated by a neural model trained with examples sampled from the base model, demanding no additional auxiliary labeled data. Based on posterior regularization, we present the closed-form optimal solution to incorporate the token-level guidance into the base model for controllable generation. We further provide a theoretical analysis of how the approximation quality of NADO affects the controllable generation results. Experiments conducted on two applications: (1) text generation with lexical constraints and (2) machine translation with formality control demonstrate that our framework efficiently guides the base model towards the given oracle while maintaining high generation quality.},
   author = {Tao Meng and Sidi Lu and Nanyun Peng and Kai-Wei Chang},
   month = {5},
   title = {Controllable Text Generation with Neurally-Decomposed Oracle},
   url = {http://arxiv.org/abs/2205.14219},
   year = {2022},
}
@article{Gong2022,
   abstract = {Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is difficult due to the discrete nature of text. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.},
   author = {Shansan Gong and Mukai Li and Jiangtao Feng and Zhiyong Wu and Lingpeng Kong},
   month = {10},
   title = {DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
   url = {http://arxiv.org/abs/2210.08933},
   year = {2022},
}
@article{Cao2022,
   abstract = {Deep learning shows excellent potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly concerning certain implied parameters. Recently, the diffusion Model has become a rising class of generative models by its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this field. However, the diffusion model has its genuine drawback of a slow generation process, single data types, low likelihood, and the inability for dimension reduction. They are leading to many enhanced works. This survey makes a summary of the field of the diffusion model. We first state the main problem with two landmark works -- DDPM and DSM, and a unified landmark work -- Score SDE. Then, we present improved techniques for existing problems in the diffusion-based model field, including speed-up improvement For model speed-up improvement, data structure diversification, likelihood optimization, and dimension reduction. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to specific NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this field together with limitations \& further directions. The summation of existing well-classified methods is in our Github:https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model.},
   author = {Hanqun Cao and Cheng Tan and Zhangyang Gao and Guangyong Chen and Pheng-Ann Heng and Stan Z. Li},
   month = {9},
   title = {A Survey on Generative Diffusion Model},
   url = {http://arxiv.org/abs/2209.02646},
   year = {2022},
}
@article{Iqbal2018,
   abstract = {User demand for blocking advertising and tracking online is large and growing. Existing tools, both deployed and described in research, have proven useful, but lack either the completeness or robustness needed for a general solution. Existing detection approaches generally focus on only one aspect of advertising or tracking (e.g. URL patterns, code structure), making existing approaches susceptible to evasion. In this work we present AdGraph, a novel graph-based machine learning approach for detecting advertising and tracking resources on the web. AdGraph differs from existing approaches by building a graph representation of the HTML structure, network requests, and JavaScript behavior of a webpage, and using this unique representation to train a classifier for identifying advertising and tracking resources. Because AdGraph considers many aspects of the context a network request takes place in, it is less susceptible to the single-factor evasion techniques that flummox existing approaches. We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly accurate, able to replicate the labels of human-generated filter lists with 95.33% accuracy, and can even identify many mistakes in filter lists. We implement AdGraph as a modification to Chromium. AdGraph adds only minor overhead to page loading and execution, and is actually faster than stock Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we conclude that AdGraph is both accurate enough and performant enough for online use, breaking comparable or fewer websites than popular filter list based approaches.},
   author = {Umar Iqbal and Peter Snyder and Shitong Zhu and Benjamin Livshits and Zhiyun Qian and Zubair Shafiq},
   month = {5},
   title = {AdGraph: A Graph-Based Approach to Ad and Tracker Blocking},
   url = {http://arxiv.org/abs/1805.09155},
   year = {2018},
}
@report{august2022control,
   abstract = {Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific reader's background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We introduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation base-lines.},
   author = {Tal August and Katharina Reinecke and Noah A Smith and Paul G Allen},
   pages = {8298-8317},
   publisher = {Long Papers},
   title = {Generating Scientific Definitions with Controllable Complexity},
   volume = {1},
   url = {https://en.wikipedia.org/wiki/},
}
@article{Suzgun2022,
   abstract = {We propose a method for arbitrary textual style transfer (TST)--the task of transforming a text into any given style--utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Specifically, our method first uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks these candidates according to a combination of the three components above. Empirically, our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while consuming two orders of magnitude less compute and memory. Finally, we conduct a systematic investigation of the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets.},
   author = {Mirac Suzgun and Luke Melas-Kyriazi and Dan Jurafsky},
   month = {5},
   title = {Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models},
   url = {http://arxiv.org/abs/2205.11503},
   year = {2022},
}
@article{John2018,
   abstract = {This paper tackles the problem of disentangling the latent variables of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for label prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning method is applied to style transfer on non-parallel corpora. We achieve substantially better results in terms of transfer accuracy, content preservation and language fluency, in comparison to previous state-of-the-art approaches.},
   author = {Vineet John and Lili Mou and Hareesh Bahuleyan and Olga Vechtomova},
   month = {8},
   title = {Disentangled Representation Learning for Non-Parallel Text Style Transfer},
   url = {http://arxiv.org/abs/1808.04339},
   year = {2018},
}







































 
% references for the IEEEtran.bst documentation
% the distribution site for IEEEtran.bst
@electronic{IEEEexample:shellCTANpage,
  author        = "Michael Shell",
  title         = "{IEEE}tran Homepage on {CTAN}",
  url           = "http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/",
  year          = "2002"
};

% the IEEE website
% sort key is needed for sorting styles
@electronic{IEEEexample:IEEEwebsite,
  title         = "The {IEEE} Website",
  url           = "http://www.ieee.org/",
  year          = "2002",
  key           = "IEEE"
};

% The BibTeX user's guide.
% The filename could have been put in the URL instead. But, there might
% be other interesting things for the user in the same directory - and
% the filename might change before the directory that contains it.
@electronic{IEEEexample:bibtexuser,
  author        = "Oren Patashnik",
  title         = "{\BibTeX}ing",
  howpublished  = "{btxdoc.pdf}",
  url           = "http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/",
  month         = feb,
  year          = "1988"
};

% The BibTeX style designer's guide.
@electronic{IEEEexample:bibtexdesign,
  author        = "Oren Patashnik",
  title         = "Designing {\BibTeX\ } Styles",
  howpublished  = "{btxhak.pdf}",
  url           = "http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/",
  month         = feb,
  year          = "1988"
};

% The BibTeX Tips and FAQ guide.
@electronic{IEEEexample:bibtexFAQ,
  author        = "David Hoadley and Michael Shell",
  title         = "{\BibTeX}\ Tips and {FAQ}",
  howpublished  = "{btxFAQ.txt}",
  url           = "http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/",
  month         = oct,
  year          = "2002"
};

% TeX User Group Bibliography Archive
@electronic{IEEEexample:beebe_archive,
  author        = "Nelson H. F. Beebe",
  title         = "{\TeX\ }User Group Bibliography Archive",
  url           = "http://www.math.utah.edu:8080/pub/tex/bib/index-table.html",
  month         = may,
  year          = "2002"
};

% The url.sty package.
@electronic{IEEEexample:urlsty,
  author        = "Donald Arseneau",
  title         = "The url.sty package",
  url           = "http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/",
  month         = mar,
  year          = "1999",
};


% The hyperref.sty package.
@electronic{IEEEexample:hyperrefsty,
  author        = "Sebastian Rahtz and Heiko Oberdiek",
  title         = "The hyperref.sty package",
  url           = "http://www.ctan.org/tex-archive/macros/latex/contrib/supported/hyperref/",
  month         = jul,
  year          = "2002",
};

% The Babel package.
@electronic{IEEEexample:babel,
  author        = "Johannes Braams",
  title         = "The {Babel} package",
  url           = "http://www.ctan.org/tex-archive/macros/latex/required/babel/",
  month         = feb,
  year          = "2001",
};





% The three most common and typical types of references used in
% IEEE publications:

% an article in a journal
% Note the use of the IEEE_J_EDL string, defined in the IEEEabrv.bib file,
% for the journal name. IEEEtran.bst defines the BibTeX standard three
% letter month codes per IEEE style.
% From the June 2002 issue of "IEEE Transactions on Electron Devices",
% page 996, reference #16.
@article{IEEEexample:article_typical,
  author        = "S. Zhang and C. Zhu and J. K. O. Sin and P. K. T. Mok",
  title         = "A Novel Ultrathin Elevated Channel Low-temperature 
                   Poly-{Si} {TFT}",
  journal       = IEEE_J_EDL,
  volume        = "20",
  month         = nov,
  year          = "1999",
  pages         = "569-571"
};

% journal article using et al.
% The (five) authors are actually: F. Delorme, S. Slempkes, G. Alibert, 
% B. Rose, J. Brandon
% The month (July) was not given here.
% From the September 1998 issue of "IEEE Journal on Selected Areas in
% Communications", page 1257, reference #28.
@article{IEEEexample:articleetal,
  author        = "F. Delorme and others",
  title         = "Butt-jointed {DBR} Laser With 15 {nm} Tunability Grown
                   in Three {MOVPE} Steps",
  journal       = "Electron. Lett.",
  volume        = "31",
  number        = "15",
  year          = "1995",
  pages         = "1244-1245"
};


% a paper in a conference proceedings
% "conference" can be used as an alias for "inproceedings"
% From the June 2002 issue of "Journal of Microelectromechanical Systems",
% page 205, reference #16.
@inproceedings{IEEEexample:conf_typical,
  author        = "R. K. Gupta and S. D. Senturia",
  title         = "Pull-in Time Dynamics as a Measure of Absolute Pressure",
  booktitle     = "Proc. {IEEE} International Workshop on
                   Microelectromechanical Systems ({MEMS}'97)",
  address       = "Nagoya, Japan",
  month         = jan,
  year          = "1997",
  pages         = "290-294"
};


% a book
% From the May 2002 issue of "IEEE Transactions on Magnetics",
% page 1466, reference #4.
@book{IEEEexample:book_typical,
  author        = "B. D. Cullity",
  title         = "Introduction to Magnetic Materials",
  publisher     = "Addison-Wesley",
  address       = "Reading, MA",
  year          = "1972"
};




% Other examples

% journal article with large page numbers, IEEE will divide numbers 5 digits
% or longer into groups of three with small spaces between them. Page ranges
% can be separated via either "-" or "--", IEEEtran.bst will automatically
% convert the separator dash(es) to "--".
% Authors and/or IEEE do not always provide/use the journal number, but it
% was used in this case. IEEEtran.bst can be configured to ignore journal
% numbers if desired.
% From the August 2000 issue of "IEEE Photonics Technology Letters",
% page 1039, reference #11.
@article{IEEEexample:articlelargepages,
  author        = "A. Castaldini and A. Cavallini and B. Fraboni
                   and P. Fernandez and J. Piqueras",
  title         = "Midgap Traps Related to Compensation Processes in
                   {CdTe} Alloys",
  journal       = "Phys. Rev. B.",
  volume        = "56",
  number        = "23",
  year          = "1997",
  pages         = "14897-14900"
};


% journal article with dual months 
% use the BibTeX "#" concatenation operator
% From the March 2002 issue of "IEEE Transactions on Mechatronics",
% page 21, reference #8.
@article{IEEEexample:articledualmonths,
  author        = "Y. Okada and K. Dejima and T. Ohishi",
  title         = "Analysis and Comparison of {PM} Synchronous Motor and
                   Induction Motor Type Magnetic Bearings",
  journal       = IEEE_J_IA,
  volume        = "31",
  month         = sep # "/" # oct,
  year          = "1995",
  pages         = "1  047-1053"
};


% journal article to be published as a misc entry type
% date information like month and year is optional
% However, the article form like that below may be a better approach.
% From the May 2002 issue of "IEEE Journal of Selected Areas in 
% Communication", page 725, reference #3.
@misc{IEEEexample:TBPmisc,
  author        = "M. Coates and A. Hero and R. Nowak and B. Yu",
  title         = "Internet Tomography",
  howpublished  = IEEE_M_SP,
  month         = may,
  year          = "2002",
  note          = "to be published"
};


% journal article to be published as an article entry type
% year is required, so if absent, use the year field to hold
% the "submitted for publication" in order to avoid a warning for
% the missing year field.
% From the June 2002 issue of "IEEE Transactions on Information Theory",
% page 1461, reference #21.
@article{IEEEexample:TBParticle,
  author        = "N. Kahale and R. Urbanke",
  title         = "On the Minimum Distance of Parallel and Serially
                   Concatenated Codes",
  journal       = IEEE_J_IT,
  year          = "submitted for publication"
};




% book with editor and no author
% From the June 2002 issue of "IEEE Transactions on Information Theory",
% page 1725, reference #1.
@book{IEEEexample:bookwitheditor,
  editor        = "J. C. Candy and G. C. Temes",
  title         = "Oversampling Delta-Sigma Data Converters Theory,
                   Design and Simulation",
  publisher     = "{IEEE} Press.",
  address       = "New York",
  year          = "1992"
};


% book with edition, author and editor
% Note that the standard BibTeX styles do not support book entries with both
% author and editor fields, but IEEEtran.bst does.
% The standard BibTeX way of specifying the edition is to use capitalized
% ordinal words such as "First", "Second", etc. Most .bst files can convert
% up to about "Fifth" into the format needed. IEEEtran.bst can convert up
% to "Tenth" to the "Arabic ordinal" form IEEE uses (e.g., "10th"). For
% editions over the tenth, it is best to use the "Arabic ordinal" form for
% IEEE related work (e.g., "101st")
% Note how "Jr." has to be entered.
% From the May 2002 issue of "Journal of Lightwave Technology", page 856,
% reference #17.
@book{IEEEexample:book,
  author        = "S. M. Metev and V. P. Veiko",
  editor        = "Osgood, Jr., R. M.",
  title         = "Laser Assisted Microtechnology",
  edition       = "Second",
  publisher     = "Springer-Verlag",
  address       = "Berlin, Germany",
  year          = "1998"
};


% book with series and volume
% From the January 2000 issue of "IEEE Transactions on Communications",
% page 11, reference #31.
@book{IEEEexample:bookwithseriesvolume,
  editor        = "J. Breckling",
  title         = "The Analysis of Directional Time Series: Applications to
                   Wind Speed and Direction",
  series        = "Lecture Notes in Statistics",
  publisher     = "Springer",
  address       = "Berlin, Germany",
  year          = "1989",
  volume        = "61"
};


% inbook with chapter number. The pages field could also have been given.
% The chapter number could be changed to something else such as a section
% number via the type field: type = "sec.".
% From the May 2002 issue of "IEEE Transactions on Circuits and Systems---I: 
% Fundamental Applications and Theory", page 638, reference #22.
@inbook{IEEEexample:inbook,
  author        = "H. E. Rose",
  title         = "A Course in Number Theory",
  publisher     = "Oxford Univ. Press",
  address       = "New York, NY",
  year          = "1988",
  chapter       = "3"
};


% inbook with pages and note. The language field is not set to Russian
% because the title is presented here in its translated, English, form.
% From the May 2002 issue of "IEEE Transactions on Magnetics", page 1533,
% reference #5.
@inbook{IEEEexample:inbookpagesnote,
  author        = "B. K. Bul",
  title         = "Theory Principles and Design of Magnetic Circuits",
  publisher     = "Energia Press",
  address       = "Moscow",
  year          = "1964",
  pages         = "464",
  note          = "(in Russian)"
};





% incollection with author and editor
% From the May 2002 issue of "Journal of Lightwave Technology",
% page 807, reference #7.
@incollection{IEEEexample:incollection,
  author        = "W. V. Sorin",
  editor        = "D. Derickson",
  title         = "Optical Reflectometry for Component Characterization",
  booktitle     = "Fiber Optic Test and Measurement",
  publisher     = "Prentice-Hall",
  address       = "Englewood Cliffs, NJ",
  year          = "1998"
};


% incollection with series
% From the April 2000 issue of "IEEE Transactions on Communication",
% page 609, reference #3.
@incollection{IEEEexample:incollectionwithseries,
  author        = "J. B. Anderson and K. Tepe",
  title         = "Properties of the Tailbiting {BCJR} Decoder",
  booktitle     = "Codes, Systems and Graphical Models",
  series        = "{IMA} Volumes in Mathematics and Its Applications",
  publisher     = "Springer-Verlag",
  address       = "New York",
  year          = "2000"
  
};


% incollection with author, editor, chapter and pages
% From the January 2000 issue of "IEEE Transactions on Communications",
% page 16, reference #9.
@incollection{IEEEexample:incollection_chpp,
  author        = "P. Hedelin and P. Knagenhjelm and M. Skoglund",
  editor        = "W. B. Kleijn and K. K. Paliwal",
  title         = "Theory for Transmission of Vector Quantization Data",
  booktitle     = "Speech Coding and Synthesis",
  publisher     = "Elsevier Science",
  address       = "Amsterdam, The Netherlands",
  year          = "1995",
  chapter       = "10",
  pages         = "347-396"
};


% incollection with a large number of authors, some authors/journals will
% use et al. for so many names. IEEEtran.bst can be configured to do this
% if desired, or "R. M. A. Dawson and others" can be used instead.
% Note that IEEE may not include the publisher for incollection entries -
% IEEEtran.bst will not issue a warning if the publisher is missing for
% incollections - but other .bst files often will.
% From the June 2002 issue of "IEEE Transactions on Electron Devices",
% page 996, reference #12.
@incollection{IEEEexample:incollectionmanyauthors,
  author        = "R. M. A. Dawson and Z. Shen and D. A. Furst and
                   S. Connor and J. Hsu and M. G. Kane and R. G. Stewart and
                   A. Ipri and C. N. King and P. J. Green and R. T. Flegal
                   and S. Pearson and W. A. Barrow and E. Dickey and K. Ping
                   and C. W. Tang and S. Van. Slyke and
                   F. Chen and J. Shi and J. C. Sturm and M. H. Lu",
  title         = "Design of an Improved Pixel for a Polysilicon 
                   Active-Matrix Organic {LED} Display",
  booktitle     = "{SID} Tech. Dig.",
  volume        = "29",
  year          = "1998",
  pages         = "11-14"
};





% A Motorola data book as a manual
% It is somewhat unusual to include the data book part number.
% in the title. It might be more correct to put this information
% in the howpublished field instead.
% From the December 2000 issue of "IEEE Transactions on Communications",
% page 1986, reference #10.
@manual{IEEEexample:motmanual,
  title         = "{FLEXChip} Signal Processor ({MC68175/D})",
  organization  = "Motorola",
  year          = "1996"
};

% same reference, but using IEEEtran's howpublished extension
@manual{IEEEexample:motmanualhowpub,
  title         = "{FLEXChip} Signal Processor",
  howpublished  = "{MC68175/D}",
  organization  = "Motorola",
  year          = "1996"
};




% conference paper with an address and days. Some journals capitalize the
% letters in "Globecom", this one did not.
% From the May 2002 issue of "IEEE Transactions on Communications",
% page 697, reference #12.
@inproceedings{IEEEexample:confwithadddays,
  author        = "M. S. Yee and L. Hanzo",
  title         = "Radial Basis Function Decision Feedback Equaliser
                   Assisted Burst-by-burst Adaptive Modulation",
  booktitle     = "Proc. {IEEE} Globecom '99",
  address       = "Rio de Janeiro, Brazil",
  month         = dec # " 5--9,",
  year          = "1999",
  pages         = "2183-2187"
};


% conference paper with volume number. A conference proceedings with a vol
% number is a little uncommon, note that the vol number is placed
% before the address in the formatted bibliography entry
% From the April 2002 issue of "IEEE/ACM Transactions on Networking",
% page 181, reference #26.
@inproceedings{IEEEexample:confwithvolume,
  author        = "M. Yajnik and S. B. Moon and J. Kurose and D. Towsley",
  title         = "Measurement and Modeling of the Temporal Dependence in
                   Packet Loss",
  booktitle     = "Proc. {IEEE} {INFOCOM}'99",
  volume        = "1",
  address       = "New York, NY",
  month         = mar,
  year          = "1999",
  pages         = "345-352"
};


% conference paper with a paper number, the type field can be used to
% override the word "paper", e.g., type = "postdeadline paper". A type
% can be given even without a paper  field.
% Also, IEEEtran.bst can be configured to ignore paper numbers and types.
% From the May 2002 issue of "Journal of Lightwave Technology",
% page 807, reference #4.
@inproceedings{IEEEexample:confwithpaper,
  author        = "M. Wegmuller and J. P. von der Weid and P. Oberson
                   and N. Gisin",
  title         = "High Resolution Fiber Distributed Measurements With
                   Coherent {OFDR}",
  booktitle     = "Proc. {ECOC}'00",
  year          = "2000",
  paper         = "11.3.4",
  pages         = "109"
};


% conference paper with a postdeadline type paper, the paper field is
% optional.
% From the August 2000 issue of "IEEE Photonics Technology Letters",
% page 1087, reference #12.
@inproceedings{IEEEexample:confwithpapertype,
  author        = "B. Mikkelsen and G. Raybon and R.-J. Essiambre and
                   K. Dreyer and Y. Su. and L. E. Nelson and J. E. Johnson
                   and G. Shtengel and A. Bond and D. G. Moodie and
                   A. D. Ellis",
  title         = "160 {Gbit/s} Single-channel Transmission Over 300 km 
                   Nonzero-dispersion Fiber With Semiconductor Based
                   Transmitter and Demultiplexer",
  booktitle     = "Proc. {ECOC}'99",
  year          = "1999",
  paper         = "2-3",
  type          = "postdeadline paper",
  pages         = "28-29"
};


% presented at a conference
% intype overrides the default "in" and causes the booktitle not to be
% emphasized (rendered in italics).
% From the February 2002 issue of "IEEE/ACM Transactions on Networking",
% page 163, reference #6.
@inproceedings{IEEEexample:presentedatconf,
  author        = "S. G. Finn and M. M{\'e}dard and R. A. Barry",
  title         = "A Novel Approach to Automatic Protection Switching
                   Using Trees",
  intype        = "presented at the",
  booktitle     = "Proc. Int. Conf. Commun.",
  year          = "1997"
};





% master's thesis, often the University name will be abbreviated and the
% state or country will be included in the address. The type field can
% used to override the default type "Master's thesis"
% From the June 2002 issue of "IEEE Transactions on Microelectromechanical
% Systems", page 186, reference #11.
@mastersthesis{IEEEexample:masters,
  author        = "Nin C. Loh",
  title         = "High-Resolution Micromachined Interferometric
                   Accelerometer",
  school        = "Massachusetts Institute of Technology",
  address       = "Cambridge",
  year          = "1992"
};


% master's thesis with a type field
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 391, reference #12.
@mastersthesis{IEEEexample:masterstype,
  author        = "A. Karnik",
  title         = "Performance of {TCP} Congestion Control with Rate
                   Feedback: {TCP/ABR} and Rate Adaptive {TCP/IP}",
  school        = "Indian Institute of Science",
  type          = "M. Eng. thesis",
  address       = "Bangalore, India",
  month         = jan,
  year          = "1999"
};





% Ph.D. dissertation with a URL field, the university is abbreviated
% From the October 2001 issue of "IEEE/ACM Transactions on Networking",
% page 590, reference #11.
@phdthesis{IEEEexample:phdurl,
  author        = "Q. Li",
  title         = "Delay Characterization and Performance Control of
                   Wide-area Networks",
  school        = "Univ. of Delaware",
  address       = "Newark",
  month         = may,
  year          = "2000",
  url           = "http://www.ece.udel.edu/~qli"
};





% technical report
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 490, reference #15.
@techreport{IEEEexample:techrep,
  author        = "R. Jain and K. K. Ramakrishnan and D. M. Chiu",
  title         = "Congestion Avoidance in Computer Networks with a 
                   Connectionless Network Layer",
  institution   = "Digital Equipment Corporation",
  address       = "MA",
  number        = "DEC-TR-506",
  month         = aug,
  year          = "1987"
};


% technical report with type
% for those times when "Tech. Rep." needs to be modified
% From the February 2001 issue of "IEEE/ACM Transactions on Networking",
% page 46, reference #8.
@techreport{IEEEexample:techreptype,
  author        = "J. Padhye and V. Firoiu and D. Towsley",
  title         = "A Stochastic Model of {TCP} {R}eno Congestion Avoidance
                   and Control",
  institution   = "Univ. of Massachusetts",
  address       = "Amherst, MA",
  type          = "CMPSCI Tech. Rep.",
  number        = "99-02",
  year          = "1999"
};


% technical report with type
% for those times when "Tech. Rep." needs to be modified
% This reference did not have an address.
% From the January 2000 issue of "IEEE Transactions on Communications",
% page 117, reference #6.
@techreport{IEEEexample:techreptypeii,
  author        = "D. Middleton and A. D. Spaulding",
  title         = "A Tutorial Review of Elements of Weak Signal Detection
                   in Non-{G}aussian {EMI} Environments",
  institution   = "National Telecommunications and Information
                   Administration ({NTIA}), U.S. Dept. of Commerce",
  type          = "NTIA Report",
  number        = "86-194",
  month         = may,
  year          = "1986"
};





% an unpublished work
% for unpublished types, the note field is required. IEEE usually
% just uses the word "unpublished" for the note.
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 391, reference #16.
@unpublished{IEEEexample:unpublished,
  author        = "T. J. Ott and N. Aggarwal",
  title         = "{TCP} over {ATM}: {ABR} or {UBR}",
  note          = "Unpublished"
};





% electronic with a howpublished information field 
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 391, reference #7.
@electronic{IEEEexample:electronhowinfo,
  author        = "V. Jacobson",
  title         = "Modified {TCP} Congestion Avoidance Algorithm",
  howpublished  = "end2end-interest mailing list",
  url           = "ftp://ftp.isi.edu/end2end/end2end-interest-1990.mail",
  month         = apr,
  year          = "1990"
};


% electronic with a howpublished information field 
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 418, reference #31.
@electronic{IEEEexample:electronhowinfo2,
  author        = "V. Valloppillil and K. W. Ross",
  title         = "Cache Array Routing Protocol v1.1",
  howpublished  = "Internet draft",
  url           = "http://ds1.internic.net/internet-drafts/draft-vinod-carp-v1-03.txt",
  year          = "1998"
};


% electronic with an organization and address
% From the February 2002 issue of "IEEE/ACM Transactions on Networking",
% page 114, reference #15.
@electronic{IEEEexample:electronorgadd,
  author        = "D. H. Lorenz and A. Orda",
  title         = "Optimal Partition of {QoS} Requirements on Unicast
                   Paths and Multicast Trees",
  organization  = "Dept. Elect. Eng., Technion",
  address       = "Haifa, Israel",
  url           = "ftp://ftp.technion.ac.il/pub/supported/ee/Network/lor.mopq98.ps",
  month         = jul,
  year          = "1998"
};





% U.S. patent
% Use the type field to override the patent type. e.g.,
% type = "Patent Application"
% The address is that of the assignee. Note that IEEE does not
% display the assignee, the address, and only displays one date.
% (if year is not present, the filed dates are used.) However, this
% information should be entered as other BibTeX styles may use it.
% alternatively, nationality could be entered as "U.S." 
% From the April 2000 issue of "IEEE Transactions on Communications",
% page 542, reference #6.
@patent{IEEEexample:uspat,
  author        = "Ronald E. Sorace and Victor S. Reinhardt and
                   Steven A. Vaughn",
  assignee      = "Hughes Aircraft Company",
  address       = "Los Angeles, CA",
  title         = "High-Speed Digital-to-{RF} Converter",
  nationality   = "United States",
  number        = "5668842",
  dayfiled      = "28",
  monthfiled    = feb,
  yearfiled     = "1995",
  day           = "16",
  month         = sep,
  year          = "1997"
};


% Japanese Patent
% From the April 2000 issue of "IEEE Transactions on Communications",
% page 556, reference #6.
@patent{IEEEexample:jppat,
  author        = "U. Hideki",
  title         = "Quadrature Modulation Circuit",
  nationality   = "Japanese",
  number        = "152932/92",
  day           = "20",
  month         = may,
  year          = "1992"
};


% French Patent request, the language field must be entered in lower case
% as this is the option name Babel uses. The nationality field needs to be
% capitalized. Because this is a patent request, the date filed fields are
% used while the date fields are left empty/missing. In other countries,
% the words "Patent Application", etc. are used instead.
% From the April 2000 issue of "IEEE Transactions on Communications",
% page 556, reference #9.
@patent{IEEEexample:frenchpatreq,
  author        = "F. Kowalik and M. Isard",
  title         = "Estimateur d'un D{\'e}faut de Fonctionnement 
                   d'un Modulateur en Quadrature et {\'E}tage de Modulation
                   l'Utilisant",
  language      = "french",
  nationality   = "French",
  type          = "Patent Request",
  number        = "9500261",
  dayfiled      = "11",
  monthfiled    = jan,
  yearfiled     = "1995"
};





% a periodical
% From the April 2001 issue of "IEEE/ACM Transactions on Networking",
% page 160, reference #1.
% sort key is needed for sorting styles
@periodical{IEEEexample:periodical,
  title         = IEEE_M_PCOM # ", Special Issue on Wireless {ATM}",
  volume        = "3",
  month         = aug,
  year          = "1996",
  key           = "IEEE"
};





% standard, IEEE does not use the address for standards, but it is good
% to provide one for BibTeX styles that use it.
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 451, reference #2.
@standard{IEEEexample:standard,
  title         = "Wireless {LAN} Medium Access Control {(MAC)} and 
                   Physical Layer {(PHY)} Specification",
  organization  = "IEEE",
  address       = "Piscataway, NJ",
  number        = "802.11",
  year          = "1997"
};


% standard with type and revision, the type overrides the word standard
% (or std.). Here a standard number is not available and a revision number
% is used.
% From the August 2000 issue of "IEEE Photonics Technology Letters",
% page 1048, reference #13.
@standard{IEEEexample:standardproposed,
  title         = "Fiber Channel Physical Interface ({FC-PI})",
  institution   = "NCITS",
  address       = "Washington, DC",
  type          = "Working Draft Proposed Standard",
  revision      = "5.2",
  year          = "1999"
};


% standard draft as a misc with author
% From the May 2002 issue of "IEEE Journal of Selected Areas in
% Communication", page 725, reference #16.
@misc{IEEEexample:draftasmisc,
  author        = "I. Widjaja and A. Elwalid",
  title         = "{MATE}: {MPLS} Adaptive Traffic Engineering",
  howpublished  = "IETF Draft",
  year          = "1999"
};





% misc for a techreport like reference
% techreport is not perfectly suitable because this entry lacks
% an institution field
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 490, reference #22.
@misc{IEEEexample:miscforum,
  author        = "L. Roberts",
  title         = "Enhanced Proportional Rate Control Algorithm {PRCA}",
  howpublished  = "{ATM} Forum Contribution 94-0735R1",
  month         = aug,
  year          = "1994"
};


% misc for a white paper
% From the August 2001 issue of "IEEE/ACM Transactions on Networking",
% page 478, reference #4 - Note that the reference there (improperly?)
% used the author field for "Cisco".
@misc{IEEEexample:whitepaper,
  title         = "Advanced {QoS} Services for the Intelligent Internet",
  howpublished  = "White Paper",
  organization  = "Cisco",
  month         = may,
  year          = "1997"
};


% misc for a data sheet
% From the November 2000 issue of "IEEE Photonics Technology Letters",
% page 1551, reference #6.
@misc{IEEEexample:datasheet,
  title         = "{PDCA12-70} data sheet",
  organization  = "Opto Speed SA",
  address       = "Mezzovico, Switzerland"
};





% Other unusual references

% a private communication as a misc entry type
% sometimes the designation "personal communication" is used instead
% From the June 2002 issue of "IEEE Transactions on Information Theory",
% page 1725, reference #16.
@misc{IEEEexample:private,
  author        = "S. Konyagin",
  howpublished  = "private communication",
  year          = "1998"
};


% an internet request for comments (RFC) as a misc entry type
% It would also be nice to add a URL to these types of things.
% RFCs can also be handled as electronic references.
% From the April 2002 issue of "IEEE/ACM Transactions on Networking",
% page 181, reference #14.
@misc{IEEEexample:miscrfc,
  author        = "K. K. Ramakrishnan and S. Floyd",
  title         = "A Proposal to Add Explicit Congestion
                   Notification ({ECN}) to {IP}",
  howpublished  = "RFC 2481",
  month         = jan,
  year          = "1999"
};


% misc for a German regulation
% In German, the first letters of nouns are capitalized, so we do so here.
% From the June 2002 issue of "IEEE Journal in Selected Areas in
% Communication", page 892, reference #9.
@misc{IEEEexample:miscgermanreg,
  title         = "{M}essung von {S}t{\"o}rfeldern an {A}nlagen 
                   und {L}eitungen der {T}elekommunikation im
                   {F}requenzbereich 9 {kHz} bis 3 {GHz}",
  language      = "german",
  howpublished  = "{M}e{\ss}vorschrift {R}eg {TP} {MV} 05",
  organization  = "Regulierungsbeh{\"o}rde f{\"u}r {T}elekommunikation und
                   {P}ost ({R}eg {TP})"
};



% Ways to handle things like CCSDS's Blue Books
% journal article with a URL. However, this is not a very good approach
% because the Blue Books are not really journals and the author field has
% to be abused.
% From the June 2002 issue of "IEEE Transactions on Information Theory",
% page 1461, reference #7.
@article{IEEEexample:bluebookarticle,
  author        = "{Consulative Committee for Space Data Systems (CCSDS)}",
  title         = "Telemetry Channel Coding",
  journal       = "Blue Book",
  number        = "4",
  year          = "1999",
  url           = "http://www.ccsds.org/documents/pdf/CCSDS-101.0-B-4.pdf"
};


% CCSDS's Blue Book handled as a book
% However, it is not a good idea to have to use the author field for
% an organization (done here because the book entry type requires an
% author field).
@book{IEEEexample:bluebookbook,
  author        = "{Consulative Committee for Space Data Systems (CCSDS)}",
  title         = "Telemetry Channel Coding",
  series        = "Blue Book",
  number        = "4",
  publisher     = "CCSDS",
  address       = "Newport Beach, CA",
  year          = "1999",
  url           = "http://www.ccsds.org/documents/pdf/CCSDS-101.0-B-4.pdf"
};

% CCSDS's Blue Book handled as a manual
% This is a much better approach, but uses the howpublished field.
@manual{IEEEexample:bluebookmanual,
  title         = "Telemetry Channel Coding",
  howpublished  = "ser. Blue Book, No. 4",
  organization  = "Consulative Committee for Space Data Systems (CCSDS)",
  address       = "Newport Beach, CA",
  year          = "1999",
  url           = "http://www.ccsds.org/documents/pdf/CCSDS-101.0-B-4.pdf"
};



% CCSDS's Blue Book handled as a standard
% Probably the best approach for this particular case because the work
% is standard related. Note that IEEE does not display the address for
% standards.
@standard{IEEEexample:bluebookstandard,
  title         = "Telemetry Channel Coding",
  howpublished  = "ser. Blue Book, No. 4",
  organization  = "Consulative Committee for Space Data Systems (CCSDS)",
  address       = "Newport Beach, CA",
  type          = "Recommendation for Space Data System Standard",
  number        = "101.0-B-4",
  month         = May,
  year          = "1999",
  url           = "http://www.ccsds.org/documents/pdf/CCSDS-101.0-B-4.pdf"
};





 
% An example of a IEEEtran control entry which can change some IEEEtran.bst
% settings. An entry like this must be cited via \bstctlcite{} command
% before the first real \cite{}. The same entry key cannot be called twice
% (just like multiple \cite{} of the same entry key place only one entry
% in the bibliography.)
% The available control fields are:
% 
% CTLuse_article_number
% "no" turns off the display of the number for articles.
% "yes" enables
%
% CTLuse_paper
% "no" turns off the display of the paper and type fields in inproceedings.
% "yes" enables
% 
% CTLuse_forced_etal 
% "no" turns off the forced use of "et al."
% "yes" enables
% 
% CTLmax_names_forced_etal
% The maximum number of names that can be present beyond which an "et al."
% usage is forced. Be sure that CTLnames_show_etal (below)
% is not greater than this value!
% 
% CTLnames_show_etal
% The number of names that will be shown with a forced "et al.".
% Must be less than or equal to CTLmax_names_forced_etal
% 
% CTLuse_alt_spacing 
% "no" turns off the alternate interword spacing for entries with URLs.
% "yes" enables
% 
% CTLalt_stretch_factor
% If alternate interword spacing for entries with URLs is enabled, this is
% the interword spacing stretch factor that will be used. For example, the
% default "4" here means that the interword spacing in entries with URLs can
% stretch to four times normal. Does not have to be an integer.
% 
% CTLdash_repeated_names
% "no" turns off the "dashification" of repeated (i.e., identical to those
% of the previous entry) names. IEEE normally does this.
% "yes" enables
% 
% CTLname_format_string
% The name format control string as explained in the BibTeX style hacking
% guide.
% IEEE style "{f.~}{vv~}{ll}{, jj}" is the default,
% 
% CTL_name_latex_cmd
% A LaTeX command that each name will be fed to (e.g., "\textsc").
% Leave empty if no special font is desired for the names.
% The default is empty.
% 
% Those fields that are not to be changed can be left out or empty.
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_article_number     = "yes",
  CTLuse_paper              = "yes",
  CTLuse_forced_etal        = "no",
  CTLmax_names_forced_etal  = "10",
  CTLnames_show_etal        = "1",
  CTLuse_alt_spacing        = "yes",
  CTLalt_stretch_factor     = "4",
  CTLdash_repeated_names    = "yes",
  CTLname_format_string     = "{f.~}{vv~}{ll}{, jj}",
  CTLname_latex_cmd         = ""
};


